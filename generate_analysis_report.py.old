#!/usr/bin/env python3
"""
Generate reports based on Analysis sheet dropdown selections
Features:
- Multiple selection support (comma-separated)
- Enhanced party roles (VTP, VLP, Consumption, Production)
- Automatic data cleanup before generation
- Date format parsing + filters
"""

from google.cloud import bigquery
from googleapiclient.discovery import build
from google.oauth2.service_account import Credentials
import pandas as pd
from datetime import datetime, timedelta

SPREADSHEET_ID = '1-u794iGngn5_Ql_XocKSwvHSKWABWO0bVsudkUJAFqA'
CREDENTIALS_FILE = 'inner-cinema-credentials.json'
PROJECT_ID = 'inner-cinema-476211-u9'

print("ðŸ“Š Reading Analysis sheet selections...\n")

creds = Credentials.from_service_account_file(CREDENTIALS_FILE)
sheets_service = build('sheets', 'v4', credentials=creds)
bq_client = bigquery.Client(project=PROJECT_ID, location='US')

# ============================================================================
# STEP 0: CLEAR OLD DATA (runs every time)
# ============================================================================
print("ðŸ§¹ Clearing old report data...\n")

try:
    sheets_service.spreadsheets().values().clear(
        spreadsheetId=SPREADSHEET_ID,
        range='Analysis!A19:Z10000'
    ).execute()
    print("âœ… Cleared previous report data\n")
except Exception as e:
    print(f"âš ï¸  Could not clear old data: {e}\n")

# Read user selections
selections = sheets_service.spreadsheets().values().batchGet(
    spreadsheetId=SPREADSHEET_ID,
    ranges=['Analysis!B4', 'Analysis!D4', 'Analysis!B5:B9', 'Analysis!B11:B13']
).execute()

from_date_raw = selections['valueRanges'][0]['values'][0][0] if selections['valueRanges'][0].get('values') else None
to_date_raw = selections['valueRanges'][1]['values'][0][0] if selections['valueRanges'][1].get('values') else None
filters = [row[0] if row else 'All' for row in selections['valueRanges'][2].get('values', [['All']]*5)]
report_options = [row[0] if row else '' for row in selections['valueRanges'][3].get('values', [['']]*3)]

# Parse multiple selections (comma-separated)
def parse_multiple_selections(value):
    """Parse comma-separated values and extract role names"""
    if not value or value == 'All':
        return ['All']
    # Split by comma and clean up
    values = [v.strip() for v in value.split(',')]
    # Extract role name from "Role - Description" format
    roles = []
    for v in values:
        if ' - ' in v:
            roles.append(v.split(' - ')[0].strip())
        else:
            roles.append(v.strip())
    return roles if roles else ['All']

# Parse filters to handle multiple selections
party_roles_raw = filters[0]
party_roles = parse_multiple_selections(party_roles_raw)
bmu_id, unit_name, gen_type, lead_party = filters[1:5]

# Parse dates - handle DD/MM/YYYY from Google Sheets
def parse_date(date_str):
    if not date_str:
        return None
    date_str = str(date_str).strip()

    # Try DD/MM/YYYY format
    if '/' in date_str:
        parts = date_str.split('/')
        if len(parts) == 3:
            day, month, year = parts
            return f"{year}-{month.zfill(2)}-{day.zfill(2)}"

    # Already in YYYY-MM-DD format
    return date_str

from_date = parse_date(from_date_raw)
to_date = parse_date(to_date_raw)

report_category, report_type, graph_type = report_options

print(f"ðŸ“… Date Range: {from_date} â†’ {to_date}")
print(f"ðŸ” Party Roles: {', '.join(party_roles)}")
print(f"ðŸ” Other Filters: BMU={bmu_id}, GenType={gen_type}")
print(f"ðŸ“Š Report: {report_category}\n")

# Build query with enhanced filters
def get_query_with_filters(category, from_dt, to_dt, party_roles, gen_type='All'):
    """Build BigQuery query with party role filter support"""
    
    # Map party roles to BMU ID patterns
    role_patterns = {
        'Production': "bmUnitId LIKE 'E_%' OR bmUnitId LIKE 'M_%'",
        'VTP': "bmUnitId LIKE 'T_%'",
        'VLP': "bmUnitId LIKE '%FBPGM%' OR bmUnitId LIKE '%FESEN%' OR bmUnitId LIKE '%STORAGE%'",
        'Consumption': "bmUnitId LIKE 'D_%'",
        'Supplier': "bmUnitId LIKE '2__%'",
        'Interconnector': "bmUnitId LIKE 'I_%'",
        'Trader': "1=1",  # No specific pattern
        'Storage': "bmUnitId LIKE '%STORAGE%' OR bmUnitId LIKE '%BESS%'",
        'All': "1=1"
    }
    
    # Build party role filter
    party_filter = ""
    if 'All' not in party_roles and party_roles:
        role_conditions = [role_patterns.get(role, "1=1") for role in party_roles]
        party_filter = f"AND ({' OR '.join(role_conditions)})"
    
    # Analytics & Derived: Aggregated generation data
    if 'ðŸ“Š Analytics' in category or 'Analytics & Derived' in category:
        where_clauses = [
            f"settlementDate >= '{from_dt}'",
            f"settlementDate <= '{to_dt}'"
        ]

        if gen_type and gen_type != 'All':
            where_clauses.append(f"fuelType = '{gen_type}'")

        where_clause = " AND ".join(where_clauses)

        return f"""
        SELECT
            CAST(settlementDate AS DATE) as date,
            settlementPeriod,
            fuelType,
            SUM(generation) as generation_mw
        FROM `{PROJECT_ID}.uk_energy_prod.bmrs_fuelinst_iris`
        WHERE {where_clause}
        {party_filter}
        GROUP BY date, settlementPeriod, fuelType
        ORDER BY date, settlementPeriod, fuelType
        LIMIT 10000
        """
    
    # Generation: Raw generation data
    if 'âš¡ Generation' in category or 'Generation' in category:
    if '/' in date_str:
        parts = date_str.split('/')
        if len(parts) == 3:
            day, month, year = parts
            return f"{year}-{month.zfill(2)}-{day.zfill(2)}"

    # Already in YYYY-MM-DD format
    return date_str

from_date = parse_date(from_date_raw)
to_date = parse_date(to_date_raw)

party_role, bmu_id, unit_name, gen_type, lead_party = filters
report_category, report_type, graph_type = report_options

print(f"ðŸ“… Date Range: {from_date} â†’ {to_date}")
print(f"ðŸ” Filters: Party={party_role}, BMU={bmu_id}, GenType={gen_type}")
print(f"ðŸ“Š Report: {report_category}\n")

# Build query with filters
def get_query_with_filters(category, from_dt, to_dt, gen_type='All'):
    # Analytics & Derived: Aggregated generation data
    if 'ðŸ“Š Analytics' in category or 'Analytics & Derived' in category:
        where_clauses = [
            f"settlementDate >= '{from_dt}'",
            f"settlementDate <= '{to_dt}'"
        ]

        if gen_type and gen_type != 'All':
            where_clauses.append(f"fuelType = '{gen_type}'")

        where_clause = " AND ".join(where_clauses)

        return f"""
        SELECT
            CAST(settlementDate AS DATE) as date,
            settlementPeriod,
            fuelType,
            SUM(generation) as generation_mw
        FROM `{PROJECT_ID}.uk_energy_prod.bmrs_fuelinst_iris`
        WHERE {where_clause}
        GROUP BY date, settlementPeriod, fuelType
        ORDER BY date, settlementPeriod, fuelType
        LIMIT 10000
        """
    
    # Generation: Raw generation data
    if 'âš¡ Generation' in category:
        where_clauses = [
            f"settlementDate >= '{from_dt}'",
            f"settlementDate <= '{to_dt}'",
            "generation > 0"
        ]

        if gen_type and gen_type != 'All':
            where_clauses.append(f"fuelType = '{gen_type}'")

        where_clause = " AND ".join(where_clauses)

        return f"""
        SELECT
            CAST(settlementDate AS DATE) as date,
            settlementPeriod,
            fuelType,
            generation as generation_mw
        FROM `{PROJECT_ID}.uk_energy_prod.bmrs_fuelinst_iris`
        WHERE {where_clause}
        ORDER BY settlementDate, settlementPeriod, fuelType
        LIMIT 10000
        """

    # Default query - returns raw data for debugging
    return f"""
    SELECT
        CAST(settlementDate AS DATE) as date,
        settlementPeriod,
        fuelType,
        generation as generation_mw
    FROM `{PROJECT_ID}.uk_energy_prod.bmrs_fuelinst_iris`
    WHERE settlementDate >= '{from_dt}' AND settlementDate <= '{to_dt}'
    ORDER BY settlementDate, settlementPeriod, fuelType
    LIMIT 1000
    """

# Execute query
print("ðŸ”„ Querying BigQuery...\n")
query = get_query_with_filters(report_category, from_date, to_date, gen_type)

try:
    df = bq_client.query(query).to_dataframe()

    print(f"âœ… Retrieved {len(df)} rows\n")

    # Convert dates to strings
    for col in df.columns:
        if pd.api.types.is_datetime64_any_dtype(df[col]):
            df[col] = df[col].dt.strftime('%Y-%m-%d %H:%M:%S')
        elif hasattr(df[col].dtype, 'name') and 'date' in df[col].dtype.name:
            df[col] = df[col].astype(str)

    df = df.fillna('')

    # Write to sheets
    print("ðŸ“ Writing results to Google Sheets...\n")

    # Limit to 1000 rows for display
    df_display = df.head(1000)

    # Clear old results
    sheets_service.spreadsheets().values().clear(
        spreadsheetId=SPREADSHEET_ID,
        range='Analysis!A18:Z10000'
    ).execute()

    # Prepare data
    values = [df_display.columns.tolist()] + df_display.values.tolist()

    # Write new results
    sheets_service.spreadsheets().values().update(
        spreadsheetId=SPREADSHEET_ID,
        range='Analysis!A18',
        valueInputOption='RAW',
        body={'values': values}
    ).execute()

    print(f"âœ… Written {len(df_display)} rows to Analysis sheet")
    print(f"   Location: Row 18+")
    print(f"   Columns: {', '.join(df_display.columns)}")

    print(f"\nðŸ“ˆ Data Summary:")
    print(f"   Total Rows: {len(df):,}")
    if 'date' in df.columns:
        print(f"   Date Range: {df['date'].min()} â†’ {df['date'].max()}")
    print(f"   Category: {report_category}")

    print(f"\nâœ… Report generation complete!")

except Exception as e:
    error_msg = str(e)
    print(f"âŒ Error: {error_msg}\n")
    print(f"Query attempted:\n{query}")

    # Write error to sheet
    sheets_service.spreadsheets().values().update(
        spreadsheetId=SPREADSHEET_ID,
        range='Analysis!A18',
        valueInputOption='RAW',
        body={'values': [[f'âŒ Error: {error_msg[:200]}']]}
    ).execute()

