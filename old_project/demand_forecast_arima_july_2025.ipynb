{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41dc8b5c",
   "metadata": {},
   "source": [
    "# Advanced Statistical Analysis for UK Energy Data (July 2025)\n",
    "\n",
    "This notebook performs a comprehensive statistical analysis of UK energy data for July 2025, loaded from Google BigQuery. The analysis includes:\n",
    "- T-tests and ANOVA for price comparisons.\n",
    "- Correlation analysis between key metrics.\n",
    "- Regression models for price drivers.\n",
    "- Time series forecasting using SARIMAX.\n",
    "- Event impact analysis.\n",
    "\n",
    "Each step is contained in a separate cell for clarity and modular execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6280b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'scipy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m stats\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mstatsmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msm\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mstatsmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtsa\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mseasonal\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m seasonal_decompose\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'scipy'"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# ======== CONFIG =========\n",
    "# =========================\n",
    "PROJECT_ID = \"jibber-jabber-knowledge\"\n",
    "LOCATION = \"europe-west2\"\n",
    "DATASET_SOURCE = \"uk_energy\"\n",
    "DATASET_ANALYTICS = \"uk_energy_analysis\"\n",
    "GCS_BUCKET = \"jibber-jabber-knowledge-bmrs-data\"\n",
    "DATE_START = \"2025-07-01\"\n",
    "DATE_END = \"2025-07-31\"\n",
    "\n",
    "print(\"Configuration loaded.\")\n",
    "\n",
    "# =========================\n",
    "# ======== IMPORTS ========\n",
    "# =========================\n",
    "import pathlib\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from google.cloud import bigquery\n",
    "from pandas_gbq import to_gbq\n",
    "import warnings\n",
    "\n",
    "# Optional: GCS upload for plots\n",
    "try:\n",
    "    from google.cloud import storage as gcs_storage\n",
    "    HAS_GCS = True\n",
    "except ImportError:\n",
    "    HAS_GCS = False\n",
    "\n",
    "# Suppress future warnings for cleaner output\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# Use 'inline' for notebook display\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Libraries imported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22925e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# ====== UTILITIES ========\n",
    "# =========================\n",
    "OUTDIR = pathlib.Path(\"./output\")\n",
    "OUTDIR.mkdir(exist_ok=True)\n",
    "\n",
    "def save_plot(fig, fname: str):\n",
    "    \"\"\"Save plot locally or to GCS if configured.\"\"\"\n",
    "    local_path = OUTDIR / fname\n",
    "    fig.savefig(local_path, bbox_inches=\"tight\", dpi=150)\n",
    "    # No need to close the figure in a notebook if we want to display it\n",
    "    # plt.close(fig) \n",
    "    if GCS_BUCKET and HAS_GCS:\n",
    "        try:\n",
    "            client = gcs_storage.Client(project=PROJECT_ID)\n",
    "            bucket = client.bucket(GCS_BUCKET)\n",
    "            blob = bucket.blob(f\"output/{fname}\")\n",
    "            blob.upload_from_filename(str(local_path))\n",
    "            print(f\"[GCS] Uploaded gs://{GCS_BUCKET}/output/{fname}\")\n",
    "            return f\"gs://{GCS_BUCKET}/output/{fname}\"\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] GCS upload failed: {e}\")\n",
    "    elif GCS_BUCKET and not HAS_GCS:\n",
    "        print(\"[WARN] GCS_BUCKET set but google-cloud-storage not installed; saved locally.\")\n",
    "    return str(local_path)\n",
    "\n",
    "def write_bq(df: pd.DataFrame, table_name: str, if_exists=\"replace\"):\n",
    "    \"\"\"Write a DataFrame to BigQuery.\"\"\"\n",
    "    full_table = f\"{PROJECT_ID}.{DATASET_ANALYTICS}.{table_name}\"\n",
    "    if df.empty:\n",
    "        print(f\"[INFO] {table_name}: DataFrame is empty; skipping write.\")\n",
    "        return\n",
    "    try:\n",
    "        to_gbq(df, full_table, project_id=PROJECT_ID, if_exists=if_exists, location=LOCATION)\n",
    "        print(f\"[BQ] Wrote {len(df):,} rows to {full_table} (if_exists={if_exists})\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to write to BigQuery table {full_table}: {e}\")\n",
    "\n",
    "\n",
    "def add_calendar_fields(df: pd.DataFrame, ts_col: str):\n",
    "    ts = pd.to_datetime(df[ts_col])\n",
    "    df[\"date\"] = ts.dt.date\n",
    "    df[\"month\"] = ts.dt.month\n",
    "    df[\"dow\"] = ts.dt.dayofweek\n",
    "    df[\"is_weekend\"] = df[\"dow\"] >= 5\n",
    "    def season(m):\n",
    "        return \"Winter\" if m in (12,1,2) else \"Spring\" if m in (3,4,5) else \"Summer\" if m in (6,7,8) else \"Autumn\"\n",
    "    df[\"season\"] = df[\"month\"].apply(season)\n",
    "    return df\n",
    "\n",
    "print(\"Utility functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c71a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# ======== LOADING ========\n",
    "# =========================\n",
    "def _table_exists(client: bigquery.Client, fq_table: str) -> bool:\n",
    "    try:\n",
    "        client.get_table(fq_table)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def load_data() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load and harmonize series from uk_energy dataset for the configured date range.\n",
    "    \"\"\"\n",
    "    client = bigquery.Client(project=PROJECT_ID, location=LOCATION)\n",
    "\n",
    "    # This simplified query joins the essential tables for the analysis.\n",
    "    # It assumes tables are named like 'elexon_demand_outturn_2025_07'\n",
    "    # A more robust version would use wildcards, but this is specific to July 2025.\n",
    "    \n",
    "    # For simplicity, we will focus on a few key tables first.\n",
    "    # This query joins demand, bid-offer acceptances (for price), and system warnings.\n",
    "    \n",
    "    sql = f\"\"\"\n",
    "    WITH base_grid AS (\n",
    "        -- Create a complete 30-minute grid for July 2025\n",
    "        SELECT ts\n",
    "        FROM UNNEST(GENERATE_TIMESTAMP_ARRAY('2025-07-01 00:00:00', '2025-07-31 23:30:00', INTERVAL 30 MINUTE)) AS ts\n",
    "    ),\n",
    "    demand AS (\n",
    "        SELECT\n",
    "            TIMESTAMP_TRUNC(timeFrom, MINUTE) as ts,\n",
    "            AVG(nationalDemand) as volume\n",
    "        FROM `jibber-jabber-knowledge.uk_energy.elexon_demand_outturn_2025_07`\n",
    "        GROUP BY 1\n",
    "    ),\n",
    "    prices AS (\n",
    "        SELECT\n",
    "            TIMESTAMP_TRUNC(timeFrom, MINUTE) as ts,\n",
    "            -- Simplified proxy for SSP/SBP\n",
    "            AVG(CASE WHEN bidOfferLevel > 0 THEN bidOfferLevel ELSE NULL END) as SBP,\n",
    "            AVG(CASE WHEN bidOfferLevel < 0 THEN bidOfferLevel ELSE NULL END) as SSP\n",
    "        FROM `jibber-jabber-knowledge.uk_energy.elexon_boalf_2025_07`\n",
    "        GROUP BY 1\n",
    "    ),\n",
    "    warnings AS (\n",
    "        SELECT\n",
    "            TIMESTAMP_TRUNC(timeFrom, MINUTE) as ts,\n",
    "            TRUE as unplanned_event\n",
    "        FROM `jibber-jabber-knowledge.uk_energy.elexon_system_warnings_2025_07`\n",
    "        GROUP BY 1\n",
    "    )\n",
    "    SELECT\n",
    "        bg.ts,\n",
    "        d.volume,\n",
    "        p.SSP,\n",
    "        p.SBP,\n",
    "        (p.SBP - p.SSP) as spread,\n",
    "        w.unplanned_event\n",
    "    FROM base_grid bg\n",
    "    LEFT JOIN demand d ON bg.ts = d.ts\n",
    "    LEFT JOIN prices p ON bg.ts = p.ts\n",
    "    LEFT JOIN warnings w ON bg.ts = w.ts\n",
    "    ORDER BY bg.ts\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Executing BigQuery query...\")\n",
    "    df = client.query(sql).to_dataframe(create_bqstorage_client=True)\n",
    "    \n",
    "    if df.empty:\n",
    "        raise RuntimeError(\"No data returned from BigQuery for the specified date range.\")\n",
    "        \n",
    "    # Post-processing\n",
    "    df['unplanned_event'] = df['unplanned_event'].fillna(False)\n",
    "    df = df.interpolate(method='linear', limit_direction='forward', axis=0)\n",
    "    df = df.dropna(subset=[\"SSP\", \"SBP\"], how=\"all\")\n",
    "    df = add_calendar_fields(df, \"ts\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"Data loading function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29776c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# ===== EXECUTE LOAD ======\n",
    "# =========================\n",
    "print(f\"[{datetime.utcnow().isoformat()}] Loading harmonized data from BigQuery uk_energy…\")\n",
    "try:\n",
    "    df = load_data()\n",
    "    print(f\"[INFO] Loaded {len(df):,} rows\")\n",
    "    print(\"Data preview:\")\n",
    "    display(df.head())\n",
    "    print(\"\\nData Info:\")\n",
    "    df.info()\n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] Data loading failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616ce21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# ====== ARIMA MODEL ======\n",
    "# =========================\n",
    "def arima_ssp(df: pd.DataFrame):\n",
    "    if \"SSP\" not in df.columns:\n",
    "        print(\"[WARN] SSP column not found.\")\n",
    "        return pd.DataFrame(), None\n",
    "        \n",
    "    d = df[[\"ts\",\"SSP\"]].dropna().copy()\n",
    "    if d.empty:\n",
    "        print(\"[WARN] No SSP data available for ARIMA.\")\n",
    "        return pd.DataFrame(), None\n",
    "        \n",
    "    d[\"ts\"] = pd.to_datetime(d[\"ts\"])\n",
    "    d = d.set_index(\"ts\").sort_index()\n",
    "    y = d[\"SSP\"].asfreq(\"30min\").interpolate(limit_direction=\"both\")\n",
    "    \n",
    "    # Ensure we have enough data for weekly seasonality\n",
    "    if y.size < 48*14:\n",
    "        print(\"[WARN] Not enough data for stable weekly seasonality ARIMA (need at least 2 weeks).\")\n",
    "        return pd.DataFrame(), None\n",
    "        \n",
    "    season = 48 * 7  # 30-min periods in a week\n",
    "    order = (1,1,1)\n",
    "    seasonal_order = (1,1,1,season)\n",
    "    \n",
    "    print(\"Fitting SARIMAX model... (this may take a moment)\")\n",
    "    try:\n",
    "        model = SARIMAX(y, order=order, seasonal_order=seasonal_order,\n",
    "                        enforce_stationarity=False, enforce_invertibility=False)\n",
    "        res = model.fit(disp=False)\n",
    "        print(\"Model fitting complete.\")\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] ARIMA failed: {e}\")\n",
    "        return pd.DataFrame(), None\n",
    "        \n",
    "    steps = 48 * 3 # 3 days ahead\n",
    "    fc = res.get_forecast(steps=steps)\n",
    "    pred = fc.predicted_mean\n",
    "    ci = fc.conf_int(alpha=0.05)\n",
    "    \n",
    "    out = pd.DataFrame({\n",
    "        \"ts\": pred.index,\n",
    "        \"forecast_ssp\": pred.values,\n",
    "        \"ci_lo\": ci.iloc[:,0].values,\n",
    "        \"ci_hi\": ci.iloc[:,1].values\n",
    "    }).reset_index(drop=True)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(15,7))\n",
    "    y.tail(48*7).plot(ax=ax, label=\"Historical SSP (Last 7 days)\")\n",
    "    pred.plot(ax=ax, label=\"Forecasted SSP\", linestyle='--')\n",
    "    ax.fill_between(out[\"ts\"], out[\"ci_lo\"], out[\"ci_hi\"], color='k', alpha=0.1, label=\"95% Confidence Interval\")\n",
    "    ax.set_title(f\"SSP ARIMA Forecast (Next {steps//48} days)\")\n",
    "    ax.set_xlabel(\"Time (UTC)\")\n",
    "    ax.set_ylabel(\"SSP (£/MWh)\")\n",
    "    ax.legend()\n",
    "    ax.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "    \n",
    "    path = save_plot(fig, \"arima_ssp_forecast_july2025.png\")\n",
    "    out[\"plot_path\"] = path\n",
    "    out[\"aic\"] = res.aic\n",
    "    out[\"bic\"] = res.bic\n",
    "    out[\"order\"] = str(order)\n",
    "    out[\"seasonal_order\"] = str(seasonal_order)\n",
    "    \n",
    "    return out, fig\n",
    "\n",
    "# Run the analysis\n",
    "if 'df' in locals() and not df.empty:\n",
    "    arima_df, arima_plot = arima_ssp(df)\n",
    "    if not arima_df.empty:\n",
    "        print(\"\\nARIMA Forecast Results:\")\n",
    "        display(arima_df.head())\n",
    "        write_bq(arima_df, \"arima_forecast_ssp_july2025\")\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"DataFrame 'df' not available. Please run the data loading cell first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7d7ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# ====== T-TESTS ==========\n",
    "# =========================\n",
    "def ttest_ssp_sbp(df: pd.DataFrame):\n",
    "    if not {\"SSP\",\"SBP\"}.issubset(df.columns):\n",
    "        return pd.DataFrame()\n",
    "    a = df[\"SSP\"].astype(float).values\n",
    "    b = df[\"SBP\"].astype(float).values\n",
    "    # Only keep finite values\n",
    "    a = a[np.isfinite(a)]\n",
    "    b = b[np.isfinite(b)]\n",
    "    if len(a) < 3 or len(b) < 3:\n",
    "        return pd.DataFrame()\n",
    "    tstat, pval = stats.ttest_ind(a, b, equal_var=False, nan_policy=\"omit\")\n",
    "    n1, n2 = len(a), len(b)\n",
    "    m1, m2 = np.nanmean(a), np.nanmean(b)\n",
    "    s1, s2 = np.nanstd(a, ddof=1), np.nanstd(b, ddof=1)\n",
    "    # Welch-Satterthwaite df\n",
    "    df_denom = (s1**2/n1 + s2**2/n2)**2\n",
    "    df_num = (s1**2/n1)**2/(n1-1) + (s2**2/n2)**2/(n2-1)\n",
    "    dof = df_denom/df_num if df_num > 0 else np.nan\n",
    "    mean_diff = m2 - m1\n",
    "    se = np.sqrt(s1**2/n1 + s2**2/n2)\n",
    "    ci_lo = mean_diff - 1.96*se\n",
    "    ci_hi = mean_diff + 1.96*se\n",
    "    return pd.DataFrame([{\n",
    "        \"metric\": \"SSP_vs_SBP\",\n",
    "        \"t_stat\": tstat,\n",
    "        \"p_value\": pval,\n",
    "        \"mean_SSP\": m1,\n",
    "        \"mean_SBP\": m2,\n",
    "        \"mean_diff\": mean_diff,\n",
    "        \"ci_95_lo\": ci_lo,\n",
    "        \"ci_95_hi\": ci_hi,\n",
    "        \"dof\": dof,\n",
    "        \"n_SSP\": n1,\n",
    "        \"n_SBP\": n2\n",
    "    }])\n",
    "\n",
    "if 'df' in locals() and not df.empty:\n",
    "    ttest_df = ttest_ssp_sbp(df)\n",
    "    if not ttest_df.empty:\n",
    "        print(\"T-test Results (SSP vs SBP):\")\n",
    "        display(ttest_df)\n",
    "        write_bq(ttest_df, \"ttest_results_july2025\")\n",
    "else:\n",
    "    print(\"DataFrame 'df' not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a308d934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# ======== ANOVA ==========\n",
    "# =========================\n",
    "def anova_by_season(df: pd.DataFrame, price_col: str = \"SSP\"):\n",
    "    if price_col not in df.columns:\n",
    "        return pd.DataFrame()\n",
    "    groups = [g[price_col].dropna().values for _, g in df.groupby(\"season\")]\n",
    "    groups = [x for x in groups if len(x) > 1]\n",
    "    if len(groups) < 2:\n",
    "        return pd.DataFrame()\n",
    "    fstat, pval = stats.f_oneway(*groups)\n",
    "    out = pd.DataFrame([{\n",
    "        \"price_col\": price_col,\n",
    "        \"f_stat\": fstat,\n",
    "        \"p_value\": pval,\n",
    "        \"n_groups\": len(groups),\n",
    "        \"group_names\": list(df.groupby(\"season\").groups.keys()),\n",
    "        \"group_sizes\": [len(x) for x in groups],\n",
    "        \"group_means\": [float(np.nanmean(x)) for x in groups],\n",
    "    }])\n",
    "    return out\n",
    "\n",
    "if 'df' in locals() and not df.empty:\n",
    "    anova_ssp = anova_by_season(df, \"SSP\")\n",
    "    anova_sbp = anova_by_season(df, \"SBP\")\n",
    "    anova_all = pd.concat([anova_ssp, anova_sbp], ignore_index=True) if not anova_ssp.empty or not anova_sbp.empty else pd.DataFrame()\n",
    "    if not anova_all.empty:\n",
    "        print(\"ANOVA Results by Season:\")\n",
    "        display(anova_all)\n",
    "        write_bq(anova_all, \"anova_results_july2025\")\n",
    "else:\n",
    "    print(\"DataFrame 'df' not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae576e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# ====== CORRELATION ======\n",
    "# =========================\n",
    "def correlation_matrix(df: pd.DataFrame):\n",
    "    cols = [c for c in [\n",
    "        \"SSP\",\"SBP\",\"spread\",\"volume\"\n",
    "    ] if c in df.columns]\n",
    "    if not cols:\n",
    "        return pd.DataFrame()\n",
    "    cm = df[cols].corr(method=\"pearson\")\n",
    "    cm = cm.reset_index().rename(columns={\"index\": \"variable\"})\n",
    "    return cm\n",
    "\n",
    "def correlation_heatmap(df_corr: pd.DataFrame):\n",
    "    \"\"\"Save a correlation heatmap (matplotlib).\"\"\"\n",
    "    if df_corr.empty:\n",
    "        return \"\", None\n",
    "    \n",
    "    # Set the 'variable' column as the index for plotting\n",
    "    df_plot = df_corr.set_index('variable')\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    im = ax.imshow(df_plot.values, cmap='coolwarm', aspect=\"auto\")\n",
    "    \n",
    "    # Set ticks and labels\n",
    "    ax.set_xticks(np.arange(len(df_plot.columns)))\n",
    "    ax.set_yticks(np.arange(len(df_plot.index)))\n",
    "    ax.set_xticklabels(df_plot.columns, rotation=45, ha=\"right\")\n",
    "    ax.set_yticklabels(df_plot.index)\n",
    "    \n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    for i in range(len(df_plot.index)):\n",
    "        for j in range(len(df_plot.columns)):\n",
    "            text = ax.text(j, i, f\"{df_plot.iloc[i, j]:.2f}\",\n",
    "                           ha=\"center\", va=\"center\", color=\"w\")\n",
    "            \n",
    "    ax.set_title(\"Correlation Matrix (Pearson)\")\n",
    "    fig.tight_layout()\n",
    "    path = save_plot(fig, \"correlation_matrix_july2025.png\")\n",
    "    return path, fig\n",
    "\n",
    "if 'df' in locals() and not df.empty:\n",
    "    corr_df = correlation_matrix(df)\n",
    "    if not corr_df.empty:\n",
    "        print(\"Correlation Matrix:\")\n",
    "        display(corr_df)\n",
    "        write_bq(corr_df, \"correlation_matrix_july2025\")\n",
    "        \n",
    "        heatmap_path, heatmap_fig = correlation_heatmap(corr_df)\n",
    "        if heatmap_path:\n",
    "            print(f\"[PLOT] Correlation heatmap -> {heatmap_path}\")\n",
    "            plt.show()\n",
    "else:\n",
    "    print(\"DataFrame 'df' not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51f3c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# ====== REGRESSION 1 =====\n",
    "# =========================\n",
    "def regression_temperature_ssp(df: pd.DataFrame):\n",
    "    # This function requires a 'temperature' column which is not in the simplified loader.\n",
    "    # We will skip this for now. A more advanced version would join temperature data.\n",
    "    if not {\"SSP\",\"temperature\"}.issubset(df.columns):\n",
    "        print(\"[INFO] Skipping Temperature vs SSP regression: 'temperature' column not available in the loaded data.\")\n",
    "        return pd.DataFrame(), None\n",
    "    d = df[[\"SSP\", \"temperature\"]].dropna()\n",
    "    if len(d) < 10:\n",
    "        return pd.DataFrame(), None\n",
    "    X = sm.add_constant(d[\"temperature\"].astype(float))\n",
    "    y = d[\"SSP\"].astype(float)\n",
    "    model = sm.OLS(y, X).fit()\n",
    "    summary = {\n",
    "        \"model\": \"OLS_SSP_on_Temperature\",\n",
    "        \"n_obs\": int(model.nobs),\n",
    "        \"r_squared\": float(model.rsquared),\n",
    "        \"adj_r_squared\": float(model.rsquared_adj),\n",
    "        \"intercept\": float(model.params.get(\"const\", np.nan)),\n",
    "        \"slope_temperature\": float(model.params.get(\"temperature\", np.nan)),\n",
    "        \"p_intercept\": float(model.pvalues.get(\"const\", np.nan)),\n",
    "        \"p_temperature\": float(model.pvalues.get(\"temperature\", np.nan))\n",
    "    }\n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(7,5))\n",
    "    ax.scatter(d[\"temperature\"], d[\"SSP\"], s=8, alpha=0.5)\n",
    "    xgrid = np.linspace(d[\"temperature\"].min(), d[\"temperature\"].max(), 200)\n",
    "    yhat = summary[\"intercept\"] + summary[\"slope_temperature\"] * xgrid\n",
    "    ax.plot(xgrid, yhat, linewidth=2)\n",
    "    ax.set_xlabel(\"Temperature (°C)\")\n",
    "    ax.set_ylabel(\"SSP (£/MWh)\")\n",
    "    ax.set_title(\"Temperature vs SSP (OLS)\")\n",
    "    path = save_plot(fig, \"reg_temperature_ssp_july2025.png\")\n",
    "    summary[\"plot_path\"] = path\n",
    "    return pd.DataFrame([summary]), fig\n",
    "\n",
    "if 'df' in locals() and not df.empty:\n",
    "    reg_temp_df, reg_temp_plot = regression_temperature_ssp(df)\n",
    "    if not reg_temp_df.empty:\n",
    "        print(\"Regression Results (Temperature vs SSP):\")\n",
    "        display(reg_temp_df)\n",
    "        write_bq(reg_temp_df, \"regression_temperature_ssp_july2025\")\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"DataFrame 'df' not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e621b048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# ====== REGRESSION 2 =====\n",
    "# =========================\n",
    "def regression_volume_price(df: pd.DataFrame):\n",
    "    # Price elasticity proxy: SSP ~ log(volume)\n",
    "    need = {\"SSP\",\"volume\"}\n",
    "    if not need.issubset(df.columns):\n",
    "        return pd.DataFrame()\n",
    "    d = df[[\"SSP\",\"volume\"]].dropna()\n",
    "    d = d[d[\"volume\"] > 0]\n",
    "    if len(d) < 50:\n",
    "        return pd.DataFrame()\n",
    "    d = d.copy()\n",
    "    d[\"log_volume\"] = np.log(d[\"volume\"])\n",
    "    X = sm.add_constant(d[\"log_volume\"].astype(float))\n",
    "    y = d[\"SSP\"].astype(float)\n",
    "    model = sm.OLS(y, X).fit()\n",
    "    return pd.DataFrame([{\n",
    "        \"model\": \"OLS_SSP_on_logVolume\",\n",
    "        \"n_obs\": int(model.nobs),\n",
    "        \"r_squared\": float(model.rsquared),\n",
    "        \"adj_r_squared\": float(model.rsquared_adj),\n",
    "        \"intercept\": float(model.params.get(\"const\", np.nan)),\n",
    "        \"beta_log_volume\": float(model.params.get(\"log_volume\", np.nan)),\n",
    "        \"p_log_volume\": float(model.pvalues.get(\"log_volume\", np.nan)),\n",
    "    }])\n",
    "\n",
    "if 'df' in locals() and not df.empty:\n",
    "    reg_vol_df = regression_volume_price(df)\n",
    "    if not reg_vol_df.empty:\n",
    "        print(\"Regression Results (Volume vs Price):\")\n",
    "        display(reg_vol_df)\n",
    "        write_bq(reg_vol_df, \"regression_volume_price_july2025\")\n",
    "else:\n",
    "    print(\"DataFrame 'df' not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d7642d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# ==== DECOMPOSITION ======\n",
    "# =========================\n",
    "def seasonal_decomp(df: pd.DataFrame):\n",
    "    if \"SSP\" not in df.columns:\n",
    "        return pd.DataFrame(), None\n",
    "    d = df[[\"ts\",\"SSP\"]].dropna().copy()\n",
    "    d[\"ts\"] = pd.to_datetime(d[\"ts\"])\n",
    "    d = d.set_index(\"ts\").sort_index()\n",
    "    y = d[\"SSP\"].asfreq(\"30min\").interpolate(limit_direction=\"both\")\n",
    "    period = 48 * 7 # Weekly\n",
    "    try:\n",
    "        decomp = seasonal_decompose(y, model=\"additive\", period=period, two_sided=False, extrapolate_trend=\"freq\")\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Seasonal decomposition failed: {e}\")\n",
    "        return pd.DataFrame(), None\n",
    "    fig, axes = plt.subplots(4, 1, figsize=(12, 8), sharex=True)\n",
    "    axes[0].plot(y); axes[0].set_title(\"Observed\")\n",
    "    axes[1].plot(decomp.trend); axes[1].set_title(\"Trend\")\n",
    "    axes[2].plot(decomp.seasonal); axes[2].set_title(\"Seasonal\")\n",
    "    axes[3].plot(decomp.resid); axes[3].set_title(\"Residual\")\n",
    "    fig.tight_layout()\n",
    "    path = save_plot(fig, \"seasonal_decomposition_ssp_july2025.png\")\n",
    "    stats_out = pd.DataFrame([{\n",
    "        \"period\": int(period),\n",
    "        \"obs_count\": int(len(y)),\n",
    "        \"trend_var\": float(np.nanvar(decomp.trend.values)),\n",
    "        \"seasonal_var\": float(np.nanvar(decomp.seasonal.values)),\n",
    "        \"resid_var\": float(np.nanvar(decomp.resid.values)),\n",
    "        \"plot_path\": path\n",
    "    }])\n",
    "    return stats_out, fig\n",
    "\n",
    "if 'df' in locals() and not df.empty:\n",
    "    decomp_df, decomp_plot = seasonal_decomp(df)\n",
    "    if not decomp_df.empty:\n",
    "        print(\"Seasonal Decomposition Stats:\")\n",
    "        display(decomp_df)\n",
    "        write_bq(decomp_df, \"seasonal_decomposition_stats_july2025\")\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"DataFrame 'df' not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499c38bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# ===== EVENT IMPACT ======\n",
    "# =========================\n",
    "def outage_impact(df: pd.DataFrame):\n",
    "    \"\"\"Spread during system events vs non-events (proxy for outage/stress periods).\"\"\"\n",
    "    if \"spread\" not in df.columns or \"unplanned_event\" not in df.columns:\n",
    "        return pd.DataFrame()\n",
    "    d = df[[\"spread\",\"unplanned_event\"]].dropna()\n",
    "    if d.empty:\n",
    "        return pd.DataFrame()\n",
    "    a = d.loc[d[\"unplanned_event\"] == True,  \"spread\"].values\n",
    "    b = d.loc[d[\"unplanned_event\"] == False, \"spread\"].values\n",
    "    if len(a) < 5 or len(b) < 5:\n",
    "        return pd.DataFrame()\n",
    "    tstat, pval = stats.ttest_ind(a, b, equal_var=False, nan_policy=\"omit\")\n",
    "    return pd.DataFrame([{\n",
    "        \"metric\": \"spread_during_system_events\",\n",
    "        \"mean_with_event\": float(np.nanmean(a)),\n",
    "        \"mean_without_event\": float(np.nanmean(b)),\n",
    "        \"mean_diff\": float(np.nanmean(a) - np.nanmean(b)),\n",
    "        \"t_stat\": float(tstat),\n",
    "        \"p_value\": float(pval),\n",
    "        \"n_with\": int(np.isfinite(a).sum()),\n",
    "        \"n_without\": int(np.isfinite(b).sum())\n",
    "    }])\n",
    "\n",
    "if 'df' in locals() and not df.empty:\n",
    "    outage_df = outage_impact(df)\n",
    "    if not outage_df.empty:\n",
    "        print(\"Outage Impact on Spread:\")\n",
    "        display(outage_df)\n",
    "        write_bq(outage_df, \"outage_impact_results_july2025\")\n",
    "else:\n",
    "    print(\"DataFrame 'df' not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd5bef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# ===== NESO BEHAVIOR =====\n",
    "# =========================\n",
    "def neso_behavior(df: pd.DataFrame):\n",
    "    \"\"\"Spread ~ NESO balancing cost intensity.\"\"\"\n",
    "    # This function requires 'balancing_cost' which is not in the simplified loader.\n",
    "    # We will skip this for now.\n",
    "    if not {\"spread\",\"balancing_cost\"}.issubset(df.columns):\n",
    "        print(\"[INFO] Skipping NESO behavior analysis: 'balancing_cost' column not available.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    d = df[[\"spread\",\"balancing_cost\"]].dropna()\n",
    "    if len(d) < 50:\n",
    "        return pd.DataFrame()\n",
    "    X = sm.add_constant(d[\"balancing_cost\"].astype(float))\n",
    "    y = d[\"spread\"].astype(float)\n",
    "    model = sm.OLS(y, X).fit()\n",
    "    out = {\n",
    "        \"model\": \"Spread_on_BalancingCosts\",\n",
    "        \"n_obs\": int(model.nobs),\n",
    "        \"r_squared\": float(model.rsquared),\n",
    "        \"adj_r_squared\": float(model.rsquared_adj),\n",
    "        \"beta_balancing_cost\": float(model.params.get(\"balancing_cost\", np.nan)),\n",
    "        \"p_balancing_cost\": float(model.pvalues.get(\"balancing_cost\", np.nan)),\n",
    "    }\n",
    "    return pd.DataFrame([out])\n",
    "\n",
    "if 'df' in locals() and not df.empty:\n",
    "    neso_df = neso_behavior(df)\n",
    "    if not neso_df.empty:\n",
    "        print(\"NESO Behavior Analysis:\")\n",
    "        display(neso_df)\n",
    "        write_bq(neso_df, \"neso_behavior_results_july2025\")\n",
    "else:\n",
    "    print(\"DataFrame 'df' not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846e4b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_bid_offer_acceptances():\n",
    "    \"\"\"\n",
    "    Analyzes Bid-Offer Acceptances (BOD) data to create a summary table.\n",
    "    Includes a progress bar for the data download from BigQuery.\n",
    "    \"\"\"\n",
    "    client = bigquery.Client(project=PROJECT_ID, location=LOCATION)\n",
    "    \n",
    "    # This query assumes the table is named based on the month of data loaded.\n",
    "    # It calculates volumes, VWAP, and notional values per BMU per settlement period.\n",
    "    sql = f\"\"\"\n",
    "    SELECT\n",
    "        DATE(timeFrom) as date,\n",
    "        settlementPeriod as settlement_period,\n",
    "        bmUnit AS bmu_id,\n",
    "        SUM(CASE WHEN bidOfferLevel < 0 THEN -bidOfferLevel ELSE 0 END) AS accepted_bid_volume,\n",
    "        SUM(CASE WHEN bidOfferLevel > 0 THEN bidOfferLevel ELSE 0 END) AS accepted_offer_volume,\n",
    "        \n",
    "        SAFE_DIVIDE(\n",
    "            SUM(CASE WHEN bidOfferLevel < 0 THEN -bidOfferLevel * bidOfferPrice ELSE 0 END),\n",
    "            NULLIF(SUM(CASE WHEN bidOfferLevel < 0 THEN -bidOfferLevel ELSE 0 END), 0)\n",
    "        ) AS vwap_bid_price,\n",
    "        \n",
    "        SAFE_DIVIDE(\n",
    "            SUM(CASE WHEN bidOfferLevel > 0 THEN bidOfferLevel * bidOfferPrice ELSE 0 END),\n",
    "            NULLIF(SUM(CASE WHEN bidOfferLevel > 0 THEN bidOfferLevel ELSE 0 END), 0)\n",
    "        ) AS vwap_offer_price,\n",
    "        \n",
    "        SUM(CASE WHEN bidOfferLevel < 0 THEN -bidOfferLevel * bidOfferPrice ELSE 0 END) as notional_value_bid,\n",
    "        SUM(CASE WHEN bidOfferLevel > 0 THEN bidOfferLevel * bidOfferPrice ELSE 0 END) as notional_value_offer\n",
    "    FROM\n",
    "        `{PROJECT_ID}.{DATASET_SOURCE}.elexon_boalf_2025_07`\n",
    "    GROUP BY\n",
    "        date, settlement_period, bmu_id\n",
    "    ORDER BY\n",
    "        date, settlement_period, bmu_id;\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Executing Bid-Offer analysis query and downloading data...\")\n",
    "    \n",
    "    # The to_dataframe() call will display a progress bar in the notebook.\n",
    "    df = client.query(sql).to_dataframe(\n",
    "        create_bqstorage_client=True,\n",
    "        progress_bar_type='tqdm'\n",
    "    )\n",
    "    \n",
    "    if df.empty:\n",
    "        print(\"[WARN] No Bid-Offer data returned from BigQuery.\")\n",
    "    else:\n",
    "        print(f\"Successfully downloaded {len(df)} rows of Bid-Offer data.\")\n",
    "        \n",
    "    return df\n",
    "\n",
    "# Execute the analysis and display a preview of the results\n",
    "bod_analytics_df = analyze_bid_offer_acceptances()\n",
    "\n",
    "if not bod_analytics_df.empty:\n",
    "    print(\"\\nBid-Offer Acceptance Analysis Results (first 5 rows):\")\n",
    "    display(bod_analytics_df.head())\n",
    "    \n",
    "    # Write the results to a new BigQuery table\n",
    "    write_bq(bod_analytics_df, \"bod_analytics_july2025\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c2bc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accepted_volumes(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Plots the total accepted bid and offer volumes over time.\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        print(\"[INFO] Cannot plot volumes, DataFrame is empty.\")\n",
    "        return None\n",
    "\n",
    "    # Aggregate volumes by date for a clearer plot\n",
    "    df_agg = df.groupby('date').agg({\n",
    "        'accepted_bid_volume': 'sum',\n",
    "        'accepted_offer_volume': 'sum'\n",
    "    }).reset_index()\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15, 7))\n",
    "    \n",
    "    # Plotting as a stacked area chart\n",
    "    ax.stackplot(\n",
    "        df_agg['date'],\n",
    "        df_agg['accepted_bid_volume'],\n",
    "        df_agg['accepted_offer_volume'],\n",
    "        labels=['Accepted Bid Volume (MWh)', 'Accepted Offer Volume (MWh)'],\n",
    "        colors=['#2ca02c', '#d62728'], # Green for bids, Red for offers\n",
    "        alpha=0.7\n",
    "    )\n",
    "    \n",
    "    ax.set_title('Total Accepted Bid and Offer Volumes for July 2025', fontsize=16)\n",
    "    ax.set_xlabel('Date', fontsize=12)\n",
    "    ax.set_ylabel('Volume (MWh)', fontsize=12)\n",
    "    ax.legend(loc='upper left')\n",
    "    ax.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "    \n",
    "    # Improve date formatting on the x-axis\n",
    "    fig.autofmt_xdate()\n",
    "    \n",
    "    path = save_plot(fig, \"bod_accepted_volumes_july2025.png\")\n",
    "    if path:\n",
    "        print(f\"[PLOT] Accepted volumes chart saved to: {path}\")\n",
    "        \n",
    "    return fig\n",
    "\n",
    "# Generate and display the plot\n",
    "if 'bod_analytics_df' in locals() and not bod_analytics_df.empty:\n",
    "    volumes_plot = plot_accepted_volumes(bod_analytics_df)\n",
    "    if volumes_plot:\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"BOD analytics DataFrame not available. Please run the previous cell first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513410f6",
   "metadata": {},
   "source": [
    "---\n",
    "# **Analysis Module 1: Bid-Offer Acceptances (BOD)**\n",
    "\n",
    "**Primary Goal:** See accepted bids/offers and price/volume dynamics by Settlement Period and Balancing Mechanism Unit (BMU).\n",
    "\n",
    "**Best Charts:**\n",
    "*   **Line (stacked area):** Accepted bid/offer volume by SP/day.\n",
    "*   **Box/Violin:** Bid/offer prices per hour.\n",
    "*   **Scatter:** Price vs accepted volume.\n",
    "*   **Heatmap:** Average accepted price by hour × weekday.\n",
    "\n",
    "**Recommended Data Table (`bod_analytics`):**\n",
    "| Column | Description |\n",
    "| :--- | :--- |\n",
    "| `date` | The settlement date. |\n",
    "| `settlement_period` | The half-hourly settlement period (1-48). |\n",
    "| `bmu_id` | The Balancing Mechanism Unit ID. |\n",
    "| `accepted_bid_volume` | Total volume of accepted bids (MWh). |\n",
    "| `accepted_offer_volume`| Total volume of accepted offers (MWh). |\n",
    "| `vwap_bid_price` | Volume-weighted average price of accepted bids (£/MWh). |\n",
    "| `vwap_offer_price` | Volume-weighted average price of accepted offers (£/MWh). |\n",
    "| `notional_value_bid` | Total value of accepted bids (£). |\n",
    "| `notional_value_offer` | Total value of accepted offers (£). |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f303e851",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
