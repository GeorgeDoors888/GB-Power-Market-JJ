{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bea4fd0",
   "metadata": {},
   "source": [
    "# Elexon and NESO Data Downloader\n",
    "\n",
    "This notebook will help you download data from Elexon and NESO APIs that hasn't been downloaded yet. Instead of using ZeroMQ messaging (which was the focus of the previous version), we'll use a direct approach to:\n",
    "\n",
    "1. Connect to Elexon and NESO APIs\n",
    "2. Identify missing data\n",
    "3. Download the data efficiently\n",
    "4. Store it locally and/or in Google Cloud Storage\n",
    "\n",
    "Let's get started with the implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "da3177c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries for Elexon and NESO data download\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional\n",
    "import logging\n",
    "\n",
    "# Optional: Import Google Cloud libraries if you want to upload to GCS\n",
    "try:\n",
    "    from google.cloud import storage\n",
    "    gcs_available = True\n",
    "except ImportError:\n",
    "    gcs_available = False\n",
    "    print(\"Google Cloud Storage libraries not available. Data will be stored locally only.\")\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger('elexon_neso_downloader')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "68235e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elexon API Base: https://data.elexon.co.uk/bmrs/api/v1/\n",
      "NESO API Base: https://data.nationalgrideso.com/api/v1/\n",
      "GCS Available: True\n",
      "Local Data Directory: /Users/georgemajor/Jibber Jabber ChatGPT/8_august_jibber_jabber/downloaded_data\n",
      "Elexon API Key Configured: No\n",
      "NESO API Key Configured: No\n"
     ]
    }
   ],
   "source": [
    "# Configuration for Elexon and NESO APIs\n",
    "# Replace with your actual API keys and customize settings as needed\n",
    "\n",
    "# Elexon API Configuration\n",
    "ELEXON_API_BASE = 'https://data.elexon.co.uk/bmrs/api/v1/'\n",
    "ELEXON_API_KEY = os.getenv('ELEXON_API_KEY', '')  # Set your API key in environment variable or directly here\n",
    "\n",
    "# NESO API Configuration\n",
    "NESO_API_BASE = 'https://data.nationalgrideso.com/api/v1/'\n",
    "NESO_API_KEY = os.getenv('NESO_API_KEY', '')  # Set your API key if required\n",
    "\n",
    "# Google Cloud Configuration (if available)\n",
    "GCS_BUCKET_NAME = os.getenv('GCS_BUCKET_NAME', 'jibber-jabber-knowledge-bmrs-data')\n",
    "GCS_PROJECT_ID = os.getenv('GOOGLE_CLOUD_PROJECT', 'jibber-jabber-knowledge')\n",
    "\n",
    "# Local storage configuration\n",
    "LOCAL_DATA_DIR = Path('./downloaded_data')\n",
    "LOCAL_DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Print current configuration (without exposing API keys)\n",
    "print(f\"Elexon API Base: {ELEXON_API_BASE}\")\n",
    "print(f\"NESO API Base: {NESO_API_BASE}\")\n",
    "print(f\"GCS Available: {gcs_available}\")\n",
    "print(f\"Local Data Directory: {LOCAL_DATA_DIR.absolute()}\")\n",
    "print(f\"Elexon API Key Configured: {'Yes' if ELEXON_API_KEY else 'No'}\")\n",
    "print(f\"NESO API Key Configured: {'Yes' if NESO_API_KEY else 'No'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cafe847a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions for API requests with error handling and retries\n",
    "\n",
    "def create_session_with_retries(retries=3, backoff_factor=0.5):\n",
    "    \"\"\"Create a requests session with retry logic\"\"\"\n",
    "    session = requests.Session()\n",
    "    retry = requests.adapters.Retry(\n",
    "        total=retries,\n",
    "        backoff_factor=backoff_factor,\n",
    "        status_forcelist=[429, 500, 502, 503, 504],\n",
    "        allowed_methods=[\"GET\"]\n",
    "    )\n",
    "    adapter = requests.adapters.HTTPAdapter(max_retries=retry)\n",
    "    session.mount(\"http://\", adapter)\n",
    "    session.mount(\"https://\", adapter)\n",
    "    return session\n",
    "\n",
    "def make_elexon_api_request(endpoint, params=None):\n",
    "    \"\"\"Make a request to the Elexon API with error handling and retries\"\"\"\n",
    "    if params is None:\n",
    "        params = {}\n",
    "    \n",
    "    # Add API key to parameters if provided\n",
    "    if ELEXON_API_KEY:\n",
    "        params['APIKey'] = ELEXON_API_KEY\n",
    "    \n",
    "    url = f\"{ELEXON_API_BASE.rstrip('/')}/{endpoint.lstrip('/')}\"\n",
    "    session = create_session_with_retries()\n",
    "    \n",
    "    try:\n",
    "        response = session.get(url, params=params, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logger.error(f\"Error making request to Elexon API {url}: {e}\")\n",
    "        return None\n",
    "    \n",
    "def make_neso_api_request(endpoint, params=None):\n",
    "    \"\"\"Make a request to the NESO API with error handling and retries\"\"\"\n",
    "    if params is None:\n",
    "        params = {}\n",
    "    \n",
    "    # Add API key to parameters if provided\n",
    "    if NESO_API_KEY:\n",
    "        params['APIKey'] = NESO_API_KEY\n",
    "    \n",
    "    url = f\"{NESO_API_BASE.rstrip('/')}/{endpoint.lstrip('/')}\"\n",
    "    session = create_session_with_retries()\n",
    "    \n",
    "    try:\n",
    "        response = session.get(url, params=params, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logger.error(f\"Error making request to NESO API {url}: {e}\")\n",
    "        return None\n",
    "        \n",
    "def save_json_data(data, dataset_name, date_str, is_elexon=True):\n",
    "    \"\"\"Save JSON data to a file with proper naming convention\"\"\"\n",
    "    source = \"elexon\" if is_elexon else \"neso\"\n",
    "    filename = f\"{source}_{dataset_name}_{date_str}.json\"\n",
    "    filepath = LOCAL_DATA_DIR / filename\n",
    "    \n",
    "    with open(filepath, 'w') as f:\n",
    "        json.dump(data, f)\n",
    "        \n",
    "    logger.info(f\"Saved data to {filepath}\")\n",
    "    return filepath\n",
    "\n",
    "def save_csv_data(data, dataset_name, date_str, is_elexon=True):\n",
    "    \"\"\"Save data as CSV file\"\"\"\n",
    "    source = \"elexon\" if is_elexon else \"neso\"\n",
    "    filename = f\"{source}_{dataset_name}_{date_str}.csv\"\n",
    "    filepath = LOCAL_DATA_DIR / filename\n",
    "    \n",
    "    # Convert to DataFrame if it's not already\n",
    "    if not isinstance(data, pd.DataFrame):\n",
    "        df = pd.DataFrame(data)\n",
    "    else:\n",
    "        df = data\n",
    "        \n",
    "    df.to_csv(filepath, index=False)\n",
    "    logger.info(f\"Saved CSV data to {filepath}\")\n",
    "    return filepath\n",
    "\n",
    "def upload_to_gcs(local_filepath, bucket_name=GCS_BUCKET_NAME):\n",
    "    \"\"\"Upload a file to Google Cloud Storage if available\"\"\"\n",
    "    if not gcs_available:\n",
    "        logger.warning(\"GCS libraries not available. Skipping upload.\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        storage_client = storage.Client(project=GCS_PROJECT_ID)\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        blob_name = local_filepath.name\n",
    "        blob = bucket.blob(blob_name)\n",
    "        \n",
    "        blob.upload_from_filename(local_filepath)\n",
    "        logger.info(f\"Uploaded {local_filepath} to gs://{bucket_name}/{blob_name}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error uploading to GCS: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "217ef4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elexon API data download functions\n",
    "\n",
    "def get_elexon_datasets():\n",
    "    \"\"\"Get list of available datasets from Elexon API\"\"\"\n",
    "    data = make_elexon_api_request('datasets')\n",
    "    if data and 'data' in data:\n",
    "        return data['data']\n",
    "    return []\n",
    "\n",
    "def get_elexon_dataset_details(dataset_id):\n",
    "    \"\"\"Get details about a specific Elexon dataset\"\"\"\n",
    "    data = make_elexon_api_request(f'datasets/{dataset_id}')\n",
    "    if data:\n",
    "        return data\n",
    "    return None\n",
    "\n",
    "def check_data_exists(dataset_id, date_str):\n",
    "    \"\"\"Check if data already exists locally or in GCS for a given date\"\"\"\n",
    "    local_json_file = LOCAL_DATA_DIR / f\"elexon_{dataset_id}_{date_str}.json\"\n",
    "    local_csv_file = LOCAL_DATA_DIR / f\"elexon_{dataset_id}_{date_str}.csv\"\n",
    "    \n",
    "    if local_json_file.exists() or local_csv_file.exists():\n",
    "        return True\n",
    "    \n",
    "    # Could also check GCS if needed\n",
    "    return False\n",
    "\n",
    "def download_elexon_data(dataset_id, start_date, end_date=None, format='json'):\n",
    "    \"\"\"\n",
    "    Download data for a specific Elexon dataset between start_date and end_date\n",
    "    \n",
    "    Args:\n",
    "        dataset_id: The Elexon dataset ID (e.g., 'FUELINST')\n",
    "        start_date: Start date in 'YYYY-MM-DD' format\n",
    "        end_date: End date in 'YYYY-MM-DD' format (defaults to start_date if None)\n",
    "        format: Output format ('json' or 'csv')\n",
    "    \n",
    "    Returns:\n",
    "        List of downloaded file paths\n",
    "    \"\"\"\n",
    "    if end_date is None:\n",
    "        end_date = start_date\n",
    "        \n",
    "    start_dt = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    end_dt = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "    \n",
    "    downloaded_files = []\n",
    "    current_dt = start_dt\n",
    "    \n",
    "    # Iterate through each day in the date range\n",
    "    while current_dt <= end_dt:\n",
    "        date_str = current_dt.strftime(\"%Y-%m-%d\")\n",
    "        \n",
    "        # Skip if data already exists\n",
    "        if check_data_exists(dataset_id, date_str):\n",
    "            logger.info(f\"Data for {dataset_id} on {date_str} already exists. Skipping.\")\n",
    "            current_dt += timedelta(days=1)\n",
    "            continue\n",
    "        \n",
    "        # Set up parameters for the API request\n",
    "        params = {\n",
    "            'from': f\"{date_str}T00:00:00Z\",\n",
    "            'to': f\"{date_str}T23:59:59Z\",\n",
    "            'format': format\n",
    "        }\n",
    "        \n",
    "        logger.info(f\"Downloading {dataset_id} data for {date_str}\")\n",
    "        \n",
    "        # Make the API request\n",
    "        endpoint = f\"datasets/{dataset_id}\"\n",
    "        data = make_elexon_api_request(endpoint, params)\n",
    "        \n",
    "        if data:\n",
    "            # Save the data\n",
    "            if format.lower() == 'json':\n",
    "                filepath = save_json_data(data, dataset_id, date_str)\n",
    "            else:\n",
    "                # For CSV, we might need to extract the data differently\n",
    "                if 'data' in data:\n",
    "                    df = pd.DataFrame(data['data'])\n",
    "                    filepath = save_csv_data(df, dataset_id, date_str)\n",
    "                else:\n",
    "                    logger.warning(f\"Unexpected data format for {dataset_id}. Saving as JSON instead.\")\n",
    "                    filepath = save_json_data(data, dataset_id, date_str)\n",
    "            \n",
    "            # Upload to GCS if available\n",
    "            if gcs_available:\n",
    "                upload_to_gcs(filepath)\n",
    "                \n",
    "            downloaded_files.append(filepath)\n",
    "            \n",
    "        # Add a small delay to avoid API rate limits\n",
    "        time.sleep(1)\n",
    "        current_dt += timedelta(days=1)\n",
    "    \n",
    "    return downloaded_files\n",
    "\n",
    "# List of commonly used Elexon datasets\n",
    "COMMON_ELEXON_DATASETS = [\n",
    "    'FUELINST',  # Generation by fuel type\n",
    "    'DEMMF',     # Demand forecast\n",
    "    'B1610',     # Actual generation output per unit\n",
    "    'TEMP',      # Temperature data\n",
    "    'WINDFOR',   # Wind forecast\n",
    "    'BOD',       # Bid-offer data\n",
    "    'MID',       # Market index data\n",
    "    'SYSWARN'    # System warnings\n",
    "]\n",
    "\n",
    "def download_all_missing_elexon_data(datasets=COMMON_ELEXON_DATASETS, days_back=30):\n",
    "    \"\"\"\n",
    "    Download all missing data for specified Elexon datasets for the last N days\n",
    "    \n",
    "    Args:\n",
    "        datasets: List of dataset IDs to download\n",
    "        days_back: Number of days back from today to check\n",
    "    \"\"\"\n",
    "    end_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    start_date = (datetime.now() - timedelta(days=days_back)).strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for dataset_id in datasets:\n",
    "        logger.info(f\"Processing dataset {dataset_id}\")\n",
    "        try:\n",
    "            files = download_elexon_data(dataset_id, start_date, end_date)\n",
    "            results[dataset_id] = len(files)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error downloading {dataset_id}: {e}\")\n",
    "            results[dataset_id] = f\"Error: {str(e)}\"\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a3ba143e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NESO API data download functions\n",
    "\n",
    "def get_neso_datasets():\n",
    "    \"\"\"Get list of available datasets from NESO API\"\"\"\n",
    "    data = make_neso_api_request('datasets')\n",
    "    if data and 'data' in data:\n",
    "        return data['data']\n",
    "    return []\n",
    "\n",
    "def check_neso_data_exists(dataset_id, date_str):\n",
    "    \"\"\"Check if NESO data already exists locally or in GCS for a given date\"\"\"\n",
    "    local_json_file = LOCAL_DATA_DIR / f\"neso_{dataset_id}_{date_str}.json\"\n",
    "    local_csv_file = LOCAL_DATA_DIR / f\"neso_{dataset_id}_{date_str}.csv\"\n",
    "    \n",
    "    if local_json_file.exists() or local_csv_file.exists():\n",
    "        return True\n",
    "    \n",
    "    # Could also check GCS if needed\n",
    "    return False\n",
    "\n",
    "def download_neso_data(dataset_id, start_date, end_date=None, format='json'):\n",
    "    \"\"\"\n",
    "    Download data for a specific NESO dataset between start_date and end_date\n",
    "    \n",
    "    Args:\n",
    "        dataset_id: The NESO dataset ID\n",
    "        start_date: Start date in 'YYYY-MM-DD' format\n",
    "        end_date: End date in 'YYYY-MM-DD' format (defaults to start_date if None)\n",
    "        format: Output format ('json' or 'csv')\n",
    "    \n",
    "    Returns:\n",
    "        List of downloaded file paths\n",
    "    \"\"\"\n",
    "    if end_date is None:\n",
    "        end_date = start_date\n",
    "        \n",
    "    start_dt = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    end_dt = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "    \n",
    "    downloaded_files = []\n",
    "    current_dt = start_dt\n",
    "    \n",
    "    # Iterate through each day in the date range\n",
    "    while current_dt <= end_dt:\n",
    "        date_str = current_dt.strftime(\"%Y-%m-%d\")\n",
    "        \n",
    "        # Skip if data already exists\n",
    "        if check_neso_data_exists(dataset_id, date_str):\n",
    "            logger.info(f\"NESO data for {dataset_id} on {date_str} already exists. Skipping.\")\n",
    "            current_dt += timedelta(days=1)\n",
    "            continue\n",
    "        \n",
    "        # Set up parameters for the API request\n",
    "        params = {\n",
    "            'from': f\"{date_str}T00:00:00Z\",\n",
    "            'to': f\"{date_str}T23:59:59Z\",\n",
    "            'format': format\n",
    "        }\n",
    "        \n",
    "        logger.info(f\"Downloading NESO {dataset_id} data for {date_str}\")\n",
    "        \n",
    "        # Make the API request\n",
    "        endpoint = f\"datasets/{dataset_id}\"\n",
    "        data = make_neso_api_request(endpoint, params)\n",
    "        \n",
    "        if data:\n",
    "            # Save the data\n",
    "            if format.lower() == 'json':\n",
    "                filepath = save_json_data(data, dataset_id, date_str, is_elexon=False)\n",
    "            else:\n",
    "                # For CSV, we might need to extract the data differently\n",
    "                if 'data' in data:\n",
    "                    df = pd.DataFrame(data['data'])\n",
    "                    filepath = save_csv_data(df, dataset_id, date_str, is_elexon=False)\n",
    "                else:\n",
    "                    logger.warning(f\"Unexpected NESO data format for {dataset_id}. Saving as JSON instead.\")\n",
    "                    filepath = save_json_data(data, dataset_id, date_str, is_elexon=False)\n",
    "            \n",
    "            # Upload to GCS if available\n",
    "            if gcs_available:\n",
    "                upload_to_gcs(filepath)\n",
    "                \n",
    "            downloaded_files.append(filepath)\n",
    "            \n",
    "        # Add a small delay to avoid API rate limits\n",
    "        time.sleep(1)\n",
    "        current_dt += timedelta(days=1)\n",
    "    \n",
    "    return downloaded_files\n",
    "\n",
    "# List of commonly used NESO datasets\n",
    "COMMON_NESO_DATASETS = [\n",
    "    'demand-data',\n",
    "    'generation-mix',\n",
    "    'carbon-intensity',\n",
    "    'frequency-data',\n",
    "    'embedded-wind-and-solar-forecasts',\n",
    "    'transmission-system-warnings'\n",
    "]\n",
    "\n",
    "def download_all_missing_neso_data(datasets=COMMON_NESO_DATASETS, days_back=30):\n",
    "    \"\"\"\n",
    "    Download all missing data for specified NESO datasets for the last N days\n",
    "    \n",
    "    Args:\n",
    "        datasets: List of dataset IDs to download\n",
    "        days_back: Number of days back from today to check\n",
    "    \"\"\"\n",
    "    end_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    start_date = (datetime.now() - timedelta(days=days_back)).strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for dataset_id in datasets:\n",
    "        logger.info(f\"Processing NESO dataset {dataset_id}\")\n",
    "        try:\n",
    "            files = download_neso_data(dataset_id, start_date, end_date)\n",
    "            results[dataset_id] = len(files)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error downloading NESO {dataset_id}: {e}\")\n",
    "            results[dataset_id] = f\"Error: {str(e)}\"\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c585d5a1",
   "metadata": {},
   "source": [
    "## Download Missing Data from Elexon and NESO\n",
    "\n",
    "Now let's use the functions we've created to:\n",
    "\n",
    "1. List available datasets\n",
    "2. Check for missing data\n",
    "3. Download the missing data\n",
    "\n",
    "You can adjust the date ranges and datasets as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f295f03c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will check for missing Elexon data in these datasets: ['FUELINST', 'DEMMF', 'TEMP']\n",
      "Date range: Last 7 days\n"
     ]
    }
   ],
   "source": [
    "# 1. First, let's list available Elexon datasets\n",
    "# Uncomment to run (note: might be rate-limited without an API key)\n",
    "# elexon_datasets = get_elexon_datasets()\n",
    "# print(f\"Found {len(elexon_datasets)} Elexon datasets\")\n",
    "# for i, dataset in enumerate(elexon_datasets[:10]):  # Show first 10\n",
    "#     print(f\"{i+1}. {dataset.get('datasetId', 'Unknown')}: {dataset.get('name', 'No name')}\")\n",
    "\n",
    "# 2. Download missing data for common Elexon datasets for the last 7 days\n",
    "# Customize the list and date range as needed\n",
    "selected_elexon_datasets = ['FUELINST', 'DEMMF', 'TEMP']  # Subset for testing\n",
    "days_to_check = 7  # Last 7 days\n",
    "\n",
    "print(f\"Will check for missing Elexon data in these datasets: {selected_elexon_datasets}\")\n",
    "print(f\"Date range: Last {days_to_check} days\")\n",
    "\n",
    "# Uncomment to execute the download (ensure you have set ELEXON_API_KEY)\n",
    "# elexon_results = download_all_missing_elexon_data(\n",
    "#     datasets=selected_elexon_datasets,\n",
    "#     days_back=days_to_check\n",
    "# )\n",
    "# \n",
    "# print(\"\\nElexon download results:\")\n",
    "# for dataset, count in elexon_results.items():\n",
    "#     print(f\"{dataset}: {'Error' if isinstance(count, str) else f'{count} files downloaded'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b35bd44a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will check for missing NESO data in these datasets: ['actual_generation_per_unit', 'forecast_demand_published']\n",
      "Date range: Last 7 days\n"
     ]
    }
   ],
   "source": [
    "# 3. Download missing data for NESO datasets for the last 7 days\n",
    "# Customize the list and date range as needed\n",
    "selected_neso_datasets = ['actual_generation_per_unit', 'forecast_demand_published']  # Subset for testing\n",
    "days_to_check = 7  # Last 7 days\n",
    "\n",
    "print(f\"Will check for missing NESO data in these datasets: {selected_neso_datasets}\")\n",
    "print(f\"Date range: Last {days_to_check} days\")\n",
    "\n",
    "# Uncomment to execute the download (ensure you have set NESO_API_KEY)\n",
    "# neso_results = download_all_missing_neso_data(\n",
    "#     datasets=selected_neso_datasets,\n",
    "#     days_back=days_to_check\n",
    "# )\n",
    "# \n",
    "# print(\"\\nNESO download results:\")\n",
    "# for dataset, count in neso_results.items():\n",
    "#     print(f\"{dataset}: {'Error' if isinstance(count, str) else f'{count} files downloaded'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0d5f52f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Comprehensive download function to get all missing data from both systems\n",
    "def download_all_missing_data(elexon_datasets=None, neso_datasets=None, days_back=7, \n",
    "                             use_gcs=False, bucket_name=None):\n",
    "    \"\"\"\n",
    "    Download all missing data from both Elexon and NESO APIs\n",
    "    \n",
    "    Args:\n",
    "        elexon_datasets: List of Elexon datasets to check\n",
    "        neso_datasets: List of NESO datasets to check\n",
    "        days_back: Number of days to look back for missing data\n",
    "        use_gcs: Whether to also upload to Google Cloud Storage\n",
    "        bucket_name: GCS bucket name if use_gcs is True\n",
    "        \n",
    "    Returns:\n",
    "        dict: Summary of downloaded files for each dataset\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # If not specified, use common datasets for each system\n",
    "    if elexon_datasets is None:\n",
    "        elexon_datasets = ['DEMMF', 'FUELINST', 'TEMP', 'B1610', 'SYSWARN']\n",
    "    \n",
    "    if neso_datasets is None:\n",
    "        neso_datasets = ['actual_generation_per_unit', 'forecast_demand_published', \n",
    "                         'system_warnings', 'system_alerts']\n",
    "    \n",
    "    # Download Elexon data\n",
    "    print(f\"Downloading missing Elexon data for {len(elexon_datasets)} datasets...\")\n",
    "    elexon_results = download_all_missing_elexon_data(\n",
    "        datasets=elexon_datasets,\n",
    "        days_back=days_back,\n",
    "        use_gcs=use_gcs,\n",
    "        bucket_name=bucket_name\n",
    "    )\n",
    "    results['elexon'] = elexon_results\n",
    "    \n",
    "    # Download NESO data\n",
    "    print(f\"\\nDownloading missing NESO data for {len(neso_datasets)} datasets...\")\n",
    "    neso_results = download_all_missing_neso_data(\n",
    "        datasets=neso_datasets,\n",
    "        days_back=days_back,\n",
    "        use_gcs=use_gcs,\n",
    "        bucket_name=bucket_name\n",
    "    )\n",
    "    results['neso'] = neso_results\n",
    "    \n",
    "    # Print summary\n",
    "    total_files = 0\n",
    "    total_errors = 0\n",
    "    \n",
    "    print(\"\\n===== DOWNLOAD SUMMARY =====\")\n",
    "    print(\"\\nELEXON RESULTS:\")\n",
    "    for dataset, count in elexon_results.items():\n",
    "        if isinstance(count, str):\n",
    "            print(f\"  {dataset}: ERROR - {count}\")\n",
    "            total_errors += 1\n",
    "        else:\n",
    "            print(f\"  {dataset}: {count} files downloaded\")\n",
    "            total_files += count\n",
    "    \n",
    "    print(\"\\nNESO RESULTS:\")\n",
    "    for dataset, count in neso_results.items():\n",
    "        if isinstance(count, str):\n",
    "            print(f\"  {dataset}: ERROR - {count}\")\n",
    "            total_errors += 1\n",
    "        else:\n",
    "            print(f\"  {dataset}: {count} files downloaded\")\n",
    "            total_files += count\n",
    "    \n",
    "    print(f\"\\nTOTAL: {total_files} files downloaded, {total_errors} datasets with errors\")\n",
    "    return results\n",
    "\n",
    "# Uncomment to run a full download of missing data from both systems\n",
    "# all_results = download_all_missing_data(\n",
    "#     days_back=7,  # Last 7 days\n",
    "#     use_gcs=False  # Set to True if you want to upload to GCS\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8a98e735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Analyze and validate downloaded data\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def analyze_downloaded_data(base_dir='./data', days_back=7):\n",
    "    \"\"\"\n",
    "    Analyze the downloaded data files to verify completeness and quality\n",
    "    \n",
    "    Args:\n",
    "        base_dir: Base directory where data is stored\n",
    "        days_back: Number of days to analyze\n",
    "        \n",
    "    Returns:\n",
    "        dict: Summary statistics of the analyzed data\n",
    "    \"\"\"\n",
    "    summary = {\n",
    "        'elexon': {},\n",
    "        'neso': {},\n",
    "        'total_files': 0,\n",
    "        'total_size_mb': 0,\n",
    "        'date_coverage': {}\n",
    "    }\n",
    "    \n",
    "    # Calculate date range\n",
    "    end_date = datetime.now()\n",
    "    start_date = end_date - timedelta(days=days_back)\n",
    "    date_range = [(start_date + timedelta(days=i)).strftime('%Y-%m-%d') \n",
    "                 for i in range(days_back + 1)]\n",
    "    \n",
    "    # Initialize date coverage tracking\n",
    "    for date in date_range:\n",
    "        summary['date_coverage'][date] = {'elexon': {}, 'neso': {}}\n",
    "    \n",
    "    # Check Elexon data\n",
    "    elexon_dir = os.path.join(base_dir, 'elexon')\n",
    "    if os.path.exists(elexon_dir):\n",
    "        for dataset in os.listdir(elexon_dir):\n",
    "            dataset_path = os.path.join(elexon_dir, dataset)\n",
    "            if os.path.isdir(dataset_path):\n",
    "                files = [f for f in os.listdir(dataset_path) \n",
    "                        if os.path.isfile(os.path.join(dataset_path, f))]\n",
    "                \n",
    "                total_size = sum(os.path.getsize(os.path.join(dataset_path, f)) \n",
    "                                for f in files) / (1024 * 1024)  # MB\n",
    "                \n",
    "                # Count files per day\n",
    "                daily_counts = {}\n",
    "                for date in date_range:\n",
    "                    matching_files = [f for f in files if date in f]\n",
    "                    daily_counts[date] = len(matching_files)\n",
    "                    summary['date_coverage'][date]['elexon'][dataset] = len(matching_files)\n",
    "                \n",
    "                summary['elexon'][dataset] = {\n",
    "                    'file_count': len(files),\n",
    "                    'size_mb': round(total_size, 2),\n",
    "                    'daily_counts': daily_counts\n",
    "                }\n",
    "                \n",
    "                summary['total_files'] += len(files)\n",
    "                summary['total_size_mb'] += total_size\n",
    "    \n",
    "    # Check NESO data\n",
    "    neso_dir = os.path.join(base_dir, 'neso')\n",
    "    if os.path.exists(neso_dir):\n",
    "        for dataset in os.listdir(neso_dir):\n",
    "            dataset_path = os.path.join(neso_dir, dataset)\n",
    "            if os.path.isdir(dataset_path):\n",
    "                files = [f for f in os.listdir(dataset_path) \n",
    "                        if os.path.isfile(os.path.join(dataset_path, f))]\n",
    "                \n",
    "                total_size = sum(os.path.getsize(os.path.join(dataset_path, f)) \n",
    "                                for f in files) / (1024 * 1024)  # MB\n",
    "                \n",
    "                # Count files per day\n",
    "                daily_counts = {}\n",
    "                for date in date_range:\n",
    "                    matching_files = [f for f in files if date in f]\n",
    "                    daily_counts[date] = len(matching_files)\n",
    "                    summary['date_coverage'][date]['neso'][dataset] = len(matching_files)\n",
    "                \n",
    "                summary['neso'][dataset] = {\n",
    "                    'file_count': len(files),\n",
    "                    'size_mb': round(total_size, 2),\n",
    "                    'daily_counts': daily_counts\n",
    "                }\n",
    "                \n",
    "                summary['total_files'] += len(files)\n",
    "                summary['total_size_mb'] += total_size\n",
    "    \n",
    "    summary['total_size_mb'] = round(summary['total_size_mb'], 2)\n",
    "    return summary\n",
    "\n",
    "# Uncomment to analyze downloaded data\n",
    "# analysis_results = analyze_downloaded_data(days_back=7)\n",
    "# \n",
    "# print(f\"Total files downloaded: {analysis_results['total_files']}\")\n",
    "# print(f\"Total data size: {analysis_results['total_size_mb']} MB\")\n",
    "# \n",
    "# print(\"\\nELEXON DATASETS:\")\n",
    "# for dataset, info in analysis_results['elexon'].items():\n",
    "#     print(f\"  {dataset}: {info['file_count']} files ({info['size_mb']} MB)\")\n",
    "# \n",
    "# print(\"\\nNESO DATASETS:\")\n",
    "# for dataset, info in analysis_results['neso'].items():\n",
    "#     print(f\"  {dataset}: {info['file_count']} files ({info['size_mb']} MB)\")\n",
    "# \n",
    "# # Save analysis to file\n",
    "# with open('data_download_analysis.json', 'w') as f:\n",
    "#     json.dump(analysis_results, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8381d295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Visualize data coverage and completeness\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def visualize_data_coverage(analysis_results, days_back=7):\n",
    "    \"\"\"\n",
    "    Create visualizations of data coverage and completeness\n",
    "    \n",
    "    Args:\n",
    "        analysis_results: Results from analyze_downloaded_data\n",
    "        days_back: Number of days analyzed\n",
    "        \n",
    "    Returns:\n",
    "        None (displays visualizations)\n",
    "    \"\"\"\n",
    "    # Create date range for plotting\n",
    "    end_date = datetime.now()\n",
    "    start_date = end_date - timedelta(days=days_back)\n",
    "    date_range = [(start_date + timedelta(days=i)).strftime('%Y-%m-%d') \n",
    "                 for i in range(days_back + 1)]\n",
    "    \n",
    "    # 1. Overall data volume by system\n",
    "    elexon_size = sum(info['size_mb'] for info in analysis_results['elexon'].values())\n",
    "    neso_size = sum(info['size_mb'] for info in analysis_results['neso'].values())\n",
    "    \n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.bar(['Elexon', 'NESO'], [elexon_size, neso_size])\n",
    "    plt.title('Data Volume by System (MB)')\n",
    "    plt.ylabel('Size (MB)')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # 2. Files per dataset\n",
    "    dataset_counts = []\n",
    "    dataset_names = []\n",
    "    colors = []\n",
    "    \n",
    "    for dataset, info in analysis_results['elexon'].items():\n",
    "        dataset_counts.append(info['file_count'])\n",
    "        dataset_names.append(f\"Elexon: {dataset}\")\n",
    "        colors.append('skyblue')\n",
    "        \n",
    "    for dataset, info in analysis_results['neso'].items():\n",
    "        dataset_counts.append(info['file_count'])\n",
    "        dataset_names.append(f\"NESO: {dataset}\")\n",
    "        colors.append('lightgreen')\n",
    "    \n",
    "    plt.subplot(2, 2, 2)\n",
    "    bars = plt.barh(dataset_names, dataset_counts, color=colors)\n",
    "    plt.title('Files per Dataset')\n",
    "    plt.xlabel('Number of Files')\n",
    "    plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add count labels to bars\n",
    "    for bar in bars:\n",
    "        width = bar.get_width()\n",
    "        plt.text(width + 0.5, bar.get_y() + bar.get_height()/2, f'{int(width)}', \n",
    "                ha='left', va='center')\n",
    "    \n",
    "    # 3. Heatmap of daily coverage\n",
    "    # Prepare data for heatmap\n",
    "    elexon_datasets = list(analysis_results['elexon'].keys())\n",
    "    neso_datasets = list(analysis_results['neso'].keys())\n",
    "    \n",
    "    all_datasets = [(system, dataset) for system, datasets in [('Elexon', elexon_datasets), ('NESO', neso_datasets)] \n",
    "                   for dataset in datasets]\n",
    "    \n",
    "    heatmap_data = []\n",
    "    for date in date_range:\n",
    "        row = []\n",
    "        for system, dataset in all_datasets:\n",
    "            count = analysis_results['date_coverage'][date][system.lower()].get(dataset, 0)\n",
    "            row.append(count)\n",
    "        heatmap_data.append(row)\n",
    "    \n",
    "    plt.subplot(2, 1, 2)\n",
    "    sns.heatmap(\n",
    "        heatmap_data, \n",
    "        annot=True, \n",
    "        fmt='d', \n",
    "        cmap='YlGnBu',\n",
    "        xticklabels=[f\"{system}: {dataset}\" for system, dataset in all_datasets],\n",
    "        yticklabels=date_range,\n",
    "        cbar_kws={'label': 'Files'}\n",
    "    )\n",
    "    plt.title('Daily Data Coverage by Dataset')\n",
    "    plt.ylabel('Date')\n",
    "    plt.xlabel('Dataset')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 4. Create a completeness summary table\n",
    "    completeness_html = \"\"\"\n",
    "    <h3>Data Completeness Summary</h3>\n",
    "    <table border=\"1\" class=\"dataframe\">\n",
    "      <thead>\n",
    "        <tr style=\"text-align: center;\">\n",
    "          <th>System</th>\n",
    "          <th>Dataset</th>\n",
    "          <th>Files</th>\n",
    "          <th>Size (MB)</th>\n",
    "          <th>Days with Data</th>\n",
    "          <th>Completeness</th>\n",
    "        </tr>\n",
    "      </thead>\n",
    "      <tbody>\n",
    "    \"\"\"\n",
    "    \n",
    "    for system, datasets in [('Elexon', analysis_results['elexon']), ('NESO', analysis_results['neso'])]:\n",
    "        for dataset, info in datasets.items():\n",
    "            days_with_data = sum(1 for count in info['daily_counts'].values() if count > 0)\n",
    "            completeness_pct = round((days_with_data / len(date_range)) * 100, 1)\n",
    "            \n",
    "            # Color code based on completeness\n",
    "            if completeness_pct >= 90:\n",
    "                color = \"darkgreen\"\n",
    "            elif completeness_pct >= 70:\n",
    "                color = \"orange\"\n",
    "            else:\n",
    "                color = \"red\"\n",
    "                \n",
    "            completeness_html += f\"\"\"\n",
    "            <tr>\n",
    "              <td>{system}</td>\n",
    "              <td>{dataset}</td>\n",
    "              <td style=\"text-align: right;\">{info['file_count']}</td>\n",
    "              <td style=\"text-align: right;\">{info['size_mb']}</td>\n",
    "              <td style=\"text-align: right;\">{days_with_data} / {len(date_range)}</td>\n",
    "              <td style=\"text-align: right; color: {color}; font-weight: bold;\">{completeness_pct}%</td>\n",
    "            </tr>\n",
    "            \"\"\"\n",
    "    \n",
    "    completeness_html += \"\"\"\n",
    "      </tbody>\n",
    "    </table>\n",
    "    \"\"\"\n",
    "    \n",
    "    display(HTML(completeness_html))\n",
    "\n",
    "# Uncomment to visualize data coverage (requires having run the analysis above)\n",
    "# visualize_data_coverage(analysis_results, days_back=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4bd158de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Set up automated download scheduler for regular updates\n",
    "import schedule\n",
    "import time\n",
    "import threading\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "def setup_automated_download_scheduler(elexon_datasets=None, neso_datasets=None, \n",
    "                                      interval_hours=24, log_file=None):\n",
    "    \"\"\"\n",
    "    Set up an automated scheduler to download missing data at regular intervals\n",
    "    \n",
    "    Args:\n",
    "        elexon_datasets: List of Elexon datasets to download\n",
    "        neso_datasets: List of NESO datasets to download\n",
    "        interval_hours: Hours between download attempts\n",
    "        log_file: Path to log file (if None, prints to console)\n",
    "        \n",
    "    Returns:\n",
    "        threading.Thread: Background thread running the scheduler\n",
    "    \"\"\"\n",
    "    # Set up logging\n",
    "    if log_file:\n",
    "        logging.basicConfig(\n",
    "            filename=log_file,\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s'\n",
    "        )\n",
    "    else:\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s'\n",
    "        )\n",
    "    \n",
    "    # Default datasets if not specified\n",
    "    if elexon_datasets is None:\n",
    "        elexon_datasets = ['DEMMF', 'FUELINST', 'TEMP', 'B1610', 'SYSWARN']\n",
    "    \n",
    "    if neso_datasets is None:\n",
    "        neso_datasets = ['actual_generation_per_unit', 'forecast_demand_published', \n",
    "                         'system_warnings', 'system_alerts']\n",
    "    \n",
    "    # Define the download job\n",
    "    def download_job():\n",
    "        job_start_time = datetime.now()\n",
    "        logging.info(f\"Starting scheduled download job at {job_start_time}\")\n",
    "        \n",
    "        try:\n",
    "            # Download data from both systems\n",
    "            results = download_all_missing_data(\n",
    "                elexon_datasets=elexon_datasets,\n",
    "                neso_datasets=neso_datasets,\n",
    "                days_back=2  # Only check the last 2 days for regular updates\n",
    "            )\n",
    "            \n",
    "            # Count total files downloaded\n",
    "            elexon_count = sum(count for ds, count in results['elexon'].items() \n",
    "                              if not isinstance(count, str))\n",
    "            neso_count = sum(count for ds, count in results['neso'].items() \n",
    "                            if not isinstance(count, str))\n",
    "            \n",
    "            # Count errors\n",
    "            elexon_errors = sum(1 for ds, count in results['elexon'].items() \n",
    "                               if isinstance(count, str))\n",
    "            neso_errors = sum(1 for ds, count in results['neso'].items() \n",
    "                             if isinstance(count, str))\n",
    "            \n",
    "            job_duration = (datetime.now() - job_start_time).total_seconds() / 60.0\n",
    "            \n",
    "            logging.info(f\"Download job completed in {job_duration:.2f} minutes\")\n",
    "            logging.info(f\"Downloaded {elexon_count} Elexon files and {neso_count} NESO files\")\n",
    "            \n",
    "            if elexon_errors > 0 or neso_errors > 0:\n",
    "                logging.warning(f\"Encountered errors: {elexon_errors} Elexon errors, {neso_errors} NESO errors\")\n",
    "            \n",
    "            # Optionally run analysis\n",
    "            try:\n",
    "                analysis_results = analyze_downloaded_data(days_back=2)\n",
    "                logging.info(f\"Analysis complete: {analysis_results['total_files']} total files, \"\n",
    "                           f\"{analysis_results['total_size_mb']} MB\")\n",
    "                \n",
    "                # Save analysis to file with timestamp\n",
    "                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "                with open(f'data_download_analysis_{timestamp}.json', 'w') as f:\n",
    "                    json.dump(analysis_results, f, indent=2)\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error during analysis: {str(e)}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error during scheduled download: {str(e)}\")\n",
    "    \n",
    "    # Schedule the job\n",
    "    schedule.every(interval_hours).hours.do(download_job)\n",
    "    logging.info(f\"Scheduled download job to run every {interval_hours} hours\")\n",
    "    \n",
    "    # Run in background thread\n",
    "    def run_scheduler():\n",
    "        logging.info(\"Starting scheduler thread\")\n",
    "        while True:\n",
    "            schedule.run_pending()\n",
    "            time.sleep(60)  # Check every minute\n",
    "    \n",
    "    scheduler_thread = threading.Thread(target=run_scheduler, daemon=True)\n",
    "    scheduler_thread.start()\n",
    "    \n",
    "    return scheduler_thread\n",
    "\n",
    "# Example usage (uncomment to activate)\n",
    "# Set up a scheduler to run every 6 hours\n",
    "# timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "# scheduler_thread = setup_automated_download_scheduler(\n",
    "#     interval_hours=6,\n",
    "#     log_file=f'data_download_scheduler_{timestamp}.log'\n",
    "# )\n",
    "# \n",
    "# print(f\"Scheduler started at {datetime.now()}\")\n",
    "# print(\"Will download missing data every 6 hours\")\n",
    "# print(\"Run this notebook cell again to restart scheduler if needed\")\n",
    "# \n",
    "# # Note: The scheduler runs in a daemon thread and will stop when the notebook kernel is shut down\n",
    "# # To manually run a download immediately:\n",
    "# # download_job()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216833fb",
   "metadata": {},
   "source": [
    "## Conclusion and Next Steps\n",
    "\n",
    "This notebook provides a comprehensive solution for downloading, analyzing, and managing missing data from the Elexon and NESO APIs. Here's a summary of what you can do with it:\n",
    "\n",
    "### Key Features\n",
    "1. **Download Missing Data**: Identify and download data that hasn't been collected yet from both APIs\n",
    "2. **Customizable Dataset Selection**: Choose which datasets to focus on\n",
    "3. **Flexible Date Ranges**: Specify how far back to check for missing data\n",
    "4. **Local and Cloud Storage**: Option to store data locally and/or in Google Cloud Storage\n",
    "5. **Data Analysis**: Analyze downloaded data for completeness and coverage\n",
    "6. **Data Visualization**: Visualize data coverage with charts and tables\n",
    "7. **Automated Scheduling**: Set up regular downloads to keep data current\n",
    "\n",
    "### Getting Started\n",
    "1. First, make sure your API keys are set (set `ELEXON_API_KEY` and `NESO_API_KEY` environment variables or update in the config section)\n",
    "2. Start with small test runs using the example cells (sections 2-3)\n",
    "3. Once verified, you can use the comprehensive downloader (section 4)\n",
    "4. Analyze your downloaded data (section 5)\n",
    "5. Create visualizations to check coverage (section 6)\n",
    "6. Optionally set up automated scheduling (section 7)\n",
    "\n",
    "### Customization Options\n",
    "- Modify the dataset lists to focus on specific data types\n",
    "- Adjust date ranges to backfill older data or focus on recent periods\n",
    "- Configure storage paths and GCS options for your environment\n",
    "- Customize the scheduler timing for your operational needs\n",
    "\n",
    "### Troubleshooting\n",
    "- If you encounter API rate limits, increase the retry delays and backoff settings\n",
    "- For large datasets, consider running downloads in smaller batches\n",
    "- Check the error logging for specific API errors\n",
    "- Verify your API keys and credentials if authentication fails\n",
    "\n",
    "Feel free to modify and extend this notebook to suit your specific requirements!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "74714cca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nreview_results = review_last_6_days_with_api_keys(\\n    elexon_api_key=\"YOUR_ELEXON_API_KEY_HERE\",  # Replace with your actual Elexon API key\\n    neso_api_key=\"YOUR_NESO_API_KEY_HERE\",      # Replace with your actual NESO API key\\n    elexon_datasets=[\\'DEMMF\\', \\'FUELINST\\'],      # Subset for testing\\n    neso_datasets=[\\'actual_generation_per_unit\\'],\\n    download_missing=True,\\n    analyze_results=True\\n)\\n'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Review and download data from the last 6 days with direct API key integration\n",
    "\n",
    "def review_last_6_days_with_api_keys(elexon_api_key=None, neso_api_key=None, \n",
    "                                   elexon_datasets=None, neso_datasets=None,\n",
    "                                   download_missing=True, analyze_results=True,\n",
    "                                   use_gcs=False):\n",
    "    \"\"\"\n",
    "    Comprehensive function to review data from the last 6 days with direct API key integration.\n",
    "    \n",
    "    Args:\n",
    "        elexon_api_key: Your Elexon API key (if None, will use the environment variable)\n",
    "        neso_api_key: Your NESO API key (if None, will use the environment variable)\n",
    "        elexon_datasets: List of Elexon datasets to check (if None, uses common datasets)\n",
    "        neso_datasets: List of NESO datasets to check (if None, uses common datasets)\n",
    "        download_missing: Whether to download missing data (True) or just report (False)\n",
    "        analyze_results: Whether to analyze and visualize the results\n",
    "        use_gcs: Whether to upload to Google Cloud Storage\n",
    "        \n",
    "    Returns:\n",
    "        Dict containing review results and downloaded data summary\n",
    "    \"\"\"\n",
    "    # Update API keys if provided\n",
    "    global ELEXON_API_KEY, NESO_API_KEY\n",
    "    \n",
    "    if elexon_api_key:\n",
    "        ELEXON_API_KEY = elexon_api_key\n",
    "        print(f\"Using provided Elexon API key: {elexon_api_key[:5]}...\")\n",
    "    else:\n",
    "        print(f\"Using existing Elexon API key configuration\")\n",
    "        \n",
    "    if neso_api_key:\n",
    "        NESO_API_KEY = neso_api_key\n",
    "        print(f\"Using provided NESO API key: {neso_api_key[:5]}...\")\n",
    "    else:\n",
    "        print(f\"Using existing NESO API key configuration\")\n",
    "    \n",
    "    # Default datasets if not specified\n",
    "    if elexon_datasets is None:\n",
    "        elexon_datasets = ['DEMMF', 'FUELINST', 'TEMP', 'B1610', 'SYSWARN']\n",
    "    \n",
    "    if neso_datasets is None:\n",
    "        neso_datasets = ['actual_generation_per_unit', 'forecast_demand_published', \n",
    "                         'system_warnings', 'system_alerts']\n",
    "    \n",
    "    # Calculate date range for last 6 days\n",
    "    end_date = datetime.now()\n",
    "    start_date = end_date - timedelta(days=6)\n",
    "    date_range = [(start_date + timedelta(days=i)).strftime('%Y-%m-%d') \n",
    "                 for i in range(7)]  # Include today\n",
    "    \n",
    "    print(f\"Reviewing data from {date_range[0]} to {date_range[-1]}\")\n",
    "    \n",
    "    results = {\n",
    "        'elexon': {\n",
    "            'datasets': elexon_datasets,\n",
    "            'date_range': date_range,\n",
    "            'existing_data': {},\n",
    "            'missing_data': {},\n",
    "            'downloaded': {}\n",
    "        },\n",
    "        'neso': {\n",
    "            'datasets': neso_datasets,\n",
    "            'date_range': date_range,\n",
    "            'existing_data': {},\n",
    "            'missing_data': {},\n",
    "            'downloaded': {}\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Check Elexon data\n",
    "    print(\"\\nChecking Elexon datasets...\")\n",
    "    for dataset in elexon_datasets:\n",
    "        print(f\"  Dataset: {dataset}\")\n",
    "        results['elexon']['existing_data'][dataset] = []\n",
    "        results['elexon']['missing_data'][dataset] = []\n",
    "        \n",
    "        for date in date_range:\n",
    "            if check_data_exists(dataset, date):\n",
    "                results['elexon']['existing_data'][dataset].append(date)\n",
    "                print(f\"     {date} - Data exists\")\n",
    "            else:\n",
    "                results['elexon']['missing_data'][dataset].append(date)\n",
    "                print(f\"     {date} - Data missing\")\n",
    "    \n",
    "    # Check NESO data\n",
    "    print(\"\\nChecking NESO datasets...\")\n",
    "    for dataset in neso_datasets:\n",
    "        print(f\"  Dataset: {dataset}\")\n",
    "        results['neso']['existing_data'][dataset] = []\n",
    "        results['neso']['missing_data'][dataset] = []\n",
    "        \n",
    "        for date in date_range:\n",
    "            if check_neso_data_exists(dataset, date):\n",
    "                results['neso']['existing_data'][dataset].append(date)\n",
    "                print(f\"     {date} - Data exists\")\n",
    "            else:\n",
    "                results['neso']['missing_data'][dataset].append(date)\n",
    "                print(f\"     {date} - Data missing\")\n",
    "    \n",
    "    # Download missing data if requested\n",
    "    if download_missing:\n",
    "        print(\"\\nDownloading missing Elexon data...\")\n",
    "        for dataset in elexon_datasets:\n",
    "            missing_dates = results['elexon']['missing_data'][dataset]\n",
    "            if not missing_dates:\n",
    "                print(f\"  {dataset}: No missing data to download\")\n",
    "                results['elexon']['downloaded'][dataset] = 0\n",
    "                continue\n",
    "                \n",
    "            print(f\"  {dataset}: Downloading {len(missing_dates)} missing dates\")\n",
    "            \n",
    "            downloaded = 0\n",
    "            for date in missing_dates:\n",
    "                try:\n",
    "                    files = download_elexon_data(dataset, date, date)\n",
    "                    if files:\n",
    "                        downloaded += len(files)\n",
    "                except Exception as e:\n",
    "                    print(f\"    Error downloading {dataset} for {date}: {e}\")\n",
    "            \n",
    "            results['elexon']['downloaded'][dataset] = downloaded\n",
    "            print(f\"    Downloaded {downloaded} files for {dataset}\")\n",
    "        \n",
    "        print(\"\\nDownloading missing NESO data...\")\n",
    "        for dataset in neso_datasets:\n",
    "            missing_dates = results['neso']['missing_data'][dataset]\n",
    "            if not missing_dates:\n",
    "                print(f\"  {dataset}: No missing data to download\")\n",
    "                results['neso']['downloaded'][dataset] = 0\n",
    "                continue\n",
    "                \n",
    "            print(f\"  {dataset}: Downloading {len(missing_dates)} missing dates\")\n",
    "            \n",
    "            downloaded = 0\n",
    "            for date in missing_dates:\n",
    "                try:\n",
    "                    files = download_neso_data(dataset, date, date)\n",
    "                    if files:\n",
    "                        downloaded += len(files)\n",
    "                except Exception as e:\n",
    "                    print(f\"    Error downloading {dataset} for {date}: {e}\")\n",
    "            \n",
    "            results['neso']['downloaded'][dataset] = downloaded\n",
    "            print(f\"    Downloaded {downloaded} files for {dataset}\")\n",
    "    \n",
    "    # Analyze and visualize if requested\n",
    "    if analyze_results and download_missing:\n",
    "        print(\"\\nAnalyzing downloaded data...\")\n",
    "        try:\n",
    "            analysis_results = analyze_downloaded_data(days_back=6)\n",
    "            results['analysis'] = analysis_results\n",
    "            \n",
    "            # Display summary\n",
    "            print(f\"\\nData Summary:\")\n",
    "            print(f\"  Total files: {analysis_results['total_files']}\")\n",
    "            print(f\"  Total size: {analysis_results['total_size_mb']} MB\")\n",
    "            \n",
    "            # Visualize if possible\n",
    "            try:\n",
    "                visualize_data_coverage(analysis_results, days_back=6)\n",
    "            except Exception as e:\n",
    "                print(f\"Could not create visualizations: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error during analysis: {e}\")\n",
    "    \n",
    "    # Save results to file\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    result_file = f'data_review_{timestamp}.json'\n",
    "    \n",
    "    with open(result_file, 'w') as f:\n",
    "        # Convert any datetime objects to strings for JSON serialization\n",
    "        json_results = json.dumps(results, default=str, indent=2)\n",
    "        f.write(json_results)\n",
    "    \n",
    "    print(f\"\\nReview complete! Results saved to {result_file}\")\n",
    "    return results\n",
    "\n",
    "# Example usage with direct API keys\n",
    "# Uncomment and modify the API keys below to run the review\n",
    "\"\"\"\n",
    "review_results = review_last_6_days_with_api_keys(\n",
    "    elexon_api_key=\"YOUR_ELEXON_API_KEY_HERE\",  # Replace with your actual Elexon API key\n",
    "    neso_api_key=\"YOUR_NESO_API_KEY_HERE\",      # Replace with your actual NESO API key\n",
    "    elexon_datasets=['DEMMF', 'FUELINST'],      # Subset for testing\n",
    "    neso_datasets=['actual_generation_per_unit'],\n",
    "    download_missing=True,\n",
    "    analyze_results=True\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d28050a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nreview_results = review_last_6_days_with_api_keys(\\n    elexon_api_key=ELEXON_API_KEY_DIRECT,\\n    neso_api_key=NESO_API_KEY_DIRECT,\\n    # Customize datasets as needed (using smaller subsets for initial testing is recommended)\\n    elexon_datasets=[\\'DEMMF\\', \\'FUELINST\\', \\'TEMP\\'],  # Common Elexon datasets\\n    neso_datasets=[\\'actual_generation_per_unit\\', \\'forecast_demand_published\\'],  # Common NESO datasets\\n    download_missing=True,  # Set to False to only check without downloading\\n    analyze_results=True,   # Set to True to get visualizations\\n    use_gcs=False           # Set to True if you want to upload to Google Cloud Storage\\n)\\n\\n# Print total counts of downloaded files\\nif \\'review_results\\' in locals():\\n    elexon_total = sum(review_results[\\'elexon\\'][\\'downloaded\\'].values())\\n    neso_total = sum(review_results[\\'neso\\'][\\'downloaded\\'].values())\\n    print(f\"Total files downloaded: {elexon_total + neso_total}\")\\n    print(f\"  - Elexon: {elexon_total} files\")\\n    print(f\"  - NESO: {neso_total} files\")\\n'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Execute the review for the last 6 days with direct API key configuration\n",
    "# This will both analyze existing data and download any missing data\n",
    "\n",
    "# Set your API keys directly here (this is more convenient but less secure)\n",
    "# For production, consider using environment variables instead\n",
    "ELEXON_API_KEY_DIRECT = \"your_elexon_api_key_here\"  # Replace with your actual API key\n",
    "NESO_API_KEY_DIRECT = \"your_neso_api_key_here\"      # Replace with your actual API key\n",
    "\n",
    "# Execute the review with the specified API keys\n",
    "# Uncomment to run\n",
    "\"\"\"\n",
    "review_results = review_last_6_days_with_api_keys(\n",
    "    elexon_api_key=ELEXON_API_KEY_DIRECT,\n",
    "    neso_api_key=NESO_API_KEY_DIRECT,\n",
    "    # Customize datasets as needed (using smaller subsets for initial testing is recommended)\n",
    "    elexon_datasets=['DEMMF', 'FUELINST', 'TEMP'],  # Common Elexon datasets\n",
    "    neso_datasets=['actual_generation_per_unit', 'forecast_demand_published'],  # Common NESO datasets\n",
    "    download_missing=True,  # Set to False to only check without downloading\n",
    "    analyze_results=True,   # Set to True to get visualizations\n",
    "    use_gcs=False           # Set to True if you want to upload to Google Cloud Storage\n",
    ")\n",
    "\n",
    "# Print total counts of downloaded files\n",
    "if 'review_results' in locals():\n",
    "    elexon_total = sum(review_results['elexon']['downloaded'].values())\n",
    "    neso_total = sum(review_results['neso']['downloaded'].values())\n",
    "    print(f\"Total files downloaded: {elexon_total + neso_total}\")\n",
    "    print(f\"  - Elexon: {elexon_total} files\")\n",
    "    print(f\"  - NESO: {neso_total} files\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "58ad12eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Load API keys from environment variables or .env file\\napi_keys = configure_api_keys_from_env()\\n\\n# Run the review using keys from environment\\nif api_keys[\\'elexon\\'] or api_keys[\\'neso\\']:\\n    review_results = review_last_6_days_with_api_keys(\\n        elexon_api_key=api_keys[\\'elexon\\'],\\n        neso_api_key=api_keys[\\'neso\\'],\\n        # Customize datasets as needed\\n        elexon_datasets=[\\'DEMMF\\', \\'FUELINST\\'],\\n        neso_datasets=[\\'actual_generation_per_unit\\'],\\n        download_missing=True,\\n        analyze_results=True\\n    )\\nelse:\\n    print(\"No API keys found. Please set environment variables or use direct API keys.\")\\n'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A more secure approach to handle API keys using environment variables\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "def configure_api_keys_from_env(env_file='.env'):\n",
    "    \"\"\"\n",
    "    Configure API keys from environment variables or .env file\n",
    "    This is a more secure approach than hardcoding keys in the notebook\n",
    "    \n",
    "    Args:\n",
    "        env_file: Path to .env file (default: '.env')\n",
    "        \n",
    "    Returns:\n",
    "        dict: API keys loaded from environment\n",
    "    \"\"\"\n",
    "    # Try to load from .env file if it exists\n",
    "    if os.path.exists(env_file):\n",
    "        load_dotenv(env_file)\n",
    "        print(f\"Loaded environment variables from {env_file}\")\n",
    "    else:\n",
    "        print(f\"No {env_file} file found, using existing environment variables\")\n",
    "    \n",
    "    # Get API keys from environment variables\n",
    "    elexon_key = os.getenv('ELEXON_API_KEY')\n",
    "    neso_key = os.getenv('NESO_API_KEY')\n",
    "    \n",
    "    # Check if keys are available\n",
    "    keys = {\n",
    "        'elexon': elexon_key,\n",
    "        'neso': neso_key\n",
    "    }\n",
    "    \n",
    "    for source, key in keys.items():\n",
    "        if key:\n",
    "            # Show first few characters for verification\n",
    "            print(f\"{source.upper()} API Key: {key[:5]}... (found)\")\n",
    "        else:\n",
    "            print(f\"{source.upper()} API Key: Not found in environment\")\n",
    "    \n",
    "    return keys\n",
    "\n",
    "# Example usage with environment variables\n",
    "# First, create a .env file with your API keys or set them in your environment:\n",
    "# ELEXON_API_KEY=your_elexon_api_key\n",
    "# NESO_API_KEY=your_neso_api_key\n",
    "\n",
    "# Uncomment to load and use API keys from environment\n",
    "\"\"\"\n",
    "# Load API keys from environment variables or .env file\n",
    "api_keys = configure_api_keys_from_env()\n",
    "\n",
    "# Run the review using keys from environment\n",
    "if api_keys['elexon'] or api_keys['neso']:\n",
    "    review_results = review_last_6_days_with_api_keys(\n",
    "        elexon_api_key=api_keys['elexon'],\n",
    "        neso_api_key=api_keys['neso'],\n",
    "        # Customize datasets as needed\n",
    "        elexon_datasets=['DEMMF', 'FUELINST'],\n",
    "        neso_datasets=['actual_generation_per_unit'],\n",
    "        download_missing=True,\n",
    "        analyze_results=True\n",
    "    )\n",
    "else:\n",
    "    print(\"No API keys found. Please set environment variables or use direct API keys.\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5376f169",
   "metadata": {},
   "source": [
    "## Reviewing Data for the Last 6 Days with API Key Management\n",
    "\n",
    "This section provides functions to review data for the last 6 days from both Elexon and NESO APIs. You have two options for managing API keys:\n",
    "\n",
    "### Option 1: Direct API Key Specification\n",
    "- **Advantage**: Quick and simple for testing\n",
    "- **Disadvantage**: Less secure as keys are stored in the notebook\n",
    "- **Best for**: Development and testing environments\n",
    "\n",
    "### Option 2: Environment Variables (Recommended for Production)\n",
    "- **Advantage**: More secure, keys not stored in the notebook\n",
    "- **Disadvantage**: Requires setting up environment variables or a .env file\n",
    "- **Best for**: Production environments and shared code\n",
    "\n",
    "### How to Use:\n",
    "\n",
    "1. **Choose your API key approach**:\n",
    "   - Direct: Edit the `ELEXON_API_KEY_DIRECT` and `NESO_API_KEY_DIRECT` variables\n",
    "   - Environment: Create a `.env` file or set environment variables\n",
    "\n",
    "2. **Run the review function**:\n",
    "   - Uncomment the appropriate code block\n",
    "   - Customize dataset selection if needed\n",
    "   - Set download_missing=True to download missing data\n",
    "   - Set analyze_results=True to get visualizations\n",
    "\n",
    "3. **Interpret the results**:\n",
    "   - Review will show existing and missing data for each dataset\n",
    "   - If download_missing=True, it will download missing data\n",
    "   - If analyze_results=True, it will create visualizations\n",
    "   - Results are saved to a JSON file for later reference\n",
    "\n",
    "### Installing Required Packages:\n",
    "If you're using environment variables with .env file, you'll need to install:\n",
    "```\n",
    "pip install python-dotenv\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5541fe57",
   "metadata": {},
   "source": [
    "## Six-Day Review and Download with API Keys\n",
    "\n",
    "The following cell provides a practical implementation that:\n",
    "1. Reviews code and data availability over the last 6 days\n",
    "2. Includes API keys directly in the code for immediate execution\n",
    "3. Downloads missing data automatically\n",
    "4. Generates a detailed report of what was found and downloaded\n",
    "\n",
    "**Security Note**: For a production environment, consider using environment variables instead of placing API keys directly in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f9b0d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elexon API Key configured: No - please update the key\n",
      "NESO API Key configured: No - please update the key\n",
      "Reviewing data from 2025-08-18 to 2025-08-24\n",
      "\n",
      "=== CHECKING ELEXON DATA ===\n",
      "\n",
      "Dataset: DEMMF\n",
      "   2025-08-18 - Data missing\n",
      "   2025-08-19 - Data missing\n",
      "   2025-08-20 - Data missing\n",
      "   2025-08-21 - Data missing\n",
      "   2025-08-22 - Data missing\n",
      "   2025-08-23 - Data missing\n",
      "   2025-08-24 - Data missing\n",
      "\n",
      "Dataset: FUELINST\n",
      "   2025-08-18 - Data missing\n",
      "   2025-08-19 - Data missing\n",
      "   2025-08-20 - Data missing\n",
      "   2025-08-21 - Data missing\n",
      "   2025-08-22 - Data missing\n",
      "   2025-08-23 - Data missing\n",
      "   2025-08-24 - Data missing\n",
      "\n",
      "Dataset: TEMP\n",
      "   2025-08-18 - Data missing\n",
      "   2025-08-19 - Data missing\n",
      "   2025-08-20 - Data missing\n",
      "   2025-08-21 - Data missing\n",
      "   2025-08-22 - Data missing\n",
      "   2025-08-23 - Data missing\n",
      "   2025-08-24 - Data missing\n",
      "\n",
      "Dataset: B1610\n",
      "   2025-08-18 - Data missing\n",
      "   2025-08-19 - Data missing\n",
      "   2025-08-20 - Data missing\n",
      "   2025-08-21 - Data missing\n",
      "   2025-08-22 - Data missing\n",
      "   2025-08-23 - Data missing\n",
      "   2025-08-24 - Data missing\n",
      "\n",
      "Dataset: SYSWARN\n",
      "   2025-08-18 - Data missing\n",
      "   2025-08-19 - Data missing\n",
      "   2025-08-20 - Data missing\n",
      "   2025-08-21 - Data missing\n",
      "   2025-08-22 - Data missing\n",
      "   2025-08-23 - Data missing\n",
      "   2025-08-24 - Data missing\n",
      "\n",
      "=== CHECKING NESO DATA ===\n",
      "\n",
      "Dataset: actual_generation_per_unit\n",
      "   2025-08-18 - Data missing\n",
      "   2025-08-19 - Data missing\n",
      "   2025-08-20 - Data missing\n",
      "   2025-08-21 - Data missing\n",
      "   2025-08-22 - Data missing\n",
      "   2025-08-23 - Data missing\n",
      "   2025-08-24 - Data missing\n",
      "\n",
      "Dataset: forecast_demand_published\n",
      "   2025-08-18 - Data missing\n",
      "   2025-08-19 - Data missing\n",
      "   2025-08-20 - Data missing\n",
      "   2025-08-21 - Data missing\n",
      "   2025-08-22 - Data missing\n",
      "   2025-08-23 - Data missing\n",
      "   2025-08-24 - Data missing\n",
      "\n",
      "Dataset: system_warnings\n",
      "   2025-08-18 - Data missing\n",
      "   2025-08-19 - Data missing\n",
      "   2025-08-20 - Data missing\n",
      "   2025-08-21 - Data missing\n",
      "   2025-08-22 - Data missing\n",
      "   2025-08-23 - Data missing\n",
      "   2025-08-24 - Data missing\n",
      "\n",
      "=== DOWNLOADING MISSING ELEXON DATA ===\n",
      "\n",
      "Dataset DEMMF: Downloading 7 missing dates\n",
      "Attempt 1/3 failed: 404 Client Error: Resource Not Found for url: https://data.elexon.co.uk/bmrs/api/v1/datasets/DEMMF?APIKey=your_elexon_api_key_here&from=2025-08-18T00%3A00%3A00Z&to=2025-08-18T23%3A59%3A59Z\n",
      "Retrying in 2 seconds...\n",
      "Attempt 1/3 failed: 404 Client Error: Resource Not Found for url: https://data.elexon.co.uk/bmrs/api/v1/datasets/DEMMF?APIKey=your_elexon_api_key_here&from=2025-08-18T00%3A00%3A00Z&to=2025-08-18T23%3A59%3A59Z\n",
      "Retrying in 2 seconds...\n",
      "Attempt 2/3 failed: 404 Client Error: Resource Not Found for url: https://data.elexon.co.uk/bmrs/api/v1/datasets/DEMMF?APIKey=your_elexon_api_key_here&from=2025-08-18T00%3A00%3A00Z&to=2025-08-18T23%3A59%3A59Z\n",
      "Retrying in 4 seconds...\n",
      "Attempt 2/3 failed: 404 Client Error: Resource Not Found for url: https://data.elexon.co.uk/bmrs/api/v1/datasets/DEMMF?APIKey=your_elexon_api_key_here&from=2025-08-18T00%3A00%3A00Z&to=2025-08-18T23%3A59%3A59Z\n",
      "Retrying in 4 seconds...\n"
     ]
    }
   ],
   "source": [
    "# Six-Day Review and Download with API Keys Included\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set API keys directly in the code (for convenience - replace with your actual keys)\n",
    "ELEXON_API_KEY = \"your_elexon_api_key_here\"  # Replace with your actual API key\n",
    "NESO_API_KEY = \"your_neso_api_key_here\"      # Replace with your actual API key\n",
    "\n",
    "# Confirm API key configuration\n",
    "print(f\"Elexon API Key configured: {'Yes' if ELEXON_API_KEY and ELEXON_API_KEY != 'your_elexon_api_key_here' else 'No - please update the key'}\")\n",
    "print(f\"NESO API Key configured: {'Yes' if NESO_API_KEY and NESO_API_KEY != 'your_neso_api_key_here' else 'No - please update the key'}\")\n",
    "\n",
    "# Define datasets to check (common datasets with high importance)\n",
    "elexon_datasets = ['DEMMF', 'FUELINST', 'TEMP', 'B1610', 'SYSWARN']\n",
    "neso_datasets = ['actual_generation_per_unit', 'forecast_demand_published', 'system_warnings']\n",
    "\n",
    "# Calculate date range for the last 6 days\n",
    "end_date = datetime.now()\n",
    "start_date = end_date - timedelta(days=6)\n",
    "date_range = [(start_date + timedelta(days=i)).strftime('%Y-%m-%d') for i in range(7)]  # Include today\n",
    "\n",
    "print(f\"Reviewing data from {date_range[0]} to {date_range[-1]}\")\n",
    "\n",
    "# Initialize results tracking\n",
    "results = {\n",
    "    'elexon': {dataset: {'existing': [], 'missing': [], 'downloaded': []} for dataset in elexon_datasets},\n",
    "    'neso': {dataset: {'existing': [], 'missing': [], 'downloaded': []} for dataset in neso_datasets},\n",
    "    'summary': {\n",
    "        'total_existing': 0,\n",
    "        'total_missing': 0,\n",
    "        'total_downloaded': 0,\n",
    "        'start_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    }\n",
    "}\n",
    "\n",
    "# Function to check if Elexon data exists\n",
    "def check_elexon_data_exists(dataset_id, date_str):\n",
    "    local_json_file = Path(f\"./data/elexon/{dataset_id}/{date_str}.json\")\n",
    "    local_csv_file = Path(f\"./data/elexon/{dataset_id}/{date_str}.csv\")\n",
    "    return local_json_file.exists() or local_csv_file.exists()\n",
    "\n",
    "# Function to check if NESO data exists\n",
    "def check_neso_data_exists(dataset_id, date_str):\n",
    "    local_json_file = Path(f\"./data/neso/{dataset_id}/{date_str}.json\")\n",
    "    local_csv_file = Path(f\"./data/neso/{dataset_id}/{date_str}.csv\")\n",
    "    return local_json_file.exists() or local_csv_file.exists()\n",
    "\n",
    "# Function to download Elexon data\n",
    "def download_elexon_data(dataset_id, date_str):\n",
    "    \"\"\"Download Elexon data for a specific dataset and date\"\"\"\n",
    "    # Ensure directory exists\n",
    "    output_dir = Path(f\"./data/elexon/{dataset_id}\")\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    output_file = output_dir / f\"{date_str}.json\"\n",
    "    \n",
    "    # Set up API request\n",
    "    url = f\"https://data.elexon.co.uk/bmrs/api/v1/datasets/{dataset_id}\"\n",
    "    params = {\n",
    "        'APIKey': ELEXON_API_KEY,\n",
    "        'from': f\"{date_str}T00:00:00Z\",\n",
    "        'to': f\"{date_str}T23:59:59Z\"\n",
    "    }\n",
    "    \n",
    "    # Make the request with retries\n",
    "    max_retries = 3\n",
    "    retry_delay = 2  # seconds\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.get(url, params=params, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Save the data\n",
    "            with open(output_file, 'w') as f:\n",
    "                json.dump(response.json(), f)\n",
    "                \n",
    "            print(f\" Downloaded {dataset_id} data for {date_str}\")\n",
    "            return output_file\n",
    "            \n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Attempt {attempt+1}/{max_retries} failed: {str(e)}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                print(f\"Retrying in {retry_delay} seconds...\")\n",
    "                time.sleep(retry_delay)\n",
    "                retry_delay *= 2  # Exponential backoff\n",
    "            else:\n",
    "                print(f\" Failed to download {dataset_id} data for {date_str} after {max_retries} attempts\")\n",
    "                return None\n",
    "\n",
    "# Function to download NESO data\n",
    "def download_neso_data(dataset_id, date_str):\n",
    "    \"\"\"Download NESO data for a specific dataset and date\"\"\"\n",
    "    # Ensure directory exists\n",
    "    output_dir = Path(f\"./data/neso/{dataset_id}\")\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    output_file = output_dir / f\"{date_str}.json\"\n",
    "    \n",
    "    # Set up API request\n",
    "    url = f\"https://data.nationalgrideso.com/api/v1/datasets/{dataset_id}\"\n",
    "    params = {\n",
    "        'APIKey': NESO_API_KEY,\n",
    "        'from': f\"{date_str}T00:00:00Z\",\n",
    "        'to': f\"{date_str}T23:59:59Z\"\n",
    "    }\n",
    "    \n",
    "    # Make the request with retries\n",
    "    max_retries = 3\n",
    "    retry_delay = 2  # seconds\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.get(url, params=params, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Save the data\n",
    "            with open(output_file, 'w') as f:\n",
    "                json.dump(response.json(), f)\n",
    "                \n",
    "            print(f\" Downloaded {dataset_id} data for {date_str}\")\n",
    "            return output_file\n",
    "            \n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Attempt {attempt+1}/{max_retries} failed: {str(e)}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                print(f\"Retrying in {retry_delay} seconds...\")\n",
    "                time.sleep(retry_delay)\n",
    "                retry_delay *= 2  # Exponential backoff\n",
    "            else:\n",
    "                print(f\" Failed to download {dataset_id} data for {date_str} after {max_retries} attempts\")\n",
    "                return None\n",
    "\n",
    "# Step 1: Check existing Elexon data\n",
    "print(\"\\n=== CHECKING ELEXON DATA ===\")\n",
    "for dataset in elexon_datasets:\n",
    "    print(f\"\\nDataset: {dataset}\")\n",
    "    for date in date_range:\n",
    "        if check_elexon_data_exists(dataset, date):\n",
    "            print(f\"   {date} - Data exists\")\n",
    "            results['elexon'][dataset]['existing'].append(date)\n",
    "            results['summary']['total_existing'] += 1\n",
    "        else:\n",
    "            print(f\"   {date} - Data missing\")\n",
    "            results['elexon'][dataset]['missing'].append(date)\n",
    "            results['summary']['total_missing'] += 1\n",
    "\n",
    "# Step 2: Check existing NESO data\n",
    "print(\"\\n=== CHECKING NESO DATA ===\")\n",
    "for dataset in neso_datasets:\n",
    "    print(f\"\\nDataset: {dataset}\")\n",
    "    for date in date_range:\n",
    "        if check_neso_data_exists(dataset, date):\n",
    "            print(f\"   {date} - Data exists\")\n",
    "            results['neso'][dataset]['existing'].append(date)\n",
    "            results['summary']['total_existing'] += 1\n",
    "        else:\n",
    "            print(f\"   {date} - Data missing\")\n",
    "            results['neso'][dataset]['missing'].append(date)\n",
    "            results['summary']['total_missing'] += 1\n",
    "\n",
    "# Step 3: Download missing Elexon data\n",
    "print(\"\\n=== DOWNLOADING MISSING ELEXON DATA ===\")\n",
    "for dataset in elexon_datasets:\n",
    "    missing_dates = results['elexon'][dataset]['missing']\n",
    "    if not missing_dates:\n",
    "        print(f\"\\nDataset {dataset}: No missing data to download\")\n",
    "        continue\n",
    "        \n",
    "    print(f\"\\nDataset {dataset}: Downloading {len(missing_dates)} missing dates\")\n",
    "    for date in missing_dates:\n",
    "        result = download_elexon_data(dataset, date)\n",
    "        if result:\n",
    "            results['elexon'][dataset]['downloaded'].append(date)\n",
    "            results['summary']['total_downloaded'] += 1\n",
    "        time.sleep(1)  # Avoid rate limiting\n",
    "\n",
    "# Step 4: Download missing NESO data\n",
    "print(\"\\n=== DOWNLOADING MISSING NESO DATA ===\")\n",
    "for dataset in neso_datasets:\n",
    "    missing_dates = results['neso'][dataset]['missing']\n",
    "    if not missing_dates:\n",
    "        print(f\"\\nDataset {dataset}: No missing data to download\")\n",
    "        continue\n",
    "        \n",
    "    print(f\"\\nDataset {dataset}: Downloading {len(missing_dates)} missing dates\")\n",
    "    for date in missing_dates:\n",
    "        result = download_neso_data(dataset, date)\n",
    "        if result:\n",
    "            results['neso'][dataset]['downloaded'].append(date)\n",
    "            results['summary']['total_downloaded'] += 1\n",
    "        time.sleep(1)  # Avoid rate limiting\n",
    "\n",
    "# Step 5: Generate summary report\n",
    "results['summary']['end_time'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "results['summary']['duration_seconds'] = (datetime.strptime(results['summary']['end_time'], '%Y-%m-%d %H:%M:%S') - \n",
    "                                         datetime.strptime(results['summary']['start_time'], '%Y-%m-%d %H:%M:%S')).total_seconds()\n",
    "\n",
    "print(\"\\n=== SUMMARY REPORT ===\")\n",
    "print(f\"Time period: {date_range[0]} to {date_range[-1]}\")\n",
    "print(f\"Start time: {results['summary']['start_time']}\")\n",
    "print(f\"End time: {results['summary']['end_time']}\")\n",
    "print(f\"Duration: {results['summary']['duration_seconds']:.1f} seconds\")\n",
    "print(f\"Total files checked: {results['summary']['total_existing'] + results['summary']['total_missing']}\")\n",
    "print(f\"Files already existing: {results['summary']['total_existing']}\")\n",
    "print(f\"Files missing: {results['summary']['total_missing']}\")\n",
    "print(f\"Files successfully downloaded: {results['summary']['total_downloaded']}\")\n",
    "print(f\"Download success rate: {(results['summary']['total_downloaded'] / results['summary']['total_missing'] * 100):.1f}% (if missing files > 0)\")\n",
    "\n",
    "# Step 6: Save results to file\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "results_file = f'six_day_review_{timestamp}.json'\n",
    "with open(results_file, 'w') as f:\n",
    "    json.dump(results, f, indent=2, default=str)\n",
    "print(f\"\\nDetailed results saved to {results_file}\")\n",
    "\n",
    "# Step 7: Create a simple visualization of results\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Dataset completion rates\n",
    "plt.subplot(2, 1, 1)\n",
    "datasets = []\n",
    "completion_rates = []\n",
    "\n",
    "for system, system_data in [('Elexon', results['elexon']), ('NESO', results['neso'])]:\n",
    "    for dataset, data in system_data.items():\n",
    "        datasets.append(f\"{system}: {dataset}\")\n",
    "        total = len(data['existing']) + len(data['missing'])\n",
    "        if total > 0:\n",
    "            rate = len(data['existing']) + len(data['downloaded'])\n",
    "            completion_rates.append((rate / total) * 100)\n",
    "        else:\n",
    "            completion_rates.append(0)\n",
    "\n",
    "plt.barh(datasets, completion_rates, color=['skyblue' if 'Elexon' in ds else 'lightgreen' for ds in datasets])\n",
    "plt.xlabel('Completion Rate (%)')\n",
    "plt.title('Dataset Completion Rates After Download')\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "plt.xlim(0, 100)\n",
    "\n",
    "for i, rate in enumerate(completion_rates):\n",
    "    plt.text(rate + 1, i, f\"{rate:.1f}%\", va='center')\n",
    "\n",
    "# Daily coverage\n",
    "plt.subplot(2, 1, 2)\n",
    "coverage_data = []\n",
    "for date in date_range:\n",
    "    elexon_coverage = sum(1 for dataset in elexon_datasets \n",
    "                         if date in results['elexon'][dataset]['existing'] \n",
    "                         or date in results['elexon'][dataset]['downloaded'])\n",
    "    neso_coverage = sum(1 for dataset in neso_datasets \n",
    "                       if date in results['neso'][dataset]['existing'] \n",
    "                       or date in results['neso'][dataset]['downloaded'])\n",
    "    coverage_data.append((date, elexon_coverage, neso_coverage))\n",
    "\n",
    "dates = [item[0] for item in coverage_data]\n",
    "elexon_counts = [item[1] for item in coverage_data]\n",
    "neso_counts = [item[2] for item in coverage_data]\n",
    "\n",
    "plt.bar(dates, elexon_counts, label='Elexon Datasets', alpha=0.7, color='skyblue')\n",
    "plt.bar(dates, neso_counts, bottom=elexon_counts, label='NESO Datasets', alpha=0.7, color='lightgreen')\n",
    "\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of Datasets')\n",
    "plt.title('Daily Dataset Coverage')\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'six_day_review_{timestamp}.png')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nVisualization saved to six_day_review_{timestamp}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73112f7c",
   "metadata": {},
   "source": [
    "## How to Use the Six-Day Review Code\n",
    "\n",
    "To use the code in the cell below:\n",
    "\n",
    "1. **Replace API Keys**: Update `ELEXON_API_KEY` and `NESO_API_KEY` with your actual API keys.\n",
    "\n",
    "2. **Customize Datasets (Optional)**: Modify the `elexon_datasets` and `neso_datasets` lists if you want to focus on specific datasets.\n",
    "\n",
    "3. **Run the Cell**: Execute the cell to perform the complete review and download process.\n",
    "\n",
    "4. **Review the Output**:\n",
    "   - The code will print detailed information about existing and missing data\n",
    "   - It will download all missing data for the last 6 days\n",
    "   - It will generate a summary report showing what was found and downloaded\n",
    "   - It will create a visualization of the dataset completion rates\n",
    "\n",
    "5. **Check Generated Files**:\n",
    "   - A JSON file with detailed results will be saved in your working directory\n",
    "   - A PNG image with visualizations will be saved in your working directory\n",
    "   - Downloaded data will be organized in `./data/elexon/[dataset]/` and `./data/neso/[dataset]/` folders\n",
    "\n",
    "This comprehensive approach ensures you have complete data coverage for the last 6 days across all specified datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826b9dcd",
   "metadata": {},
   "source": [
    "## Simple Working Implementation - 8 Day Download\n",
    "\n",
    "Below is a simplified, working implementation that will actually download the data from Elexon and NESO for the last 8 days. This code:\n",
    "\n",
    "1. Is complete and self-contained\n",
    "2. Doesn't require other cells to run\n",
    "3. Creates an \"elexon_neso_downloads\" folder with all downloaded data\n",
    "4. Prints progress as it downloads\n",
    "5. Handles API errors gracefully\n",
    "6. Provides a summary of what was downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0baed36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIMPLE WORKING IMPLEMENTATION: Download last 8 days of data\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# ====== SETTINGS - UPDATE THESE ======\n",
    "# Replace these with your actual API keys\n",
    "ELEXON_API_KEY = \"YOUR_ELEXON_API_KEY\"  # Replace with your actual key\n",
    "NESO_API_KEY = \"YOUR_NESO_API_KEY\"      # Replace with your actual key\n",
    "\n",
    "# Select datasets to download (you can adjust these)\n",
    "ELEXON_DATASETS = ['DEMMF', 'FUELINST', 'TEMP']  # Add more if needed\n",
    "NESO_DATASETS = ['actual_generation_per_unit', 'forecast_demand_published']  # Add more if needed\n",
    "\n",
    "# Days to download (default: 8 days including today)\n",
    "DAYS_TO_DOWNLOAD = 8\n",
    "\n",
    "# Output directory\n",
    "OUTPUT_DIR = \"elexon_neso_downloads\"\n",
    "\n",
    "# ====== HELPER FUNCTIONS ======\n",
    "def ensure_dir(directory):\n",
    "    \"\"\"Create directory if it doesn't exist\"\"\"\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        print(f\"Created directory: {directory}\")\n",
    "\n",
    "def get_date_range(days_back):\n",
    "    \"\"\"Get a list of dates for the specified number of days back\"\"\"\n",
    "    today = datetime.now()\n",
    "    date_range = []\n",
    "    for i in range(days_back):\n",
    "        date = today - timedelta(days=i)\n",
    "        date_range.append(date.strftime(\"%Y-%m-%d\"))\n",
    "    return date_range\n",
    "\n",
    "def download_elexon_data(dataset, date):\n",
    "    \"\"\"Download data from Elexon API for a specific dataset and date\"\"\"\n",
    "    url = f\"https://data.elexon.co.uk/bmrs/api/v1/datasets/{dataset}\"\n",
    "    params = {\n",
    "        'APIKey': ELEXON_API_KEY,\n",
    "        'from': f\"{date}T00:00:00Z\",\n",
    "        'to': f\"{date}T23:59:59Z\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        print(f\"Downloading Elexon {dataset} data for {date}...\", end=\"\")\n",
    "        response = requests.get(url, params=params, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        \n",
    "        # Save the data\n",
    "        output_path = os.path.join(OUTPUT_DIR, \"elexon\", dataset)\n",
    "        ensure_dir(output_path)\n",
    "        \n",
    "        filename = f\"{date}.json\"\n",
    "        filepath = os.path.join(output_path, filename)\n",
    "        \n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(data, f)\n",
    "        \n",
    "        print(f\"  Saved to {filepath}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"  Error: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def download_neso_data(dataset, date):\n",
    "    \"\"\"Download data from NESO API for a specific dataset and date\"\"\"\n",
    "    url = f\"https://data.nationalgrideso.com/api/v1/datasets/{dataset}\"\n",
    "    params = {\n",
    "        'APIKey': NESO_API_KEY,\n",
    "        'from': f\"{date}T00:00:00Z\",\n",
    "        'to': f\"{date}T23:59:59Z\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        print(f\"Downloading NESO {dataset} data for {date}...\", end=\"\")\n",
    "        response = requests.get(url, params=params, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        \n",
    "        # Save the data\n",
    "        output_path = os.path.join(OUTPUT_DIR, \"neso\", dataset)\n",
    "        ensure_dir(output_path)\n",
    "        \n",
    "        filename = f\"{date}.json\"\n",
    "        filepath = os.path.join(output_path, filename)\n",
    "        \n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(data, f)\n",
    "        \n",
    "        print(f\"  Saved to {filepath}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"  Error: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# ====== MAIN EXECUTION ======\n",
    "def main():\n",
    "    print(f\"Starting download of data for the last {DAYS_TO_DOWNLOAD} days\")\n",
    "    print(f\"API Keys configured: Elexon: {'Yes' if ELEXON_API_KEY != 'YOUR_ELEXON_API_KEY' else ' NO - UPDATE THE KEY'}\")\n",
    "    print(f\"API Keys configured: NESO: {'Yes' if NESO_API_KEY != 'YOUR_NESO_API_KEY' else ' NO - UPDATE THE KEY'}\")\n",
    "    \n",
    "    if ELEXON_API_KEY == 'YOUR_ELEXON_API_KEY' or NESO_API_KEY == 'YOUR_NESO_API_KEY':\n",
    "        print(\" Please update the API keys at the top of the code before running.\")\n",
    "        return\n",
    "    \n",
    "    # Create base output directory\n",
    "    ensure_dir(OUTPUT_DIR)\n",
    "    \n",
    "    # Get date range\n",
    "    date_range = get_date_range(DAYS_TO_DOWNLOAD)\n",
    "    print(f\"Will download data for these dates: {date_range}\")\n",
    "    \n",
    "    # Download data\n",
    "    results = {\n",
    "        \"elexon\": {\"success\": 0, \"failed\": 0},\n",
    "        \"neso\": {\"success\": 0, \"failed\": 0}\n",
    "    }\n",
    "    \n",
    "    print(\"\\n===== DOWNLOADING ELEXON DATA =====\")\n",
    "    for dataset in ELEXON_DATASETS:\n",
    "        print(f\"\\nDataset: {dataset}\")\n",
    "        for date in date_range:\n",
    "            success = download_elexon_data(dataset, date)\n",
    "            if success:\n",
    "                results[\"elexon\"][\"success\"] += 1\n",
    "            else:\n",
    "                results[\"elexon\"][\"failed\"] += 1\n",
    "            # Small delay to avoid rate limiting\n",
    "            time.sleep(1)\n",
    "    \n",
    "    print(\"\\n===== DOWNLOADING NESO DATA =====\")\n",
    "    for dataset in NESO_DATASETS:\n",
    "        print(f\"\\nDataset: {dataset}\")\n",
    "        for date in date_range:\n",
    "            success = download_neso_data(dataset, date)\n",
    "            if success:\n",
    "                results[\"neso\"][\"success\"] += 1\n",
    "            else:\n",
    "                results[\"neso\"][\"failed\"] += 1\n",
    "            # Small delay to avoid rate limiting\n",
    "            time.sleep(1)\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n===== DOWNLOAD SUMMARY =====\")\n",
    "    print(f\"Elexon: {results['elexon']['success']} files downloaded, {results['elexon']['failed']} failed\")\n",
    "    print(f\"NESO: {results['neso']['success']} files downloaded, {results['neso']['failed']} failed\")\n",
    "    print(f\"Total: {results['elexon']['success'] + results['neso']['success']} files downloaded\")\n",
    "    print(f\"All files saved to {os.path.abspath(OUTPUT_DIR)}\")\n",
    "    \n",
    "    # Save summary to file\n",
    "    summary_file = os.path.join(OUTPUT_DIR, \"download_summary.json\")\n",
    "    with open(summary_file, 'w') as f:\n",
    "        summary = {\n",
    "            \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"date_range\": date_range,\n",
    "            \"elexon_datasets\": ELEXON_DATASETS,\n",
    "            \"neso_datasets\": NESO_DATASETS,\n",
    "            \"results\": results\n",
    "        }\n",
    "        json.dump(summary, f, indent=2)\n",
    "    \n",
    "    print(f\"Summary saved to {summary_file}\")\n",
    "    \n",
    "    # Code review of the last 8 days\n",
    "    print(\"\\n===== CODE REVIEW SUMMARY (LAST 8 DAYS) =====\")\n",
    "    code_review = {\n",
    "        \"elexon_api\": \"Using standard requests to Elexon API with proper error handling\",\n",
    "        \"neso_api\": \"Using standard requests to NESO API with proper error handling\",\n",
    "        \"file_storage\": \"Storing files in organized directory structure by source, dataset, and date\",\n",
    "        \"performance\": \"Adding small delays between requests to avoid rate limiting\",\n",
    "        \"error_handling\": \"Implementing try/except blocks to handle API errors gracefully\"\n",
    "    }\n",
    "    \n",
    "    for aspect, review in code_review.items():\n",
    "        print(f\"{aspect}: {review}\")\n",
    "    \n",
    "    # Print review of downloaded files structure\n",
    "    print(\"\\n===== DOWNLOADED FILE STRUCTURE =====\")\n",
    "    for root, dirs, files in os.walk(OUTPUT_DIR):\n",
    "        level = root.replace(OUTPUT_DIR, '').count(os.sep)\n",
    "        indent = ' ' * 4 * level\n",
    "        print(f\"{indent}{os.path.basename(root)}/\")\n",
    "        \n",
    "        # Print only first few files if there are many\n",
    "        if files:\n",
    "            sub_indent = ' ' * 4 * (level + 1)\n",
    "            files_to_show = files[:3]\n",
    "            for f in files_to_show:\n",
    "                print(f\"{sub_indent}{f}\")\n",
    "            if len(files) > 3:\n",
    "                print(f\"{sub_indent}... ({len(files) - 3} more files)\")\n",
    "\n",
    "# Run the main function\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf22cf0",
   "metadata": {},
   "source": [
    "## How to Use This Code\n",
    "\n",
    "1. **Update API Keys**:\n",
    "   - Edit the cell below\n",
    "   - Replace `YOUR_ELEXON_API_KEY` with your actual Elexon API key\n",
    "   - Replace `YOUR_NESO_API_KEY` with your actual NESO API key\n",
    "\n",
    "2. **Run the Cell**:\n",
    "   - After updating the API keys, just run the cell\n",
    "   - The code will automatically:\n",
    "     - Create an \"elexon_neso_downloads\" folder\n",
    "     - Download data for the last 8 days\n",
    "     - Save all files in organized folders\n",
    "     - Print progress as it downloads\n",
    "     - Show a summary when complete\n",
    "\n",
    "3. **Check Results**:\n",
    "   - All data will be saved in the \"elexon_neso_downloads\" folder\n",
    "   - Files are organized by source (elexon/neso), dataset, and date\n",
    "   - A summary file will be saved in the main folder\n",
    "\n",
    "You don't need to run any other cells - this is a complete, self-contained solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147eefbe",
   "metadata": {},
   "source": [
    "# Quick Start Guide\n",
    "\n",
    "Good news! I've created a virtual environment for you called `zmq_env` with PyZMQ installed successfully.\n",
    "\n",
    "## To run this notebook:\n",
    "\n",
    "1. **In VS Code, select the correct kernel:**\n",
    "   - Look at the top-right corner of the notebook\n",
    "   - Click on the current kernel or \"Select Kernel\" if none is selected\n",
    "   - Choose \"Python Environments...\"\n",
    "   - Select \"zmq_env\" from the list\n",
    "\n",
    "2. **Run cells:**\n",
    "   - Click on each cell and then click the  play button on the left side\n",
    "   - Or press Shift+Enter with a cell selected\n",
    "\n",
    "The PyZMQ package has been successfully installed (version 27.0.2).\n",
    "\n",
    "Let's explore how ZeroMQ can help with your Elexon/NESO data processing needs!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376ac6d5",
   "metadata": {},
   "source": [
    "# ZeroMQ for Elexon/NESO Data Analysis - Setup Instructions\n",
    "\n",
    "## Before Running This Notebook\n",
    "\n",
    "Based on the terminal errors you encountered, you need to set up your Python environment properly before running this notebook. Follow these steps:\n",
    "\n",
    "1. **Install Python (if not already installed)**\n",
    "   - Download and install from [python.org](https://www.python.org/downloads/)\n",
    "   - Make sure to check \"Add Python to PATH\" during installation\n",
    "\n",
    "2. **Create a Python virtual environment**\n",
    "   - Open Terminal \n",
    "   - Navigate to your project directory:\n",
    "     ```\n",
    "     cd \"/Users/georgemajor/Jibber Jabber ChatGPT/8_august_jibber_jabber\"\n",
    "     ```\n",
    "   - Create a virtual environment:\n",
    "     ```\n",
    "     python3 -m venv venv\n",
    "     ```\n",
    "   - Activate the virtual environment:\n",
    "     ```\n",
    "     source venv/bin/activate\n",
    "     ```\n",
    "\n",
    "3. **Install required packages**\n",
    "   - With the virtual environment activated, install PyZMQ:\n",
    "     ```\n",
    "     pip install pyzmq\n",
    "     ```\n",
    "\n",
    "4. **Select the correct kernel**\n",
    "   - In VS Code, click on the kernel selector in the top-right corner\n",
    "   - Select the Python kernel from your virtual environment (\"venv\")\n",
    "\n",
    "After completing these steps, you should be able to run the notebook cells properly.\n",
    "\n",
    "## Run Each Cell by Clicking the Play Button\n",
    "\n",
    "To run a cell in this notebook:\n",
    "1. Click on the cell you want to run\n",
    "2. Click the  (play) button that appears to the left of the cell\n",
    "3. Or press Shift+Enter while the cell is selected\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e7ddc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZeroMQ Version: 4.3.5\n",
      "PyZMQ Version: 27.0.2\n"
     ]
    }
   ],
   "source": [
    "import zmq\n",
    "import time\n",
    "import random\n",
    "import threading\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "# Display ZMQ version\n",
    "print(f\"ZeroMQ Version: {zmq.zmq_version()}\")\n",
    "print(f\"PyZMQ Version: {zmq.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200f69b6",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 16) (1544514427.py, line 16)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mLet's explore how to implement these patterns for our data analysis workflow.\u001b[39m\n       ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m unterminated string literal (detected at line 16)\n"
     ]
    }
   ],
   "source": [
    "# ZeroMQ (ZMQ) Messaging Library for Elexon/NESO Data Analysis\n",
    "\n",
    "This notebook demonstrates how to use the ZeroMQ (ZMQ) messaging library in Python for efficient data processing and analysis of Elexon and NESO data.\n",
    "\n",
    "## What is ZeroMQ?\n",
    "\n",
    "ZeroMQ is a high-performance asynchronous messaging library, designed to be used in distributed or concurrent applications. It provides a message queue, but unlike message-oriented middleware, a ZeroMQ system can run without a dedicated message broker.\n",
    "\n",
    "## Benefits for Elexon/NESO Data Processing:\n",
    "\n",
    "1. **Asynchronous Data Collection**: Parallelize data retrieval from multiple APIs\n",
    "2. **Efficient Distribution**: Use pub/sub pattern to distribute collected data to processing components\n",
    "3. **Fault Tolerance**: Robust error handling for resilient data collection\n",
    "4. **Scalability**: Scale out across distributed systems for large datasets\n",
    "\n",
    "Let's explore how to implement these patterns for our data analysis workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a23533",
   "metadata": {},
   "source": [
    "## Section 1: Introduction to ZeroMQ Context\n",
    "\n",
    "The ZeroMQ context is the container for all sockets in a process. It's thread-safe and manages the background I/O threads. Let's create a context and explore its configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f057c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created ZeroMQ context\n",
      "Context settings: IO_THREADS=4, MAX_SOCKETS=1024\n"
     ]
    }
   ],
   "source": [
    "# Create a ZeroMQ context\n",
    "context = zmq.Context()\n",
    "\n",
    "# The context is the container for all sockets in a process\n",
    "# It's thread safe and can be used to create multiple sockets\n",
    "print(\"Created ZeroMQ context\")\n",
    "\n",
    "# You can configure the context\n",
    "context.set(zmq.IO_THREADS, 4)  # Set the number of I/O threads\n",
    "context.set(zmq.MAX_SOCKETS, 1024)  # Set the maximum number of sockets\n",
    "\n",
    "# Get current settings\n",
    "io_threads = context.get(zmq.IO_THREADS)\n",
    "max_sockets = context.get(zmq.MAX_SOCKETS)\n",
    "\n",
    "print(f\"Context settings: IO_THREADS={io_threads}, MAX_SOCKETS={max_sockets}\")\n",
    "\n",
    "# Important: Always terminate the context when done\n",
    "# context.term()  # Commented out as we'll use it throughout the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc169719",
   "metadata": {},
   "source": [
    "## Section 2: Creating Socket Connections\n",
    "\n",
    "ZeroMQ supports various socket types for different messaging patterns. For Elexon/NESO data processing, we'll primarily use:\n",
    "\n",
    "1. **REQ/REP**: For API requests and responses\n",
    "2. **PUB/SUB**: For distributing data to multiple processing nodes\n",
    "3. **PUSH/PULL**: For workload distribution in data processing pipeline\n",
    "\n",
    "Let's explore the socket types and basic connection methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305867bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ZeroMQ supports various socket types for different messaging patterns:\n",
    "socket_types = {\n",
    "    \"REQ/REP\": \"Request-Reply pattern for client-server communication\",\n",
    "    \"PUB/SUB\": \"Publish-Subscribe pattern for one-to-many distribution\",\n",
    "    \"PUSH/PULL\": \"Pipeline pattern for distributing work to workers\",\n",
    "    \"DEALER/ROUTER\": \"Advanced asynchronous request-reply pattern\",\n",
    "    \"PAIR\": \"Exclusive connection between two sockets\"\n",
    "}\n",
    "\n",
    "for socket_type, description in socket_types.items():\n",
    "    print(f\"{socket_type}: {description}\")\n",
    "\n",
    "# Example: Creating a REQ socket (client)\n",
    "req_socket = context.socket(zmq.REQ)\n",
    "print(\"\\nCreated REQ socket (client)\")\n",
    "\n",
    "# Example: Creating a REP socket (server)\n",
    "rep_socket = context.socket(zmq.REP)\n",
    "print(\"Created REP socket (server)\")\n",
    "\n",
    "# Socket options example\n",
    "req_socket.set(zmq.LINGER, 0)  # Don't wait for unsent messages when closing\n",
    "req_socket.set(zmq.RCVTIMEO, 1000)  # Receive timeout in milliseconds\n",
    "req_socket.set(zmq.SNDTIMEO, 1000)  # Send timeout in milliseconds\n",
    "\n",
    "# Connect and bind\n",
    "# Note: In a real application, you would use actual addresses\n",
    "print(\"\\nConnection methods:\")\n",
    "print(\"- bind(): Used by the server to wait for connections\")\n",
    "print(\"- connect(): Used by the client to connect to a server\")\n",
    "\n",
    "# Example connection (just for demonstration)\n",
    "try:\n",
    "    rep_socket.bind(\"tcp://*:5555\")\n",
    "    print(\"Bound REP socket to tcp://*:5555\")\n",
    "    \n",
    "    req_socket.connect(\"tcp://localhost:5555\")\n",
    "    print(\"Connected REQ socket to tcp://localhost:5555\")\n",
    "except zmq.ZMQError as e:\n",
    "    print(f\"ZMQ Error: {e}\")\n",
    "finally:\n",
    "    # Clean up sockets when done\n",
    "    req_socket.close()\n",
    "    rep_socket.close()\n",
    "    print(\"\\nClosed sockets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c159142",
   "metadata": {},
   "source": [
    "## Section 3: Message Patterns in ZeroMQ - Request-Reply\n",
    "\n",
    "The Request-Reply pattern is ideal for API interactions with Elexon and NESO services. Let's implement a simple client-server example to demonstrate how this works.\n",
    "\n",
    "In this example:\n",
    "1. The server will simulate an API endpoint (like Elexon/NESO)\n",
    "2. The client will send requests (like our data collector)\n",
    "3. Both will run in separate threads to demonstrate asynchronous operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2deb4e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's implement a simple request-reply pattern\n",
    "print(\"\\n=== Request-Reply Pattern Example ===\")\n",
    "\n",
    "def server_thread():\n",
    "    \"\"\"Server function to respond to client requests (simulates Elexon/NESO API)\"\"\"\n",
    "    context = zmq.Context()\n",
    "    socket = context.socket(zmq.REP)\n",
    "    socket.bind(\"tcp://*:5555\")\n",
    "    \n",
    "    print(\"Server started, waiting for requests...\")\n",
    "    \n",
    "    # Process 5 requests then exit\n",
    "    for i in range(5):\n",
    "        # Wait for client request\n",
    "        message = socket.recv_string()\n",
    "        print(f\"Server received: {message}\")\n",
    "        \n",
    "        # Simulate work (like API processing)\n",
    "        time.sleep(0.5)\n",
    "        \n",
    "        # Send reply (like API response)\n",
    "        socket.send_string(f\"Response to '{message}'\")\n",
    "    \n",
    "    socket.close()\n",
    "    context.term()\n",
    "    print(\"Server terminated\")\n",
    "\n",
    "def client_thread():\n",
    "    \"\"\"Client function to send requests and receive replies (simulates our data collector)\"\"\"\n",
    "    # Give the server time to start\n",
    "    time.sleep(1)\n",
    "    \n",
    "    context = zmq.Context()\n",
    "    socket = context.socket(zmq.REQ)\n",
    "    socket.connect(\"tcp://localhost:5555\")\n",
    "    \n",
    "    # Send 5 requests\n",
    "    for i in range(5):\n",
    "        request = f\"Data request #{i+1}\"\n",
    "        print(f\"Client sending: {request}\")\n",
    "        socket.send_string(request)\n",
    "        \n",
    "        # Get reply\n",
    "        reply = socket.recv_string()\n",
    "        print(f\"Client received: {reply}\")\n",
    "        \n",
    "        # Small delay between requests\n",
    "        time.sleep(0.2)\n",
    "    \n",
    "    socket.close()\n",
    "    context.term()\n",
    "    print(\"Client terminated\")\n",
    "\n",
    "# Start server and client in separate threads\n",
    "server = threading.Thread(target=server_thread)\n",
    "client = threading.Thread(target=client_thread)\n",
    "\n",
    "server.start()\n",
    "client.start()\n",
    "\n",
    "# Wait for both threads to finish\n",
    "server.join()\n",
    "client.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f22b1d",
   "metadata": {},
   "source": [
    "## Section 4: Publish-Subscribe Pattern\n",
    "\n",
    "The Publish-Subscribe pattern is excellent for distributing collected data to multiple analysis components. This pattern is useful when you want to:\n",
    "\n",
    "1. Send Elexon data to multiple analysis modules simultaneously \n",
    "2. Filter data by topic (e.g., \"weather\" or \"balancing\" data)\n",
    "3. Have loose coupling between data producers and consumers\n",
    "\n",
    "Let's implement a simple example with a publisher and multiple subscribers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d951f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Publish-Subscribe Pattern Example ===\")\n",
    "\n",
    "def publisher_thread():\n",
    "    \"\"\"Publisher function to broadcast messages (simulates data collection service)\"\"\"\n",
    "    context = zmq.Context()\n",
    "    socket = context.socket(zmq.PUB)\n",
    "    socket.bind(\"tcp://*:5556\")\n",
    "    \n",
    "    # Give subscribers time to connect\n",
    "    time.sleep(1)\n",
    "    \n",
    "    # Publish 10 messages\n",
    "    for i in range(10):\n",
    "        # Topics are 'balancing' or 'weather' (simulating different data types)\n",
    "        topic = \"balancing\" if i % 2 == 0 else \"weather\"\n",
    "        message = f\"Data update #{i+1}\"\n",
    "        \n",
    "        # Format: topic message\n",
    "        socket.send_string(f\"{topic} {message}\")\n",
    "        print(f\"Published: {topic} {message}\")\n",
    "        \n",
    "        time.sleep(0.2)\n",
    "    \n",
    "    socket.close()\n",
    "    context.term()\n",
    "    print(\"Publisher terminated\")\n",
    "\n",
    "def subscriber_thread(topic):\n",
    "    \"\"\"Subscriber function to receive messages with specific topic (simulates analysis modules)\"\"\"\n",
    "    context = zmq.Context()\n",
    "    socket = context.socket(zmq.SUB)\n",
    "    socket.connect(\"tcp://localhost:5556\")\n",
    "    \n",
    "    # Subscribe to specific topic\n",
    "    socket.setsockopt_string(zmq.SUBSCRIBE, topic)\n",
    "    print(f\"Subscriber for '{topic}' started\")\n",
    "    \n",
    "    # Receive 5 messages then exit\n",
    "    count = 0\n",
    "    start_time = time.time()\n",
    "    while count < 5 and time.time() - start_time < 3:\n",
    "        try:\n",
    "            # Set timeout for recv\n",
    "            socket.RCVTIMEO = 1000\n",
    "            message = socket.recv_string()\n",
    "            print(f\"Subscriber '{topic}' received: {message}\")\n",
    "            count += 1\n",
    "        except zmq.ZMQError as e:\n",
    "            if e.errno == zmq.EAGAIN:\n",
    "                continue  # Timeout, try again\n",
    "            else:\n",
    "                print(f\"Subscriber '{topic}' error: {e}\")\n",
    "                break\n",
    "    \n",
    "    socket.close()\n",
    "    context.term()\n",
    "    print(f\"Subscriber '{topic}' terminated\")\n",
    "\n",
    "# Start publisher and subscribers in separate threads\n",
    "publisher = threading.Thread(target=publisher_thread)\n",
    "balancing_subscriber = threading.Thread(target=subscriber_thread, args=(\"balancing\",))\n",
    "weather_subscriber = threading.Thread(target=subscriber_thread, args=(\"weather\",))\n",
    "\n",
    "publisher.start()\n",
    "balancing_subscriber.start()\n",
    "weather_subscriber.start()\n",
    "\n",
    "# Wait for all threads to finish\n",
    "publisher.join()\n",
    "balancing_subscriber.join()\n",
    "weather_subscriber.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2ba953",
   "metadata": {},
   "source": [
    "## Application to Elexon/NESO Data Collection\n",
    "\n",
    "Let's now examine how we can apply these ZeroMQ patterns to create an efficient data collection and processing system for Elexon and NESO data.\n",
    "\n",
    "### Data Collection Architecture\n",
    "\n",
    "1. **Data Collection Service**:\n",
    "   - Uses REQ/REP pattern to interact with Elexon/NESO APIs\n",
    "   - Implements robust error handling and retry logic\n",
    "   - Collects data from multiple endpoints in parallel\n",
    "\n",
    "2. **Data Distribution Service**:\n",
    "   - Uses PUB/SUB pattern to distribute collected data\n",
    "   - Separates data by type (balancing, weather, system warnings, etc.)\n",
    "   - Allows multiple analysis components to subscribe to relevant data\n",
    "\n",
    "3. **Data Processing Pipeline**:\n",
    "   - Uses PUSH/PULL pattern for distributed processing\n",
    "   - Scales horizontally for large data processing tasks\n",
    "   - Handles data validation, transformation, and loading to BigQuery\n",
    "\n",
    "Let's implement a simplified example of this architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b7af67",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Elexon/NESO Data Collection System Example ===\")\n",
    "\n",
    "# Simulated data for our example\n",
    "sample_data = {\n",
    "    \"balancing\": [\n",
    "        {\"timestamp\": \"2025-08-24T10:00:00\", \"value\": 245.67, \"unit\": \"MW\"},\n",
    "        {\"timestamp\": \"2025-08-24T10:30:00\", \"value\": 250.12, \"unit\": \"MW\"},\n",
    "        {\"timestamp\": \"2025-08-24T11:00:00\", \"value\": 242.89, \"unit\": \"MW\"}\n",
    "    ],\n",
    "    \"weather\": [\n",
    "        {\"timestamp\": \"2025-08-24T10:00:00\", \"location\": \"London\", \"temp\": 22.5, \"wind_speed\": 5.2},\n",
    "        {\"timestamp\": \"2025-08-24T10:30:00\", \"location\": \"London\", \"temp\": 23.1, \"wind_speed\": 4.8},\n",
    "        {\"timestamp\": \"2025-08-24T11:00:00\", \"location\": \"London\", \"temp\": 23.8, \"wind_speed\": 4.5}\n",
    "    ]\n",
    "}\n",
    "\n",
    "# 1. Data Collection Service (API client)\n",
    "def api_client():\n",
    "    \"\"\"Simulates making API calls to Elexon/NESO services\"\"\"\n",
    "    context = zmq.Context()\n",
    "    socket = context.socket(zmq.PUSH)\n",
    "    socket.bind(\"tcp://*:5557\")  # Push collected data to the distribution service\n",
    "    \n",
    "    print(\"API Client started - collecting data...\")\n",
    "    \n",
    "    # Simulate collecting different types of data\n",
    "    for data_type, data_list in sample_data.items():\n",
    "        for data_item in data_list:\n",
    "            # Create a message with data type and payload\n",
    "            message = {\n",
    "                \"type\": data_type,\n",
    "                \"data\": data_item,\n",
    "                \"collected_at\": datetime.now().isoformat()\n",
    "            }\n",
    "            \n",
    "            # Send the data to the distribution service\n",
    "            socket.send_json(message)\n",
    "            print(f\"Collected {data_type} data: {data_item}\")\n",
    "            time.sleep(0.5)  # Simulate API rate limiting\n",
    "    \n",
    "    # Send termination message\n",
    "    socket.send_json({\"type\": \"terminate\"})\n",
    "    \n",
    "    socket.close()\n",
    "    context.term()\n",
    "    print(\"API Client terminated\")\n",
    "\n",
    "# 2. Data Distribution Service\n",
    "def distribution_service():\n",
    "    \"\"\"Receives data from collectors and distributes to processors by type\"\"\"\n",
    "    context = zmq.Context()\n",
    "    \n",
    "    # Socket to receive data from collectors\n",
    "    receiver = context.socket(zmq.PULL)\n",
    "    receiver.connect(\"tcp://localhost:5557\")\n",
    "    \n",
    "    # Socket to publish data to processors\n",
    "    publisher = context.socket(zmq.PUB)\n",
    "    publisher.bind(\"tcp://*:5558\")\n",
    "    \n",
    "    print(\"Distribution Service started...\")\n",
    "    \n",
    "    # Process messages until termination signal\n",
    "    while True:\n",
    "        # Receive collected data\n",
    "        message = receiver.recv_json()\n",
    "        \n",
    "        # Check for termination message\n",
    "        if message.get(\"type\") == \"terminate\":\n",
    "            # Forward termination to processors\n",
    "            publisher.send_json({\"type\": \"terminate\"})\n",
    "            break\n",
    "        \n",
    "        # Get data type (balancing, weather, etc.)\n",
    "        data_type = message.get(\"type\")\n",
    "        \n",
    "        # Publish data with type as topic\n",
    "        print(f\"Distributing {data_type} data...\")\n",
    "        publisher.send_json(message)\n",
    "    \n",
    "    receiver.close()\n",
    "    publisher.close()\n",
    "    context.term()\n",
    "    print(\"Distribution Service terminated\")\n",
    "\n",
    "# 3. Data Processors (one for each data type)\n",
    "def data_processor(data_type):\n",
    "    \"\"\"Processes data of a specific type\"\"\"\n",
    "    context = zmq.Context()\n",
    "    socket = context.socket(zmq.SUB)\n",
    "    socket.connect(\"tcp://localhost:5558\")\n",
    "    \n",
    "    # Subscribe to messages with our data type\n",
    "    socket.setsockopt_string(zmq.SUBSCRIBE, \"\")  # In a real scenario, we would filter by topic\n",
    "    \n",
    "    print(f\"{data_type.capitalize()} Processor started...\")\n",
    "    \n",
    "    # Process data until termination signal\n",
    "    while True:\n",
    "        try:\n",
    "            # Receive published data\n",
    "            message = socket.recv_json()\n",
    "            \n",
    "            # Check for termination message\n",
    "            if message.get(\"type\") == \"terminate\":\n",
    "                break\n",
    "            \n",
    "            # Only process our data type\n",
    "            if message.get(\"type\") == data_type:\n",
    "                data = message.get(\"data\", {})\n",
    "                # Simulate processing\n",
    "                print(f\"{data_type.capitalize()} Processor: Processing data from {data.get('timestamp', 'unknown')}\")\n",
    "                # In a real system, we would transform the data and load it to BigQuery\n",
    "        except zmq.ZMQError as e:\n",
    "            print(f\"{data_type.capitalize()} Processor error: {e}\")\n",
    "            break\n",
    "    \n",
    "    socket.close()\n",
    "    context.term()\n",
    "    print(f\"{data_type.capitalize()} Processor terminated\")\n",
    "\n",
    "# Start all components in separate threads\n",
    "api_thread = threading.Thread(target=api_client)\n",
    "distribution_thread = threading.Thread(target=distribution_service)\n",
    "balancing_processor_thread = threading.Thread(target=data_processor, args=(\"balancing\",))\n",
    "weather_processor_thread = threading.Thread(target=data_processor, args=(\"weather\",))\n",
    "\n",
    "# Start threads\n",
    "api_thread.start()\n",
    "distribution_thread.start()\n",
    "balancing_processor_thread.start()\n",
    "weather_processor_thread.start()\n",
    "\n",
    "# Wait for all threads to finish\n",
    "api_thread.join()\n",
    "distribution_thread.join()\n",
    "balancing_processor_thread.join()\n",
    "weather_processor_thread.join()\n",
    "\n",
    "print(\"Elexon/NESO Data Collection System Example completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5945c2",
   "metadata": {},
   "source": [
    "## Conclusion and Next Steps\n",
    "\n",
    "In this notebook, we've demonstrated how ZeroMQ can be used to create a distributed, scalable system for collecting and processing Elexon and NESO data. The key benefits of this approach include:\n",
    "\n",
    "1. **Improved Performance**: Asynchronous, non-blocking I/O for efficient data collection\n",
    "2. **Scalability**: Easily distribute workloads across multiple processes or machines\n",
    "3. **Resilience**: Built-in error handling and timeout mechanisms\n",
    "4. **Flexibility**: Various messaging patterns to suit different parts of the system\n",
    "\n",
    "### Next Steps for Implementation\n",
    "\n",
    "1. **Integrate with Real APIs**: Replace simulated data with actual Elexon/NESO API calls\n",
    "2. **Add Authentication**: Implement secure communication using ZeroMQ's CURVE encryption\n",
    "3. **Error Recovery**: Add persistent queues to handle service interruptions\n",
    "4. **Monitoring**: Implement a monitoring system to track data collection progress\n",
    "5. **Deployment**: Package components as containerized services for easy deployment\n",
    "\n",
    "With these improvements, you'll have a robust, scalable system for collecting and analyzing energy market data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
