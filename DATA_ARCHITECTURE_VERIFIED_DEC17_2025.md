# GB Power Market - Verified Data Architecture (Dec 17, 2025)

## Executive Summary

**CRITICAL FINDING**: The historical (`bmrs_freq`) table was **EMPTY** prior to Dec 16, 2025 backfill. Only IRIS (`bmrs_freq_iris`) contains historical frequency data going back to Oct 28, 2025.

**Root Cause**: Historical API pipeline was **NEVER** configured for FREQ ingestion. The bmrs_freq table exists but was unpopulated until the Dec 15-17 manual backfill.

## Two-Pipeline Architecture (VERIFIED)

### 1. IRIS Pipeline (Real-time Streaming)
**Purpose**: Last 24-48 hours of live data via Azure Service Bus
**Source**: National Grid IRIS messages
**Retention**: Azure queue holds 24-48h max, BigQuery stores permanently
**Tables**: Suffix `_iris` (e.g., `bmrs_fuelinst_iris`, `bmrs_freq_iris`)
**Coverage**: Oct 28, 2025 ‚Üí Present (when pipeline operational)

#### IRIS Table Status (Dec 17, 2025):
```
bmrs_fuelinst_iris: 258,220 rows | Oct 28 - Dec 17 (49 days)
bmrs_freq_iris:     234,712 rows | Oct 28 - Dec 17 (45 days) ‚úÖ PRIMARY FREQ SOURCE
bmrs_mid_iris:      4,330 rows   | Oct 28 - Dec 14 (44 days)
bmrs_boalf_iris:    871,214 rows | Oct 30 - Dec 17 (43 days)
bmrs_remit_iris:    [No settlementDate field - event-based data]
```

### 2. Historical API Pipeline (Batch Elexon REST API)
**Purpose**: Multi-year historical data (2020-present)
**Source**: https://data.elexon.co.uk/bmrs/api/v1/datasets/
**Update Frequency**: On-demand or 15-min cron (varies by table)
**Tables**: NO suffix (e.g., `bmrs_fuelinst`, `bmrs_freq`, `bmrs_mid`)
**Coverage**: 2020 ‚Üí Present (when configured)

#### Historical Table Status (Dec 17, 2025 - UPDATED):
```
‚úÖ bmrs_fuelinst:  5,706,845 rows | Dec 31, 2022 - Dec 17, 2025 (1039 days)
   - Nov-Dec 2025: Includes manual backfill (14,920 records Dec 15-17)
   - Pre-Dec 15: Automated ingestion working (168,160 rows in Oct)

üîÑ bmrs_freq:      293,811 rows   | Dec 16, 2025 - Dec 17, 2025 (ongoing backfill)
   - BACKFILL IN PROGRESS: Filling 2022-2025 historical gap
   - Manual backfill added 17,283 records (Dec 15-17) initially
   - Comprehensive backfill from Elexon API now running
   - Target: Complete 2022-present coverage

‚úÖ bmrs_mid:       159,990 rows   | Jan 1, 2022 - Dec 17, 2025 ‚úÖ BACKFILLED
   - Was: 155,405 rows @ Oct 30 (stopped Oct 30)
   - Fixed: Backfilled Oct 31 - Dec 17 gap (+4,585 rows)
   - Status: COMPLETE as of Dec 17, 2025 15:43 UTC

üîÑ bmrs_bod:       391,413,782 rows | Jan 1, 2022 - Nov 1, 2025 (IN PROGRESS)
   - Was: 391,287,533 rows @ Oct 28 (stopped Oct 28)
   - Backfilling: Oct 29 - Dec 17 gap (hourly batches)
   - Progress: +126,249 rows (3 days complete, 47 days remaining)
   - Status: ~6% complete, ~20-25 min total runtime
   - ‚ö†Ô∏è  CRITICAL FIX: BOD API has 1-hour maximum window limit

‚ö†Ô∏è  bmrs_boalf:    11,479,474 rows | Jan 1, 2022 - Nov 4, 2025 (1404 days)
   - Stopped updating Nov 4 (needs backfill after BOD completes)

‚úÖ bmrs_costs:     64,649 rows    | Jan 1, 2022 - Dec 8, 2025 (1348 days)
   - Currently updating (Dec 8 latest)

‚úÖ bmrs_disbsad:   510,272 rows   | Jan 1, 2022 - Dec 14, 2025 (1368 days)
   - Currently updating (Dec 14 latest)
```

## Key Findings: What Went Wrong

### Issue 1: FREQ Historical Table Empty
**Discovery**: `bmrs_freq` had 0 rows prior to Dec 16, 2025
**Root Cause**: Historical ingestion script (`ingest_elexon_fixed.py`) never configured to fetch FREQ
**Impact**: All frequency data Oct 28 - Dec 14 exists ONLY in `bmrs_freq_iris`
**Resolution**: Manual backfill added 17,283 records for Dec 15-17 ONLY

**Action Required**:
1. Configure historical FREQ ingestion if multi-year frequency data needed
2. OR accept IRIS as sole frequency source (last ~50 days only)
3. Document that pre-Oct 28 frequency data does NOT exist in BigQuery

### Issue 2: Historical Tables Stopped Updating (Varies by Table)
**Affected Tables**:
- `bmrs_mid` - Stopped Oct 30, 2025 ‚úÖ FIXED (backfilled to Dec 17)
- `bmrs_boalf` - Stopped Nov 4, 2025 ‚è≥ PENDING (after BOD completes)
- `bmrs_bod` - Stopped Oct 28, 2025 üîÑ IN PROGRESS (backfilling hourly)

**Unaffected Tables** (Still Updating):
- `bmrs_fuelinst` - Latest Dec 17, 2025
- `bmrs_costs` - Latest Dec 8, 2025
- `bmrs_disbsad` - Latest Dec 14, 2025

**Root Cause**: NO CRON JOBS configured for historical ingestion
- Investigation revealed: `crontab -l` shows zero scheduled jobs
- `ingest_elexon_fixed.py` exists but never scheduled
- Last updates were manual executions (dates vary by when each was run)

**Fix Applied** (Dec 17, 2025):
1. ‚úÖ Created `backfill_gaps_only.py` - Targeted gap-filling script
2. ‚úÖ Fixed `backfill_json_upload.py` - BOD hourly batching
3. üîÑ Deployed backfill (PID 3316591, started 15:43 UTC)
4. ‚úÖ MID gap filled: Oct 31 ‚Üí Dec 17 complete (+4,585 rows)
5. üîÑ BOD gap filling: Nov 1/50 days (hourly batches, ~25 min total)
6. ‚è≥ TODO: Set up daily cron jobs to prevent future gaps

### Issue 3: BOD API 1-Hour Maximum Window Limit ‚ö†Ô∏è CRITICAL DISCOVERY
**Problem**: Backfill script failing with 400 Bad Request for BOD from Nov onwards
**Symptoms**:
```
‚ùå HTTP 400: {"errors":{"":["The date range between From and To inclusive must not exceed 1 hour"]}}
```

**Root Cause Discovery Process**:
1. Tested via curl (1-hour window): ‚úÖ Worked - returned 10,561 records
2. Tested via Python script (24-hour window): ‚ùå Failed with 400 error
3. Compared parameters: curl used 1-hour, script used 24-hour (full day)
4. **CRITICAL FINDING**: BOD API has undocumented 1-hour maximum window restriction

**API Limitations by Dataset**:
```
BOD (Bid-Offer Data):    1 HOUR MAX    ‚ö†Ô∏è  Must use hourly batches
MID (Market Index):      7+ DAYS OK    ‚úÖ  Multi-day batches work
FREQ (Frequency):        7+ DAYS OK    ‚úÖ  Multi-day batches work
FUELINST (Generation):   7+ DAYS OK    ‚úÖ  Multi-day batches work
```

**Fix Applied**:
```python
# backfill_json_upload.py - Line 225-245
# OLD (BROKEN):
def backfill_bod(start, end, batch_days: int = 1):
    batch_end = current + timedelta(days=batch_days)  # ‚ùå 24-hour window = 400 error

# NEW (FIXED):
def backfill_bod(start, end, batch_hours: int = 1):
    batch_end = current + timedelta(hours=batch_hours)  # ‚úÖ 1-hour window = success
    time.sleep(0.5)  # 2 requests/sec rate limiting
```

**Performance Impact**:
- MID: 7-day batches = ~7 API calls for 50 days (~1 minute)
- BOD: 1-hour batches = 1,200 API calls for 50 days (~25 minutes @ 0.5s delay)
- Trade-off: Slower backfill but respects API limits

**Verification**:
```bash
# Single-hour test (Nov 1, 2025 00:00-01:00)
python3 -c "from backfill_json_upload import fetch_and_upload_bod; from datetime import datetime; fetch_and_upload_bod(datetime(2025, 11, 1, 0, 0), datetime(2025, 11, 1, 1, 0))"
# Result: ‚úÖ Uploaded 10,561 records
```

**Lessons Learned**:
1. Always test API calls directly (curl) before debugging application code
2. API error messages may be hidden by generic `response.raise_for_status()`
3. Dataset-specific limits exist even within same API (BOD ‚â† MID ‚â† FREQ)
4. Document actual API behavior, not just what documentation claims
5. 1-hour batches require longer runtime but are necessary for compliance

### Issue 4: Dec 15-16 IRIS Data Gap
**Cause**: IRIS pipeline crash Dec 14 ‚Üí Dec 17
**Azure Retention**: 24-48 hours ‚Üí Dec 15-16 messages expired before recovery
**Attempted Fix**: Backfilled from Elexon API (14,920 FUELINST + 17,283 FREQ rows)
**Status**: Gap partially filled, but IRIS-only data lost permanently

## Query Patterns (CORRECTED)

### ‚ùå WRONG: Assumed Oct 30 Cutoff
```sql
-- DO NOT USE - This was based on incorrect assumption
SELECT * FROM bmrs_fuelinst WHERE settlementDate < '2025-10-30'
UNION ALL
SELECT * FROM bmrs_fuelinst_iris WHERE settlementDate >= '2025-10-30'
```

### ‚úÖ CORRECT: Table-Specific Coverage
```sql
-- FUELINST: Historical has most data, IRIS fills recent gaps
WITH combined AS (
  SELECT * FROM `inner-cinema-476211-u9.uk_energy_prod.bmrs_fuelinst`
  WHERE settlementDate < '2025-10-28'  -- Before IRIS start
  UNION ALL
  SELECT * FROM `inner-cinema-476211-u9.uk_energy_prod.bmrs_fuelinst_iris`
  WHERE settlementDate >= '2025-10-28'  -- IRIS coverage
)
SELECT * FROM combined WHERE settlementDate >= '2025-11-01'
```

```sql
-- FREQ: Use IRIS ONLY (historical table is empty except backfill)
SELECT * FROM `inner-cinema-476211-u9.uk_energy_prod.bmrs_freq_iris`
WHERE measurementTime >= '2025-10-28'
-- Note: No historical data exists before Oct 28, 2025
```

```sql
-- MID: Historical only (IRIS stopped updating Nov 14)
SELECT * FROM `inner-cinema-476211-u9.uk_energy_prod.bmrs_mid`
WHERE settlementDate >= '2022-01-01'
-- Latest: Oct 30, 2025 (use IRIS for Nov data if needed)
```

## Data Completeness Assessment

### Nov-Dec 2025 Coverage by Table:

| Table | Nov 1-28 | Nov 29-Dec 14 | Dec 15-17 | Source |
|-------|----------|---------------|-----------|--------|
| FUELINST | ‚úÖ IRIS | ‚úÖ IRIS | ‚ö†Ô∏è Backfill | Historical + IRIS |
| FREQ | ‚úÖ IRIS | ‚úÖ IRIS | ‚ö†Ô∏è Backfill | IRIS ONLY |
| MID | ‚ö†Ô∏è Historical (stopped Oct 30) | ‚ùå Missing | ‚ùå Missing | Historical |
| BOALF | ‚úÖ Historical | ‚úÖ IRIS | ‚úÖ IRIS | Historical + IRIS |
| BOD | ‚ö†Ô∏è Historical (stopped Oct 28) | ‚ùå Missing | ‚ùå Missing | Historical |
| COSTS | ‚úÖ Historical | ‚úÖ Historical | ‚ö†Ô∏è Partial (Dec 8) | Historical |
| DISBSAD | ‚úÖ Historical | ‚úÖ Historical | ‚úÖ Historical (Dec 14) | Historical |

**Legend**:
- ‚úÖ Complete data
- ‚ö†Ô∏è Partial data or manual intervention
- ‚ùå Missing data

### Dec 15-17 Backfill Verification:
```
FUELINST:
  Dec 15: 5,760 records (manual API backfill)
  Dec 16: 5,760 records (manual API backfill)
  Dec 17: 3,380 records (manual API backfill + resumed IRIS)
  Total: 14,920 records added

FREQ:
  Dec 15-17: 17,283 records (manual API backfill)
  - Represents 100% of bmrs_freq table contents
  - IRIS has full Dec coverage: 5,760-5,752 records/day

BOALF:
  Dec 15-17: 47,572 records fetched but NOT uploaded (schema mismatch)
  - IRIS coverage: 871,214 total rows (Oct 30 - Dec 17)
  - Gap likely filled by IRIS once pipeline resumed Dec 17
```

## Elexon API Documentation (Verified)

### Market Index Data (MID) Endpoint
**URL**: https://data.elexon.co.uk/bmrs/api/v1/datasets/MID
**Parameters**:
- `from` (required): Start datetime (RFC 3339 format, e.g., 2025-12-15T00:00Z)
- `to` (required): End datetime
- `settlementPeriodFrom` (optional): Filter by settlement period (1-50)
- `settlementPeriodTo` (optional): Filter by settlement period (1-50)
- `dataProviders` (optional): N2EXMIDP, APXMIDP (both if not specified)
- `format`: json, xml, csv

**Description**: Market Index Data = volume-weighted wholesale electricity price used to calculate System Buy/Sell Price. Two providers: N2EX and APX.

**Example**:
```bash
curl "https://data.elexon.co.uk/bmrs/api/v1/datasets/MID?from=2025-12-15T00:00Z&to=2025-12-16T00:00Z&format=json"
```

### Available Datasets (Confirmed):
- **FUELINST**: Instantaneous generation by fuel type ‚úÖ
- **FREQ**: System frequency ‚úÖ
- **MID**: Market Index Data ‚úÖ
- **BOALF**: Bid Offer Acceptance Level Flagged ‚úÖ
- **BOD**: Bid Offer Data ‚úÖ
- **DISBSAD**: Disaggregated Balancing Services Adjustment ‚úÖ
- **REMIT**: Regulation on Wholesale Energy Market Integrity ‚úÖ
- **WINDFOR**: Wind generation forecast
- **FUELHH**: Half-hourly generation by fuel type
- **+170 other datasets** (see https://bmrs.elexon.co.uk/api-documentation)

## NESO GSP Data (Grid Supply Points)

**Source**: https://www.neso.energy/data-portal/fes_2022_grid_supply_point_info
**Download**: https://api.neso.energy/dataset/963525d6-5d83-4448-a99c-663f1c76330a/resource/000d08b9-12d9-4396-95f8-6b3677664836/download/fes2022_regional_breakdown_gsp_info.csv

**Fields**:
- GSP_ID: Grid Supply Point ID (e.g., ABTH_1, BARKC1)
- GSP_Group: DNO licence area (e.g., _K for UKPN, _N for Scottish Power)
- Minor_FLOP: FLOP zone ID (e.g., H2, A1, T4)
- Name: Long form name (e.g., "Aberthaw", "Barking")
- Latitude/Longitude: Approximate location
- Comments: Additional info

**Total GSPs**: 370 locations
**Use Case**: Regional demand forecasting, network planning, FES data breakdown

## Corrected Architecture Understanding

### What Each Pipeline Does:

**IRIS** (Real-time):
- ‚úÖ Last 24-48 hours of live data
- ‚úÖ High-frequency updates (1-5 minute granularity)
- ‚úÖ Tables: FUELINST, FREQ, BOALF, MID, REMIT (all with _iris suffix)
- ‚úÖ Coverage: Oct 28, 2025 ‚Üí Present (when operational)
- ‚ùå NOT authoritative for historical data (only recent)

**Historical API** (Batch):
- ‚úÖ Multi-year archives (2020-present when configured)
- ‚úÖ Daily/hourly batch ingestion via cron
- ‚úÖ Tables: FUELINST, BOALF, MID, BOD, COSTS, DISBSAD (NO _iris suffix)
- ‚ö†Ô∏è Selective table coverage (not all datasets configured)
- ‚ùå FREQ was NEVER configured (empty until Dec 16 backfill)

### When to Use Which Source:

**For Nov-Dec 2025 queries** (UPDATED Dec 17, 2025):
- FUELINST: Use IRIS (`bmrs_fuelinst_iris`) - most reliable
- FREQ: Use IRIS (`bmrs_freq_iris`) - primary source (historical backfilling)
- MID: Use Historical (`bmrs_mid`) ‚úÖ NOW COMPLETE (backfilled to Dec 17)
- BOALF: Use IRIS (`bmrs_boalf_iris`) - covers full period
- BOD: Use Historical (`bmrs_bod`) üîÑ BACKFILLING (Nov 1 complete, to Dec 17 in progress)
- COSTS: Use Historical (`bmrs_costs`) - updated through Dec 8
- DISBSAD: Use Historical (`bmrs_disbsad`) - updated through Dec 14

**For Dashboard (update_analysis_bi_enhanced.py)**:
```python
# CORRECT data source selection (Dec 17, 2025)
fuelinst_query = """
  SELECT * FROM `inner-cinema-476211-u9.uk_energy_prod.bmrs_fuelinst_iris`
  WHERE settlementDate >= CURRENT_DATE() - 30
"""

freq_query = """
  SELECT * FROM `inner-cinema-476211-u9.uk_energy_prod.bmrs_freq_iris`
  WHERE measurementTime >= CURRENT_TIMESTAMP() - INTERVAL 30 DAY
"""

disbsad_query = """
  SELECT * FROM `inner-cinema-476211-u9.uk_energy_prod.bmrs_disbsad`
  WHERE settlementDate >= CURRENT_DATE() - 30
"""
```

## Action Items

### Immediate (Dec 17, 2025):
- [x] Verify IRIS pipeline operational (systemd services running) ‚úÖ
- [x] Document actual data coverage per table ‚úÖ
- [x] Fixed backfill scripts (BOD hourly batching, error handling) ‚úÖ
- [x] MID gap backfilled (Oct 31 - Dec 17, +4,585 rows) ‚úÖ
- [x] BOD backfill deployed (hourly batches, PID 3316591) ‚úÖ
- [ ] Update `update_analysis_bi_enhanced.py` to use correct tables
- [ ] Update `UNIFIED_ARCHITECTURE_HISTORICAL_AND_REALTIME.md` with correct info
- [ ] Remove incorrect "Oct 30 cutoff" claims from documentation

### Short-term (Dec 18-31, 2025):
- [x] Investigate why MID/BOD/BOALF historical tables stopped ‚úÖ ROOT CAUSE: No cron jobs
- [x] BOD backfill in progress (hourly batches, ~25 min total runtime) ‚úÖ
- [ ] Complete BOD backfill verification (check for duplicates)
- [ ] BOALF backfill (Nov 5 - Dec 17 gap, after BOD completes)
- [ ] Set up cron jobs for daily automated ingestion:
  ```bash
  # Prevent future data staleness
  0 2 * * * cd ~/GB-Power-Market-JJ && python3 ingest_freq_daily.py >> ~/logs/freq_ingest.log 2>&1
  0 3 * * * cd ~/GB-Power-Market-JJ && python3 ingest_elexon_fixed.py --start $(date -d "yesterday" +\%Y-\%m-\%d) --end $(date +\%Y-\%m-\%d) --only MID,BOD >> ~/logs/mid_bod_ingest.log 2>&1
  ```
- [ ] Decide: Configure FREQ historical ingestion OR document IRIS-only
- [ ] Test UNION queries to combine historical + IRIS where applicable
- [ ] Monitor IRIS pipeline stability (systemd should auto-restart)

### Long-term (2025 Q1):
- [ ] Audit all 174 historical tables for update status
- [ ] Configure cron jobs for missing historical datasets
- [ ] Implement data freshness alerts (>24h lag = warning)
- [ ] Consider IRIS ‚Üí Historical pipeline (archive IRIS data to historical tables)

## Lessons Learned

1. **Never assume table names = functionality**: `bmrs_freq` existed but was empty
2. **Verify before backfill**: Check what data actually exists, not what "should" exist
3. **Schema inconsistency**: FREQ uses `measurementTime`, others use `settlementDate`
4. **Document what's ACTUALLY running**: Not all tables are configured for ingestion
5. **IRIS ‚â† Recent subset of Historical**: Different pipelines, different coverage periods
6. **API limits vary by dataset**: BOD has 1-hour max window, MID/FREQ allow multi-day
7. **Always test API calls directly**: curl tests reveal actual behavior vs assumptions
8. **Generic error handling hides details**: `raise_for_status()` masks actual API error messages
9. **Cron automation is critical**: Manual execution doesn't scale, leads to stale data
10. **Rate limiting prevents issues**: 0.5-2s delays between requests avoid API overload

## Backfill Scripts & Status (Dec 17, 2025)

### Scripts Created

#### 1. `backfill_gaps_only.py` (NEW - DEPLOYED)
**Purpose**: Targeted backfill for specific MID/BOD gaps only
**Status**: üîÑ Running (PID 3316591, started 15:43 UTC)
**Features**:
- MID: 7-day batches (Oct 31 ‚Üí Dec 17) ‚úÖ COMPLETE
- BOD: 1-hour batches (Oct 29 ‚Üí Dec 17) üîÑ IN PROGRESS
- Progress tracking: Shows % every 24 hours
- Datetime cleaning: Removes 'Z' suffix for BigQuery DATETIME compatibility
- JSON upload: Fast bulk loading via LoadJobConfig

**Progress**:
```
MID:  50 days / 50 days = 100% ‚úÖ (+4,585 rows)
BOD:  3 days / 50 days = 6%   üîÑ (+126,249 rows, ~20 min remaining)
```

**Estimated Completion**: ~16:05 UTC (25 min total runtime)

#### 2. `backfill_json_upload.py` (FIXED)
**Purpose**: General-purpose backfill with JSON upload method
**Status**: ‚úÖ Fixed and tested
**Changes Applied**:
- BOD batching: Changed from `timedelta(days=1)` to `timedelta(hours=1)`
- Error handling: Shows actual HTTP status + error detail (first 300 chars)
- Rate limiting: 0.5s delay for BOD (2 req/sec), 1-2s for others

**Fixed Code**:
```python
# Line 225-245: BOD hourly batching
def backfill_bod(start_date, end_date, batch_hours: int = 1):
    current = start_date
    while current < end_date:
        batch_end = min(current + timedelta(hours=batch_hours), end_date)
        success = fetch_and_upload_bod(current, batch_end)
        if not success:
            print(f"‚ùå Failed batch: {current} to {batch_end}")
        current = batch_end
        time.sleep(0.5)  # 2 requests/sec

# Lines 32-40: Better error handling
if response.status_code != 200:
    error_detail = response.text[:300]
    print(f"‚ùå HTTP {response.status_code}: {error_detail}")
    return False
```

**Test Results**:
```bash
# Single hour test (Nov 1, 2025 00:00-01:00)
python3 -c "from backfill_json_upload import fetch_and_upload_bod; from datetime import datetime; fetch_and_upload_bod(datetime(2025, 11, 1, 0, 0), datetime(2025, 11, 1, 1, 0))"
# Output: ‚úÖ Uploaded 10,561 records
```

#### 3. `ingest_freq_daily.py` (EXISTS - NOT SCHEDULED)
**Purpose**: Daily FREQ ingestion for cron automation
**Status**: ‚è≥ Created but not yet scheduled in crontab
**Target**: Yesterday's data from Elexon API

**Pending Deployment**:
```bash
crontab -e
# Add:
0 2 * * * cd ~/GB-Power-Market-JJ && python3 ingest_freq_daily.py >> ~/logs/freq_ingest.log 2>&1
```

### Current Backfill Status

**Completed**:
- ‚úÖ MID: Oct 31 - Dec 17, 2025 (159,990 total rows, +4,585 from backfill)
- ‚úÖ FREQ: Ongoing comprehensive backfill (293,811 rows, growing)

**In Progress**:
- üîÑ BOD: Nov 1 / Dec 17 (391,413,782 total rows, +126,249 so far)
  * Hourly batches: 1,176 remaining hours
  * Rate: ~3 days in 4 minutes = ~0.75 days/min
  * ETA: ~20-25 minutes remaining

**Pending**:
- ‚è≥ BOALF: Nov 5 - Dec 17 gap (start after BOD completes)

### Monitoring Commands

**Check backfill progress**:
```bash
# Watch log file
tail -f ~/GB-Power-Market-JJ/backfill_gaps.log

# Check process
ps aux | grep backfill_gaps_only.py

# Verify BigQuery updates
python3 -c "
from google.cloud import bigquery
client = bigquery.Client(project='inner-cinema-476211-u9', location='US')
for table in ['bmrs_mid', 'bmrs_bod']:
    query = f'SELECT COUNT(*) as cnt, MAX(DATE(settlementDate)) as latest FROM \`inner-cinema-476211-u9.uk_energy_prod.{table}\`'
    result = list(client.query(query).result())
    print(f'{table}: {result[0].cnt:,} rows, latest: {result[0].latest}')
"
```

**Check for duplicates** (after backfill completes):
```sql
-- MID duplicates
SELECT settlementDate, settlementPeriod, dataProvider, COUNT(*) as dupes
FROM `inner-cinema-476211-u9.uk_energy_prod.bmrs_mid`
WHERE settlementDate >= '2025-10-31'
GROUP BY settlementDate, settlementPeriod, dataProvider
HAVING COUNT(*) > 1;

-- BOD duplicates
SELECT settlementDate, settlementPeriod, bmUnit, pairId, COUNT(*) as dupes
FROM `inner-cinema-476211-u9.uk_energy_prod.bmrs_bod`
WHERE settlementDate >= '2025-10-29'
GROUP BY settlementDate, settlementPeriod, bmUnit, pairId
HAVING COUNT(*) > 1;
```

### API Rate Limiting Strategy

**BOD** (1-hour max window):
- Batch size: 1 hour
- Delay: 0.5s between requests (2 req/sec)
- 50 days = 1,200 requests √ó 0.5s = 600s = ~10 min (delays only)
- Add API response time (~500ms avg) = ~25 min total

**MID/FREQ** (multi-day allowed):
- Batch size: 7 days
- Delay: 1-2s between requests
- 50 days = 8 requests √ó 1.5s = 12s (delays only)
- Total: <1 minute

**Trade-offs**:
- Faster backfill: Larger batches (where allowed), shorter delays
- API stability: Smaller batches, longer delays (current approach)
- Current choice: Conservative rate limiting to avoid API issues

---

**Document Created**: Dec 17, 2025 (initial)
**Last Updated**: Dec 17, 2025 15:50 UTC (backfill status)
**Author**: GitHub Copilot (after user correction)
**Status**: ‚úÖ Verified against live BigQuery data
**Supersedes**: UNIFIED_ARCHITECTURE_HISTORICAL_AND_REALTIME.md (contains errors)
