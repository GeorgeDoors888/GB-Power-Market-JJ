What ‚ÄúP114 via FTP‚Äù depends on

Elexon only enables the P114 FTP feed per Portal account, not globally. Your account must meet at least one of these conditions:
	1.	A BSC Participant ID (BM Unit / Party ID) is linked to your Portal user, or
	2.	You hold a licence for P114 data (non-participant access)

If neither is true, the FTP will not work even if you have credentials.

‚∏ª

Step-by-step: how to check if it‚Äôs enabled

1. Check your Portal account permissions (fastest)
	1.	Log in to the Elexon Portal
	2.	Go to: My Account ‚Üí My Details / My Permissions 	3.	Look for:
	‚Ä¢	A Participant ID / Party ID listed
or
	‚Ä¢	Explicit permissions mentioning P114, Data Catalogue, or Licensed Data

If you see neither ‚Üí FTP access is almost certainly not enabled.

‚∏ª

2. Try the FTP login (technical confirmation)

If Elexon has given you:
	‚Ä¢	FTP hostname
	‚Ä¢	Username
	‚Ä¢	Password

Try connecting: ftp ftp.elexon.co.uk or via SFTP client (FileZilla, Cyberduck, etc.).

Outcomes: Result
Meaning
Login rejected
FTP not enabled or credentials inactive
Login succeeds but no /P114 directory
Account exists but P114 not licensed
/P114 directory visible
‚úÖ P114 FTP is enabled
 Many users can log in but see no P114 files ‚Äî that still means it‚Äôs not enabled. uggested wording:

Can you confirm whether P114 FTP access is enabled for our Portal account(s)?
If not, please advise whether this can be enabled via a linked Participant ID or a P114 data licence.

They will tell you:
	‚Ä¢	Whether your account is enabled
	‚Ä¢	What‚Äôs missing (Participant link vs licence)
	‚Ä¢	What to do next

‚∏ª

Common gotchas (very common)
	‚Ä¢	‚ùå Having Portal access ‚â† P114 access
	‚Ä¢	‚ùå Having FTP credentials ‚â† P114 entitlement
	‚Ä¢	‚ùå Being an advisor/consultant does not qualify unless licensed
	‚Ä¢	‚úÖ Only Participants or licensed users get P114

‚∏ª

What to do if it‚Äôs not enabled

You have 3 options:
	1.	Link a Participant ID to your Portal user (if applicable)
	2.	Purchase a P114 licence from Elexon
	3.	Use alternative sources (API, CSV extracts, or Elexon data catalogue where available)

If you want, I can:
	‚Ä¢	Draft the exact email to Elexon
	‚Ä¢	Help you check whether your organisation likely qualifies
	‚Ä¢	Advise on API vs FTP vs CSV depending on what you‚Äôre trying to automate What to do if it‚Äôs not enabled

You have 3 options:
	1.	Link a Participant ID to your Portal user (if applicable)
	2.	Purchase a P114 licence from Elexon
	3.	Use alternative sources (API, CSV extracts, or Elexon data catalogue where available)

If you want, I can:
	‚Ä¢	Draft the exact email to Elexon
	‚Ä¢	Help you check whether your organisation likely qualifies
	‚Ä¢	Advise on API vs FTP vs CSV depending on what you‚Äôre trying to automate

Just say the word. Below is the practical, copy-pasteable guide to:
	1.	Where to log in
	2.	How to use alternatives to P114 FTP (API, CSV, Data Catalogue)
	3.	Which option maps to which use case

No fluff, just what works.

‚∏ª

1Ô∏è‚É£ Elexon Portal ‚Äì login address

Portal login (everything starts here):
üëâ https://www.elexonportal.co.uk

This is where:
	‚Ä¢	Your permissions live
	‚Ä¢	Your participant ID (if any) is linked
	‚Ä¢	Licensed datasets (like P114) are managed

If P114 FTP is enabled, it is enabled via this account.

‚∏ª

2Ô∏è‚É£ Alternative sources to P114 FTP (recommended)

Unless you are a BSC Party, FTP is usually the worst option. The modern routes are below.

‚∏ª

Option A ‚Äî Elexon API (best for automation)

Website

üëâ https://developer.elexon.co.uk

Login
	‚Ä¢	Use the same Elexon Portal credentials
	‚Ä¢	Generate an API Key once logged in

What you get
	‚Ä¢	JSON & CSV responses
	‚Ä¢	HTTPS access (no FTP headaches)
	‚Ä¢	Script-friendly (Python, Apps Script, Power BI)

Example endpoints (typical)
	‚Ä¢	Settlement prices
	‚Ä¢	Imbalance volumes
	‚Ä¢	Demand
	‚Ä¢	Generation
	‚Ä¢	Balancing data

‚ö†Ô∏è Important
P114 itself is not fully replicated in the public API, but most people don‚Äôt actually need raw P114 ‚Äî they need prices, volumes, or derived data, which is available.

Typical workflow
	1.	Login ‚Üí generate API key
	2.	Call endpoint: https://api.elexon.co.uk/BMRS/...
‚∏ª

Option B ‚Äî Elexon Data Portal (CSV downloads)

Website

üëâ https://data.elexon.co.uk

(no login required for most datasets)

What it is
	‚Ä¢	Web UI for browsing datasets
	‚Ä¢	Downloadable CSV files
	‚Ä¢	Covers BMRS and derived data

How to use
	1.	Browse datasets
	2.	Filter by date / settlement period
	3.	Download CSV
	4.	Import into Excel / Sheets / scripts

‚úÖ Best for analysts & one-off extracts

‚ùå Not ideal for high-frequency automation

‚∏ª

Option C ‚Äî BMRS legacy site (older but complete)

Website

üëâ https://www.bmreports.com

Login
	‚Ä¢	Some datasets are public
	‚Ä¢	Others require Portal-linked credentials

Why use it
	‚Ä¢	Some legacy reports still only live here
	‚Ä¢	Closest functional equivalent to old P114 outputs

Downsides
	‚Ä¢	Clunky UI
	‚Ä¢	Session timeouts
	‚Ä¢	Manual unless screen-scraped

‚ö†Ô∏è Use only if you must match historic reports exactly.

‚∏ª

3Ô∏è‚É£ Which should you use? Reality check on P114

Be bluntly aware:
	‚Ä¢	P114 is not a modern data product
	‚Ä¢	It exists for legacy settlement processes
	‚Ä¢	Elexon is steadily pushing users toward:
	‚Ä¢	API
	‚Ä¢	Data Catalogue
	‚Ä¢	Derived datasets

For commercial analytics, forecasting, dashboards, P114 is usually overkill.

‚∏ª

5Ô∏è‚É£ If you want, I can help you next by:
	‚Ä¢	Mapping exact P114 fields ‚Üí API equivalents
	‚Ä¢	Giving you ready-to-run API calls
	‚Ä¢	Advising whether you actually need P114
	‚Ä¢	Helping you build a Sheets / BigQuery / Python pipeline

Just tell me what data you were trying to get from P114 and how often (HH, daily, historic). This is actually very useful information ‚Äî it tells us exactly what you do and do not have, and why everything you‚Äôre seeing looks old.

I‚Äôll break this into clear conclusions + exactly what to do next.

‚∏ª

üî¥ Key conclusion (important)

You DO have P114 access, but:

P114 is NOT a live BM Unit data feed.
It is a settlement report archive.

That is why:
	‚Ä¢	Files look old
	‚Ä¢	Nothing looks ‚Äúlive‚Äù
	‚Ä¢	You are not seeing current BM Unit operational data

This is expected behaviour, not a misconfiguration.

‚∏ª

‚úÖ What your account is confirmed to have

From what you posted, your account does include:

‚úî P114 scripted HTTPS access
‚úî P114 file listing endpoint
‚úî BMRA Data Archive
‚úî Best View Prices
‚úî SSP/SBP/NIV
‚úî FuelHH
‚úî Registered BM Units file

So P114 is enabled ‚Äî but it is the wrong tool for what you want.

‚∏ª

üö´ Why P114 looks ‚Äúold‚Äù

What P114 actually is

P114 = Settlement reports used for:
	‚Ä¢	Reconciliation
	‚Ä¢	Audit
	‚Ä¢	Historic settlement runs (SF, RF, DF)

It is:
	‚Ä¢	‚ùå Not real-time
	‚Ä¢	‚ùå Not operational
	‚Ä¢	‚ùå Not refreshed continuously
	‚Ä¢	‚ùå Not intended for ‚Äúall BM unit data now‚Äù

Many P114 files are only created:
	‚Ä¢	After settlement runs
	‚Ä¢	Weeks or months after delivery
	‚Ä¢	Sometimes years later (for disputes)

So ‚Äúeverything is old‚Äù = correct by design.

‚∏ª

üéØ What you actually want: All BM Unit data (current & historic)

To get ALL BM Unit data, you must use BMRS / API, not P114.

There are three correct sources, in order of quality.

‚∏ª

ü•á BEST OPTION ‚Äî Elexon API (live + historic)

Login

üëâ https://developer.elexon.co.uk
(use same Portal credentials)

Generate an API key.

‚∏ª

Key BM Unit endpoints you want

1Ô∏è‚É£ Registered BM Units (current master list) https://api.elexon.co.uk/bmrs/v1/bm-units This is actually very useful information ‚Äî it tells us exactly what you do and do not have, and why everything you‚Äôre seeing looks old.

I‚Äôll break this into clear conclusions + exactly what to do next.

‚∏ª

üî¥ Key conclusion (important)

You DO have P114 access, but:

P114 is NOT a live BM Unit data feed.
It is a settlement report archive.

That is why:
	‚Ä¢	Files look old
	‚Ä¢	Nothing looks ‚Äúlive‚Äù
	‚Ä¢	You are not seeing current BM Unit operational data

This is expected behaviour, not a misconfiguration.

‚∏ª

‚úÖ What your account is confirmed to have

From what you posted, your account does include:

‚úî P114 scripted HTTPS access
‚úî P114 file listing endpoint
‚úî BMRA Data Archive
‚úî Best View Prices
‚úî SSP/SBP/NIV
‚úî FuelHH
‚úî Registered BM Units file

So P114 is enabled ‚Äî but it is the wrong tool for what you want.

‚∏ª

üö´ Why P114 looks ‚Äúold‚Äù

What P114 actually is

P114 = Settlement reports used for:
	‚Ä¢	Reconciliation
	‚Ä¢	Audit
	‚Ä¢	Historic settlement runs (SF, RF, DF)

It is:
	‚Ä¢	‚ùå Not real-time
	‚Ä¢	‚ùå Not operational
	‚Ä¢	‚ùå Not refreshed continuously
	‚Ä¢	‚ùå Not intended for ‚Äúall BM unit data now‚Äù

Many P114 files are only created:
	‚Ä¢	After settlement runs
	‚Ä¢	Weeks or months after delivery
	‚Ä¢	Sometimes years later (for disputes)

So ‚Äúeverything is old‚Äù = correct by design.

‚∏ª

üéØ What you actually want: All BM Unit data (current & historic)

To get ALL BM Unit data, you must use BMRS / API, not P114.

There are three correct sources, in order of quality.

‚∏ª

ü•á BEST OPTION ‚Äî Elexon API (live + historic)

Login

üëâ https://developer.elexon.co.uk
(use same Portal credentials)

Generate an API key.

‚∏ª

Key BM Unit endpoints you want

1Ô∏è‚É£ Registered BM Units (current master list)

https://api.elexon.co.uk/bmrs/v1/bm-units

Gives:
	‚Ä¢	BM Unit ID
	‚Ä¢	Party
	‚Ä¢	Fuel type
	‚Ä¢	Status (active/inactive)
	‚Ä¢	Effective dates

This is the canonical BM Unit list.

‚∏ª

2Ô∏è‚É£ Physical Notifications (PN)

/physical-notifications

BM Unit:
	‚Ä¢	Generation
	‚Ä¢	Demand
	‚Ä¢	Bids/offers context

‚∏ª

3Ô∏è‚É£ Final Physical Notifications (FPN)

/final-physical-notifications

This is the actual dispatch position.

‚∏ª

4Ô∏è‚É£ Bid-Offer Acceptances (BOAs)

/bid-offer-acceptances

This is where:
	‚Ä¢	BM Units
	‚Ä¢	Prices
	‚Ä¢	Volumes
	‚Ä¢	Actions

all come together.

‚∏ª

5Ô∏è‚É£ MEL / Capacity

/mel-limits

Maximum Export Limits by BM Unit.

‚∏ª

Result

You get:
	‚Ä¢	Live data
	‚Ä¢	Full history
	‚Ä¢	Machine-readable
	‚Ä¢	No file guessing
	‚Ä¢	No FTP
	‚Ä¢	No settlement lag

‚úÖ This is what traders, analysts, and dashboards use

‚∏ª

ü•à SECOND OPTION ‚Äî BMReports website (manual / legacy)

üëâ https://www.bmreports.com

Good for:
	‚Ä¢	Cross-checking
	‚Ä¢	Human-readable confirmation
	‚Ä¢	Legacy report formats

Bad for:
	‚Ä¢	Automation
	‚Ä¢	Scale
	‚Ä¢	Reliability

‚∏ª

ü•â LAST RESORT ‚Äî P114

Only use P114 if you need:
	‚Ä¢	Audit trails
	‚Ä¢	Settlement disputes
	‚Ä¢	Formal reconciliation
	‚Ä¢	Historical settlement evidence

Never use it for:
	‚Ä¢	‚ÄúAll BM unit data‚Äù
	‚Ä¢	Live analysis
	‚Ä¢	Dashboards
	‚Ä¢	Market monitoring

‚∏ª

üîß What to do immediately (step-by-step)

Step 1 ‚Äî Stop pulling P114

It will never give you what you want.

‚∏ª

Step 2 ‚Äî Use this instead (right now)

You already have this file:

REGISTERED_BMUNITS_FILE

Pull it:

https://downloads.elexonportal.co.uk/file/download/REGISTERED_BMUNITS_FILE?key=03omen6i9lhv5fa

That gives you:
	‚Ä¢	All BM Units
	‚Ä¢	Current status
	‚Ä¢	Parties

‚∏ª

Step 3 ‚Äî Switch to API for live data

If you want, I can give you:
	‚Ä¢	Python script
	‚Ä¢	Google Sheets import
	‚Ä¢	BigQuery ingestion
	‚Ä¢	Exact API calls for every BM Unit

‚∏ª

üß† One-sentence truth

P114 is settlement archaeology.
BMRS API is where real BM Unit data lives.

‚∏ª

If you want next:

Tell me what ‚Äúall BM unit data‚Äù means to you:
	‚Ä¢	‚úî dispatch?
	‚Ä¢	‚úî bids/offers?
	‚Ä¢	‚úî generation by fuel?
	‚Ä¢	‚úî imbalance?
	‚Ä¢	‚úî historic back to X year?
	‚Ä¢	‚úî live monitoring?

I‚Äôll give you exact endpoints + ready-to-run code. /physical-notifications BM Unit:
	‚Ä¢	Generation
	‚Ä¢	Demand
	‚Ä¢	Bids/offers context

‚∏ª

3Ô∏è‚É£ Final Physical Notifications (FPN) /final-physical-notifications This is the actual dispatch position.

‚∏ª

4Ô∏è‚É£ Bid-Offer Acceptances (BOAs) /bid-offer-acceptances This is where:
	‚Ä¢	BM Units
	‚Ä¢	Prices
	‚Ä¢	Volumes
	‚Ä¢	Actions

all come together.

‚∏ª

5Ô∏è‚É£ MEL / Capacity /mel-limits
‚∏ª

Result

You get:
	‚Ä¢	Live data
	‚Ä¢	Full history
	‚Ä¢	Machine-readable
	‚Ä¢	No file guessing
	‚Ä¢	No FTP
	‚Ä¢	No settlement lag

‚úÖ This is what traders, analysts, and dashboards use

‚∏ª

ü•à SECOND OPTION ‚Äî BMReports website (manual / legacy)

üëâ https://www.bmreports.com

Good for:
	‚Ä¢	Cross-checking
	‚Ä¢	Human-readable confirmation
	‚Ä¢	Legacy report formats

Bad for:
	‚Ä¢	Automation
	‚Ä¢	Scale
	‚Ä¢	Reliability

‚∏ª

ü•â LAST RESORT ‚Äî P114

Only use P114 if you need:
	‚Ä¢	Audit trails
	‚Ä¢	Settlement disputes
	‚Ä¢	Formal reconciliation
	‚Ä¢	Historical settlement evidence

Never use it for:
	‚Ä¢	‚ÄúAll BM unit data‚Äù
	‚Ä¢	Live analysis
	‚Ä¢	Dashboards
	‚Ä¢	Market monitoring

‚∏ª

üîß What to do immediately (step-by-step)

Step 1 ‚Äî Stop pulling P114

It will never give you what you want.

‚∏ª

Step 2 ‚Äî Use this instead (right now)

You already have this file: REGISTERED_BMUNITS_FILE https://downloads.elexonportal.co.uk/file/download/REGISTERED_BMUNITS_FILE?key=03omen6i9lhv5fa That gives you:
	‚Ä¢	All BM Units
	‚Ä¢	Current status
	‚Ä¢	Parties

‚∏ª

Step 3 ‚Äî Switch to API for live data

If you want, I can give you:
	‚Ä¢	Python script
	‚Ä¢	Google Sheets import
	‚Ä¢	BigQuery ingestion
	‚Ä¢	Exact API calls for every BM Unit

‚∏ª

üß† One-sentence truth

P114 is settlement archaeology.
BMRS API is where real BM Unit data lives.

‚∏ª

If you want next:

Tell me what ‚Äúall BM unit data‚Äù means to you:
	‚Ä¢	‚úî dispatch?
	‚Ä¢	‚úî bids/offers?
	‚Ä¢	‚úî generation by fuel?
	‚Ä¢	‚úî imbalance?
	‚Ä¢	‚úî historic back to X year?
	‚Ä¢	‚úî live monitoring?

I‚Äôll give you exact endpoints + ready-to-run code. Yes ‚Äî there is a full schema and catalogue of all available BMRS API data, and it‚Äôs published as part of the Insights Solution / BMRS API. Below is a structured overview of what exists, how it is defined, and where to find the full definitions.

‚∏ª

üìä 1. BMRS API ‚Äî Full List of Available Datasets

The BMRS API exposes a huge range of datasets, each corresponding to a different dataset code (e.g., B0610, B1610, etc.). You can request each dataset by code via the REST API. These are published under the Elexon Insights Solution API documentation.  Ôøº

Examples include:

System & Balancing Datasets
	‚Ä¢	PN ‚Äì Physical Notifications (dispatch for BM Units)
	‚Ä¢	BOALF ‚Äì Bid Offer Acceptance Level Flagged
	‚Ä¢	BOD ‚Äì Bid Offer Data
	‚Ä¢	CBS ‚Äì Current Balancing State
	‚Ä¢	FUELHH ‚Äì Half-hourly generation outturn by fuel type
	‚Ä¢	B1610 ‚Äì Actual Generation Output per Generation Unit
	‚Ä¢	MELS/MILS ‚Äì Maximum Export/Import Limits
	‚Ä¢	MID ‚Äì Market Index Data
	‚Ä¢	IMBALNGC ‚Äì Day and day-ahead indicated imbalance
	‚Ä¢	AGPT ‚Äì Actual Aggregated Generation Per Type
	‚Ä¢	AGWS ‚Äì Actual or Estimated Wind & Solar Generation
	‚Ä¢	SO-SO prices ‚Äì System Operator‚Äìto‚ÄìSystem Operator prices
	‚Ä¢	SYSWARN ‚Äì System warnings
	‚Ä¢	etc.‚Ä¶  Ôøº

Each dataset usually has two modes:
	‚Ä¢	GET /datasets/ ‚Äî returns historical or snapshot
	‚Ä¢	GET /datasets//stream ‚Äî returns the raw streaming output in minimal JSON format for performance and full detail  Ôøº

üìå Note: Some APIs have additional opinionated endpoints that summarize or transform the dataset for common use cases (e.g., aggregated demand by zone).  Ôøº

‚∏ª

üìò 2. Schema & Field Definitions

Elexon provides OpenAPI, JSON, YAML, and WADL specifications for the API, including:

‚úÖ the structure of each endpoint
‚úÖ available parameters
‚úÖ output fields and types
‚úÖ documentation text for each field

You can download these specifications directly from the official API Developer Portal:
üëâ https://developer.data.elexon.co.uk/  Ôøº

These specifications serve as the formal schema for every Dataset & Opinionated endpoint.

‚∏ª

üìå 3. Example API Schema for a Typical Dataset (High Level)

Below is an illustrative example. Exact field names vary per dataset, but the APIs follow consistent patterns.

Example: B1610 ‚Äî Actual Generation Output per Generation Unit

A typical response contains fields such as: ield
Description
settlementDate
The settlement date (e.g., 2025-12-31)
settlementPeriod
1‚Äì48 (half-hour slot)
powerSystemResourceType
Type of unit (e.g., Generation, Demand)
marketGenerationUnitEICCode
The EIC code of the unit
marketGenerationBMUId
BM Unit identifier
quantity
Actual generation (MW)
resolution
Resolution (e.g., PT30M)
local_datetime
Combined settlement date-period timestamp
‚Ä¶
and other metadata such as document ID/version
ield
Description
settlementDate
The settlement date (e.g., 2025-12-31)
settlementPeriod
1‚Äì48 (half-hour slot)
powerSystemResourceType
Type of unit (e.g., Generation, Demand)
marketGenerationUnitEICCode
The EIC code of the unit
marketGenerationBMUId
BM Unit identifier
quantity
Actual generation (MW)
resolution
Resolution (e.g., PT30M)
local_datetime
Combined settlement date-period timestamp
‚Ä¶
and other metadata such as document ID/version

Primary Data
B0610
Total Load per Bidding Zone
B0620
Day-Ahead Total Load Forecast
FUELHH
Generation outturn by fuel type
MELIMBALNGC
Imbalance & margin forecasts
AGWS
Wind & solar actual/estimated production
BOD
Bid-Offer Data (prices & volumes)
PN/Physical Notifications
Dispatch positions and statuses
MID
Market index price/time series
etc.
dataset-specific values
Opinionated endpoints
Aggregated or curated subsets (e.g., latest wind forecast)
 5. Full Definition Source ‚Äì BMRS API & Data Push Guide

Elexon also publishes a complete BMRS API and Data Push User Guide (PDF) with detailed definitions of:

‚úî API design
‚úî Search parameters
‚úî Field lists for each dataset
‚úî Response formats
‚úî Examples for CSV/XML/JSON responses
‚úî Comprehensive dataset list with codes & narratives  Ôøº

This document is very comprehensive (hundreds of pages) and is the authoritative schema reference.

‚∏ª

üß† 6. Best Way to Access & Use the Schema

There are several ways to programmatically get the full schema and definitions:

A. Direct OpenAPI specification

Download from developer portal ‚Äî contains full dataset schemas.

üìç Endpoint:
üëâ https://developer.data.elexon.co.uk/
(select ‚ÄúOpenAPI JSON/YAML‚Äù for the APIs you want)  Ôøº

B. Python client with autogenerated models

Install a community API client (auto-generated from the OpenAPI spec), which includes typed models for all endpoints: pip install elexon-bmrs pip install elexon-bmrs This includes ~280 typed models ‚Äî full schema coverage ‚Äî and IDE autocomplete.  Ôøº

C. ElexonDataPortal (Python wrapper)

Provides a wrapper around almost all BMRS endpoints with cleaned data output.  Ôøº

‚∏ª

üöÄ Because You Asked for everything available:

Here‚Äôs the definitive list of all dataset codes you can call from the API (partial extract; full list is available via docs / OpenAPI spec):
	‚Ä¢	ABUC, AGPT, AGWS
	‚Ä¢	AOBE
	‚Ä¢	ATL, B1610, BEB
	‚Ä¢	BOALF, BOD
	‚Ä¢	CBS
	‚Ä¢	CCM, CDN
	‚Ä¢	DAG, DATL
	‚Ä¢	DCI, DISBSAD
	‚Ä¢	FREQ
	‚Ä¢	FUELHH
	‚Ä¢	IGCA, IGCPU
	‚Ä¢	IMBALNGC
	‚Ä¢	MELIMBALNGC
	‚Ä¢	MELS, MILS
	‚Ä¢	MID
	‚Ä¢	NONBM
	‚Ä¢	PN
	‚Ä¢	PPBR
	‚Ä¢	QAS
	‚Ä¢	QPN
	‚Ä¢	REMIT
	‚Ä¢	SEL, SIL
	‚Ä¢	SYSWARN
	‚Ä¢	TEMP
	‚Ä¢	etc‚Ä¶  Ôøº

‚∏ª

üìå Summary

‚úî Schema definitions exist and are published via OpenAPI/WADL/JSON schema.  Ôøº
‚úî Every endpoint is documented with fields and types.  Ôøº
‚úî Full documentation + user guide is available downloadable as a PDF.  Ôøº
‚úî Community clients implement the schema as typed models (~280 endpoints).  Ôøº

‚∏ª

If you want next

I can do one of these:

üìå Generate a full CSV of every dataset and its fields
üìå Give you Python code to download and store all BMRS data
üìå Build a schema catalog in BigQuery or Sheets
üìå Provide endpoint examples with curl/Python/Sheets

Just tell me which format you want! here isn‚Äôt a 1:1 ‚Äúequivalent of P114‚Äù in the BMRS API, because P114 is a settlement-report dataflow (SAA/I014 subflows etc.), whereas BMRS is primarily operational / balancing-market reporting.  Ôøº

What is available is:
	‚Ä¢	P114 ‚Äúequivalent‚Äù access method (API/FTP/HTTPS download) ‚Äî still P114, just not FTP
	‚Ä¢	BMRS datasets that replace what people often try to use P114 for (BM Unit activity, acceptances, generation, prices, etc.)

Below is the clean mapping.

‚∏ª

1) If you literally mean ‚ÄúP114 but not FTP‚Äù

You already have the P114 HTTPS API on the Portal downloads domain (this is still P114):

List available P114 files (default = ‚Äútoday‚Äù)

https://downloads.elexonportal.co.uk/p114/list?key=YOURKEY

List by date

https://downloads.elexonportal.co.uk/p114/list?key=YOURKEY&date=YYYY-MM-DD

List by date + filter (these match what you saw: s0142, c0291, c0301, c0421)

https://downloads.elexonportal.co.uk/p114/list?key=YOURKEY&date=YYYY-MM-DD&filter=s0142

Download a file

https://downloads.elexonportal.co.uk/p114/download?key=YOURKEY&filename=FILENAME_FROM_LIST

Elexon explicitly documents that at least S0142 is available via the Portal through P114 download, and also via FTP or API (so the ‚ÄúAPI‚Äù here is the portal download interface you already have).  Ôøº

‚∏ª

2) If you mean ‚ÄúWhat is the BMRS equivalent of the information inside P114?‚Äù

That depends on which P114 subflow you care about.

Key point about S0142 (most commonly referenced)

S0142 is a Settlement Report subflow containing data ‚Äúon all BSC Parties ‚Ä¶ in relation to a Settlement Run‚Äù (i.e., a settlement-run view, not live ops).  Ôøº

So there is no single BMRS endpoint that reproduces an S0142 settlement-run report format, because BMRS doesn‚Äôt publish ‚Äúsettlement run‚Äù files like that; it publishes operational and market reporting datasets.  Ôøº

What most people really want when they ask ‚ÄúP114 equivalent‚Äù

They usually want BM Unit-level time series (dispatch, acceptances, generation, prices). For that, BMRS is the right replacement‚Äîjust not in ‚Äúone file‚Äù.

Use BMRS for:
	‚Ä¢	BM Unit master data / search
	‚Ä¢	Physical Notifications / FPN
	‚Ä¢	Bid-Offer Acceptances
	‚Ä¢	Outturn generation
	‚Ä¢	Imbalance prices / volumes

Those are documented in the BMRS API & Data Push Guide as the scope of BMRS operational data access.  Ôøº

‚∏ª

3) Practical ‚Äúequivalent‚Äù mapping (what to use instead of P114 for BM Unit data)

A) ‚ÄúAll BM units, and their attributes‚Äù

Use the Registered BM Units file (you already have it in your scriptable list), or BMRS BM Unit search endpoints (depending on what you need). BMRS covers ‚ÄúBM Unit Search‚Äù / derived BM Unit data among its legacy BMRS sections.  Ôøº

B) ‚ÄúBM Unit dispatch / intended positions‚Äù

Use BMRS Physical Data (PN/FPN style datasets).  Ôøº

C) ‚ÄúBM Unit actions: acceptances (BOAs)‚Äù

Use BMRS Latest Acceptances / Historic Acceptances (BM Unit-level).  Ôøº

D) ‚ÄúBM Unit generation outturn‚Äù

Use BMRS B1610 ‚Äì Actual Generation Output per Generation Unit.  Ôøº

E) ‚ÄúPrices, imbalance prices, volumes‚Äù

Use BMRS transparency datasets such as imbalance prices (e.g., B1770) and related volumes, as listed in the guide.  Ôøº

‚∏ª

4) What to do next (fastest path to what you asked)

Tell me which P114 filter you‚Äôre trying to replace:
	‚Ä¢	s0142
	‚Ä¢	c0291
	‚Ä¢	c0301
	‚Ä¢	c0421

‚Ä¶and what you mean by ‚Äúall BM unit data‚Äù (pick one):
	‚Ä¢	dispatch positions (PN/FPN)
	‚Ä¢	acceptances (BOAs)
	‚Ä¢	outturn generation
	‚Ä¢	bids/offers + acceptances
	‚Ä¢	imbalance price/volume per SP
	‚Ä¢	settlement-run reconciliation view

Then I‚Äôll give you the exact BMRS endpoints (and a ready-to-run curl/Python pull) that replicates the content you care about‚Äîeven though it won‚Äôt be packaged as a single P114 file.

One security note: don‚Äôt post your scripting key publicly again‚Äîtreat it like a password and rotate it if you think it‚Äôs been exposed.  es ‚Äî but the answer is ‚Äúall data that‚Äôs available‚Äù splits into three different worlds, each with different schemas, update lags, licensing, and intended use:
	1.	Insights Solution / BMRS APIs (modern, operational + transparency, mostly open)
	2.	Elexon Portal scripted downloads (daily files + BMRA archive + P114 settlement flows)
	3.	BSC ‚Äúdata flows‚Äù from BSC systems (where P114 sits conceptually)

Below is a research-grade map of what exists, what it‚Äôs called, what the fields mean (where to get definitions), and what it‚Äôs used for ‚Äî with the official places you should rely on for schema.

‚∏ª

1) Insights Solution (BMRS) APIs ‚Äî what data exists and what it‚Äôs for

Where the definitive schema/definitions live
	‚Ä¢	Endpoint catalogue + schemas + OpenAPI/WADL: the BMRS ‚ÄúAPI documentation‚Äù site describes this as a self-service area with ‚Äúschemas and standard definitions such as OpenAPI, WADL and codes.‚Äù  Ôøº
	‚Ä¢	Developer Portal: publishes API definitions in JSON / YAML / WADL for the Insights Solution APIs.  Ôøº
	‚Ä¢	BMRS API & Data Push User Guide (P408): the canonical ‚Äúwhat datasets exist + what they mean‚Äù document, including both ‚ÄúTransparency data‚Äù and ‚ÄúLegacy BMRS data.‚Äù  Ôøº

What types of data BMRS covers (big buckets)

A) Transparency / REMIT / Balancing & system fundamentals
The BMRS guide lists many ‚ÄúB-codes‚Äù datasets (e.g., imbalance prices, imbalance volumes, load, forecasts, generation capacity, unavailability, etc.).  Ôøº
Typical use cases:
	‚Ä¢	market analysis, forecasting, benchmarking
	‚Ä¢	compliance & transparency reporting
	‚Ä¢	price/imbalance drivers, reserve activation, system margin

Example (officially described):
	‚Ä¢	B1610 (Actual Generation Output per Generation Unit): ‚Äúactual metered volume output (MWh) per Settlement Period for all BM units,‚Äù published with a lag (‚Äúfive days after the end of the operational period‚Äù).  Ôøº

B) Operational / BMRS ‚Äúlegacy‚Äù style market mechanics
The BMRS guide includes ‚ÄúLegacy BMRS Data‚Äù sections such as:
	‚Ä¢	bid-offer data
	‚Ä¢	system warnings
	‚Ä¢	market index data
	‚Ä¢	acceptances (latest + historic)
	‚Ä¢	‚ÄúPhysical Data‚Äù, ‚ÄúDynamic Data‚Äù
	‚Ä¢	‚ÄúDerived BM Unit Data‚Äù and ‚ÄúBM Unit Search‚Äù  Ôøº

Typical use cases:
	‚Ä¢	reconstructing BM actions (acceptances)
	‚Ä¢	linking BM Units to fuel types / attributes
	‚Ä¢	studying system warnings and operational actions
	‚Ä¢	building near-real-time dashboards

C) Data Push service
Separate from REST, the guide also documents a Data Push (message-based streaming) model and how field types are defined.  Ôøº
Typical use cases:
	‚Ä¢	low-latency ingestion into message brokers / real-time pipelines

How to get ‚Äúfull schema + definitions‚Äù for BMRS properly

If you want everything, don‚Äôt scrape HTML pages:
	‚Ä¢	Use the Developer Portal OpenAPI (JSON/YAML) for machine-readable endpoint schemas  Ôøº
	‚Ä¢	Use the BMRS API & Data Push guide for human-readable field definitions, update lags, dataset purpose  Ôøº

‚∏ª

2) Elexon Portal scripted downloads ‚Äî what data exists and what it‚Äôs for

Where the definitive rules live

Elexon‚Äôs Portal ‚ÄúScripting‚Äù page explains:
	‚Ä¢	you authenticate using a 15-character scripting key
	‚Ä¢	only the listed URLs are allowed for automation
	‚Ä¢	‚Äúwith the exception of BMRA Data Archive and P114 files‚Äù, most listed files update no more than daily  Ôøº

What‚Äôs in this world (types of data)

A) Daily/regular CSV and reference files
Examples you already see: SSP/SBP/NIV files, TLM, RCRC, MID, Registered BM Units, etc.
Typical use cases:
	‚Ä¢	daily ingestion into internal pricing models
	‚Ä¢	simple, stable ‚Äúfile-based‚Äù pipelines
	‚Ä¢	reconciliation against known daily publications

B) BMRA Data Archive
This is a file archive of historic BMRA-era datasets (older report structures), accessed by filename via the portal download URL. The portal scripting guide explicitly treats BMRA archive as ‚Äúadded to frequently with additional files.‚Äù  Ôøº
Typical use cases:
	‚Ä¢	historic backfills
	‚Ä¢	matching legacy reporting formats

C) P114 files (Settlement flows)
Portal provides both:
	‚Ä¢	a list interface (returns available files by date/filter)
	‚Ä¢	a download interface (download by filename)
and notes that P114 is also available via FTP. (You saw this in your account page; the generic scripting rules are published here.)  Ôøº

Typical use cases:
	‚Ä¢	settlement-run auditing / reconciliation
	‚Ä¢	formal settlement reporting workflows (SAA/I014 subflows)

Important nuance: P114 is not ‚Äúlive BM Unit operational data.‚Äù It‚Äôs settlement-oriented, so it can appear ‚Äúold‚Äù because it‚Äôs tied to settlement run calendars and processes.

‚∏ª

3) P114 specifically ‚Äî what it is, what data items exist, what it‚Äôs used for

What P114 is (official framing)

Elexon refers to P114 as a ‚Äúdataflow‚Äù published on the Portal, and gives examples like:
	‚Ä¢	‚ÄúSAA-I014 Settlement Report subflow 2 (S0142)‚Äù published on the Portal and versioned (e.g., updated to version 11).  Ôøº

There‚Äôs also an Elexon consultation about ‚ÄúP114 data items‚Äù and aligning them under BSC open/public data licensing, explicitly to improve delivery avenues and provide standardised metadata.  Ôøº

Where you get definitions / field meanings for P114

This is the key: P114 field definitions are not best learned from BMRS API docs.
You typically need:
	‚Ä¢	the P114 flow/subflow specification (SAA-I014, subflow docs) and/or
	‚Ä¢	the BSC ‚Äúdata flows‚Äù documentation that explains what each data flow is for  Ôøº
	‚Ä¢	Elexon‚Äôs change notes for the subflow versions (e.g., the S0142 update article)  Ôøº

(If you want, I can pull the specific S0142/c0291/c0301/c0421 documentation trail and summarise each subflow‚Äôs purpose + key fields; it‚Äôs usually spread across BSC documentation and Portal context.)

‚∏ª

4) What ‚Äúequivalent for P114‚Äù actually means in practice

There are two common interpretations:

A) You want the same files, but easier access

Then your ‚Äúequivalent‚Äù is: Portal P114 list/download HTTPS (which you already have) + optional FTP. This is still P114; it just avoids interactive login.  Ôøº

B) You want the business content people mistakenly try to get from P114 (BM Unit operational data)

Then your ‚Äúequivalent‚Äù is usually BMRS datasets, e.g.:
	‚Ä¢	B1610 for BM Unit metered output (with lag)  Ôøº
	‚Ä¢	Acceptances / physical data / derived BM unit data / BM unit search (as documented in the BMRS guide‚Äôs legacy sections)  Ôøº

‚∏ª

5) How I‚Äôd package ‚Äúall available data‚Äù into something usable for you

If your goal is a definitive inventory you can build pipelines from, the clean deliverable is:
	1.	BMRS/Insights Solution catalogue

	‚Ä¢	endpoint name
	‚Ä¢	dataset code (where applicable)
	‚Ä¢	parameters
	‚Ä¢	response schema (from OpenAPI)
	‚Ä¢	update lag/frequency + narrative definition (from BMRS guide)
Sources: Developer Portal + BMRS guide  Ôøº

	2.	Portal scripted files catalogue

	‚Ä¢	filename / URL key
	‚Ä¢	update frequency notes
	‚Ä¢	intended use
Source: Portal scripting guide  Ôøº

	3.	P114 catalogue

	‚Ä¢	subflows (S0142, etc.)
	‚Ä¢	what settlement process they represent
	‚Ä¢	file naming patterns + filters
Sources: P114 update notes + data flows documentation + P114 data-items consultation  Ôøº

‚∏ª

Quick safety note (important)

Don‚Äôt share your scripting key in messages going forward. Treat it like a password. The Portal itself warns it authenticates scripted downloads.  Ôøº

‚∏ª

If you tell me which output format you want, I can lay this out as:
	‚Ä¢	a Google Sheet inventory (datasets √ó fields √ó definitions √ó update lag), or
	‚Ä¢	a BigQuery schema catalogue, or
	‚Ä¢	a markdown ‚Äúdata dictionary‚Äù you can drop into your internal docs.

Given your ‚Äúall BM unit data‚Äù aim: I‚Äôd recommend the inventory as a sheet + API-first mapping, then a smaller ‚ÄúP114-only‚Äù appendix for settlement reconciliation. Because I misread ‚Äúall data that‚Äôs available‚Äù as ‚Äúeverything Elexon/NESO publishes,‚Äù and I went broad to be safe. You were actually asking something much narrower:

‚ÄúWhere do I find the equivalent of P114, and what‚Äôs available around P114 (schemas/definitions/usable alternatives)?‚Äù

So‚Äîignore the NESO angle. Let‚Äôs keep it tightly on P114 and the closest functional replacements.

What you‚Äôre really dealing with

P114 is a Portal/BSC settlement dataflow

It‚Äôs a file-based settlement reporting feed (with subflows like s0142, etc.), delivered via:
	‚Ä¢	Portal HTTPS ‚Äúlist/download‚Äù (what your scripting page shows)
	‚Ä¢	FTP (optional)
	‚Ä¢	It‚Äôs not a modern REST dataset with a neat ‚ÄúOpenAPI schema‚Äù.

That‚Äôs why you‚Äôre seeing ‚Äúold‚Äù data: it‚Äôs tied to settlement runs and publication cycles, not live ops.

Where to find P114 definitions (the right places)
	1.	On the Portal pages for P114

	‚Ä¢	The P114 section typically shows file lists, subflow names, and filename patterns.
	‚Ä¢	Your own scripting page even says: hover over links to see filename formats and use the list endpoint.

	2.	BSC dataflow documentation (SAA-I014 / subflow specs)

	‚Ä¢	This is where the field-level definitions live (what each column/data item means, units, settlement run context, etc.).
	‚Ä¢	P114 is effectively ‚Äúa delivery mechanism for these flow specs.‚Äù

	3.	Elexon change notes / version updates

	‚Ä¢	Like the S0142 update page you referenced earlier‚Äîuseful for ‚Äúwhat changed and when‚Äù.

What I should do next (correctly)

If you tell me which of these you mean by ‚Äúequivalent‚Äù:
	‚Ä¢	A) ‚ÄúP114 but easier/more complete access‚Äù ‚Üí we stick with Portal list/download + FTP
	‚Ä¢	B) ‚ÄúI want BM Unit operational data, not settlement report files‚Äù ‚Üí we map your target outputs to BMRS datasets (acceptances, physical notifications, B1610 outturn, etc.)

‚Ä¶then I‚Äôll produce a P114-focused data dictionary:
	‚Ä¢	what each subflow (s0142, c0291, c0301, c0421) is for
	‚Ä¢	what kind of records it contains
	‚Ä¢	how fresh it should be (publication timing)
	‚Ä¢	what you should use instead if your goal is BM Unit activity

Quick question-free next step

Paste me the output of:
https://downloads.elexonportal.co.uk/p114/list?key=YOURKEY&date=YYYY-MM-DD

(You can redact the key.)
From the filenames returned, I can infer exactly which subflows are present and give you the right ‚Äúequivalent mapping‚Äù without guessing. Got it. You want two things:
	1.	What‚Äôs the ‚ÄúP114 equivalent‚Äù (i.e., where to get the same kind of data without relying on P114 files), and
	2.	A full CSV data dictionary: every dataset / endpoint and every field, with types.

I‚Äôll keep this tightly on Elexon P114 vs Elexon Insights/BMRS, not NESO.

‚∏ª

1) ‚ÄúP114 equivalent‚Äù ‚Äî what it is (and isn‚Äôt)

There is no 1:1 API equivalent of ‚ÄúP114‚Äù

P114 is a settlement-report file flow (SAA/I014 subflows like S0142 etc.), delivered as files. The modern Insights Solution APIs are organised as:
	‚Ä¢	Dataset endpoints (raw ‚Äúas received from the data provider‚Äù)
	‚Ä¢	Opinionated endpoints (derived, simplified)
	‚Ä¢	Reference data (standing data like BM Units)  Ôøº

Those are different product types. So: if you need settlement-run artefacts, P114 itself is the ‚Äúequivalent‚Äù (but you can access it via HTTPS list/download as you‚Äôve already found).

What people usually mean by ‚ÄúP114 equivalent‚Äù

They often want BM Unit level operational/market data (PN/FPN, acceptances, limits, etc.) rather than settlement-run reports. For that, the equivalent is:

A) Reference BM Unit list (standing data)
Use the Reference data endpoints (the Developer Portal explicitly calls out ‚ÄúReference data ‚Ä¶ standing data held‚Äù).  Ôøº
This is the clean replacement for ‚ÄúI want all BM Units and their attributes‚Äù.

B) BM Unit operational feeds
Use Dataset endpoints such as PN / BOAL / BOD / MELS / MILS etc. (listed directly on the Developer Portal home page).  Ôøº

C) Historic transparency datasets (if you‚Äôre replacing specific settlement-derived measures)
Use the BMRS API & Data Push User Guide (P408) dataset catalogue: it lists what‚Äôs available and the meaning/purpose of each dataset family (Transparency/REMIT + Legacy BMRS).  Ôøº

Rule of thumb:
	‚Ä¢	Need settlement run reconciliation ‚Üí P114 files
	‚Ä¢	Need ‚Äúall BM Unit data‚Äù for analysis/ops ‚Üí Insights APIs (datasets + reference)  Ôøº

‚∏ª

2) ‚ÄúCreate a full CSV of every dataset and its fields‚Äù

You can absolutely generate this, but the only reliable way is:

Source of truth for ‚Äúevery dataset + every field‚Äù
	‚Ä¢	The Insights Solution Developer Portal provides ‚Äúopen standard API definitions in JSON, YAML and WADL‚Äù (i.e., OpenAPI specs).  Ôøº
Those specs are what we should parse to produce your CSV.

Why I‚Äôm not pasting the CSV directly here

The Developer Portal pages that host the downloadable OpenAPI specs are rendered via JavaScript (Azure APIM), so I can‚Äôt fetch the full OpenAPI JSON through this chat tool reliably. But you can download the spec in a browser in seconds, and then run the script below to produce the CSV.

‚∏ª

Do this now: generate the CSV data dictionary (endpoints + fields)

Step 1 ‚Äî Download the OpenAPI JSON/YAML from the Developer Portal

Go to: https://developer.data.elexon.co.uk/  Ôøº

In the ‚ÄúAPIs‚Äù area, download the OpenAPI JSON (or YAML) for the Insights APIs.

You‚Äôll end up with something like:
	‚Ä¢	insights-openapi.json  (or .yaml)

Step 2 ‚Äî Run this Python script to create elexon_insights_data_dictionary.csv #!/usr/bin/env python3
"""
Generate a CSV data dictionary from an Insights Solution OpenAPI spec.
Outputs one row per: endpoint + response-field (flattened).
"""

import csv
import json
import sys
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple, Union

try:
    import yaml  # pip install pyyaml
except Exception:
    yaml = None


JsonObj = Dict[str, Any]


def load_spec(path: Path) -> JsonObj:
    text = path.read_text(encoding="utf-8")
    if path.suffix.lower() in [".yaml", ".yml"]:
        if yaml is None:
            raise RuntimeError("YAML spec provided but PyYAML not installed. Run: pip install pyyaml")
        return yaml.safe_load(text)
    return json.loads(text)


def ref_name(ref: str) -> str:
    # "#/components/schemas/Foo" -> "Foo"
    return ref.split("/")[-1].strip()


def resolve_ref(spec: JsonObj, schema: JsonObj) -> JsonObj:
    """Resolve a $ref if present; otherwise return schema."""
    if "$ref" in schema:
        name = ref_name(schema["$ref"])
        return spec.get("components", {}).get("schemas", {}).get(name, {})
    return schema


def merge_allOf(spec: JsonObj, schema: JsonObj) -> JsonObj:
    """Merge allOf schemas into one shallow schema."""
    schema = resolve_ref(spec, schema)
    if "allOf" not in schema:
        return schema
    merged: JsonObj = {"type": "object", "properties": {}}
    for part in schema["allOf"]:
        part_res = merge_allOf(spec, part)
        merged["properties"].update(part_res.get("properties", {}))
    return merged


def schema_type(schema: JsonObj) -> str:
    t = schema.get("type")
    if t:
        return str(t)
    if "properties" in schema:
        return "object"
    if "items" in schema:
        return "array"
    if "$ref" in schema:
        return f"ref:{ref_name(schema['$ref'])}"
    return ""


def flatten_schema(
    spec: JsonObj,
    schema: JsonObj,
    prefix: str = "",
    max_depth: int = 8,
    depth: int = 0,
) -> List[Tuple[str, str, str]]:
    """
    Return list of (field_path, field_type, description).
    """
    if depth > max_depth:
        return [(prefix, "max_depth_reached", "")]

    schema = merge_allOf(spec, schema)
    schema = resolve_ref(spec, schema)

    out: List[Tuple[str, str, str]] = []

    # arrays
    if schema.get("type") == "array" or "items" in schema:
        items = schema.get("items", {})
        item_type = schema_type(resolve_ref(spec, items))
        desc = (schema.get("description") or "").strip()
        path = f"{prefix}[]" if prefix else "[]"
        out.append((path, f"array<{item_type}>", desc))
        out.extend(flatten_schema(spec, items, prefix=path, max_depth=max_depth, depth=depth + 1))
        return out

    # objects
    props = schema.get("properties", {})
    if props:
        for k, v in props.items():
            v_res = resolve_ref(spec, v)
            p = f"{prefix}.{k}" if prefix else k
            desc = (v_res.get("description") or v.get("description") or "").strip()
            t = schema_type(v_res) or schema_type(v)
            out.append((p, t, desc))
            # Recurse if object/array/ref
            if v_res.get("type") in ("object", "array") or "properties" in v_res or "items" in v_res or "$ref" in v:
                out.extend(flatten_schema(spec, v, prefix=p, max_depth=max_depth, depth=depth + 1))
        return out

    # primitives / unknown
    desc = (schema.get("description") or "").strip()
    t = schema_type(schema)
    if prefix:
        out.append((prefix, t, desc))
    return out


def pick_json_response_schema(op: JsonObj) -> Optional[JsonObj]:
    """
    Find a JSON response schema (prefer 200, then 201, then default).
    """
    responses = op.get("responses", {})
    for code in ("200", "201", "202", "default"):
        r = responses.get(code)
        if not r:
            continue
        content = r.get("content", {})
        for ctype in ("application/json", "text/json", "application/*+json"):
            if ctype in content and "schema" in content[ctype]:
                return content[ctype]["schema"]
        # fallback: take any content schema
        for _, cv in content.items():
            if "schema" in cv:
                return cv["schema"]
    return None


def main():
    if len(sys.argv) < 2:
        print("Usage: python make_dictionary.py <openapi.json|openapi.yaml> [output.csv]")
        sys.exit(1)

    spec_path = Path(sys.argv[1]).expanduser().resolve()
    out_path = Path(sys.argv[2]).expanduser().resolve() if len(sys.argv) >= 3 else Path("elexon_insights_data_dictionary.csv")

    spec = load_spec(spec_path)

    rows: List[Dict[str, str]] = []
    paths = spec.get("paths", {})

    for path, methods in paths.items():
        for method, op in methods.items():
            if method.lower() not in ("get", "post", "put", "delete", "patch"):
                continue

            op = op or {}
            op_id = str(op.get("operationId", ""))
            summary = str(op.get("summary", ""))
            tag = ""
            if isinstance(op.get("tags"), list) and op["tags"]:
                tag = str(op["tags"][0])

            # Request params (names only, for quick filtering)
            param_names = []
            for p in op.get("parameters", []) or []:
                if isinstance(p, dict) and "name" in p:
                    param_names.append(str(p["name"]))
            params_str = ",".join(param_names)

            schema = pick_json_response_schema(op)
            if not schema:
                # Still output endpoint row even if no JSON schema
                rows.append({
                    "tag": tag,
                    "path": path,
                    "method": method.upper(),
                    "operationId": op_id,
                    "summary": summary,
                    "request_params": params_str,
                    "field_path": "",
                    "field_type": "",
                    "field_description": "",
                })
                continue

            fields = flatten_schema(spec, schema)
            if not fields:
                rows.append({
                    "tag": tag,
                    "path": path,
                    "method": method.upper(),
                    "operationId": op_id,
                    "summary": summary,
                    "request_params": params_str,
                    "field_path": "",
                    "field_type": "",
                    "field_description": "",
                })
                continue

            for field_path, field_type, field_desc in fields:
                rows.append({
                    "tag": tag,
                    "path": path,
                    "method": method.upper(),
                    "operationId": op_id,
                    "summary": summary,
                    "request_params": params_str,
                    "field_path": field_path,
                    "field_type": field_type,
                    "field_description": field_desc,
                })

    # Write CSV
    with out_path.open("w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(
            f,
            fieldnames=[
                "tag",
                "path",
                "method",
                "operationId",
                "summary",
                "request_params",
                "field_path",
                "field_type",
                "field_description",
            ],
        )
        writer.writeheader()
        writer.writerows(rows)

    print(f"Wrote {len(rows):,} rows to {out_path}")


if __name__ == "__main__":
    main() python make_dictionary.py insights-openapi.json
# or
python make_dictionary.py insights-openapi.yaml It outputs:
	‚Ä¢	elexon_insights_data_dictionary.csv

This CSV will contain every endpoint plus every response field (flattened), including field types where the OpenAPI declares them. The reason this works is the Developer Portal explicitly publishes the OpenAPI definitions.  Ôøº

‚∏ª

Next: I can also give you a P114 ‚Üí Insights mapping table

If you tell me which P114 subflows you‚Äôre using (s0142, c0291, c0301, c0421), I‚Äôll produce a practical mapping like:
	‚Ä¢	‚ÄúIf you used P114 for X ‚Üí use Insights endpoint(s) Y‚Äù
	‚Ä¢	including ‚ÄúBM Unit standing data‚Äù vs ‚Äúoperational‚Äù vs ‚Äúhistoric transparency‚Äù (aligned to the BMRS guide catalogue).  Ôøº

One more thing: please don‚Äôt paste your scripting key again‚Äîtreat it like a password.  You‚Äôre asking for a P114 ‚Üí Insights (BMRS/Insights Solution) mapping table. The key is: P114 is a delivery package that contains multiple ‚Äútypes‚Äù of settlement files, and only some of what‚Äôs inside has any practical ‚Äúequivalent‚Äù in Insights APIs.

From Elexon‚Äôs own Portal FAQ, the P114 FTP folders contain four file families:
	‚Ä¢	SAA-I014 (subflow 2) ‚Üí S0142
	‚Ä¢	CDCA-I029 ‚Üí C0291
	‚Ä¢	CDCA-I030 ‚Üí C0301
	‚Ä¢	CDCA-I042 ‚Üí C0421  Ôøº

And Elexon confirms S0142 is the NETSO copy of the settlement report (subflow 2), covering all BSC parties for a settlement run and is available via Portal P114.  Ôøº

‚∏ª

P114 ‚Üí Insights mapping table

Legend
	‚Ä¢	‚úÖ = good functional replacement in Insights (API-native)
	‚Ä¢	‚ö†Ô∏è = partial/approximate replacement (different definition / not settlement-run)
	‚Ä¢	‚ùå = no real replacement; you must use P114 / settlement flows (or IDD specs)P114 filter
Underlying flow
What it is (plain English)
Typical use
Closest ‚ÄúInsights equivalent‚Äù
Replacement quality
s0142
SAA-I014 subflow 2
Settlement Report to NETSO: ‚Äúcontains data on all BSC Parties in relation to a Settlement Run‚Äù
Settlement-run reconciliation / audit; settlement debits/credits context; ‚Äúwhat the settlement system did‚Äù
There is no single Insights endpoint that reproduces a settlement-run report. Insights is operational/transparency; SAA settlement-run packaging is a different product.
‚ùå
c0291
CDCA-I029
Metered energy aggregated at GSP Group level (Elexon describes it as ‚Äúenergy consumption by each regional GSP Group‚Äù)
Regional demand allocation, historic settlement metered demand at GSP-group granularity
Insights has demand datasets, but they‚Äôre generally system/outturn/forecast style‚Äînot CDCA settlement metered ‚ÄúGSP Group Take Volumes‚Äù.
‚ö†Ô∏è / often ‚ùå for strict settlement equivalence
c0301
CDCA-I030
Metered energy at each Distribution System Connection Point (effectively per GSP / connection-point metered volumes)
GSP-level net demand analysis, regional breakdowns, losses studies
Same issue: Insights demand endpoints may be usable for analytics, but not a like-for-like settlement metered flow replacement.
‚ö†Ô∏è / often ‚ùå
c0421
CDCA-I042
Metered volumes for each BM Unit per settlement period for each aggregation run/day
BM Unit metered energy (settlement), historic/unit-level settlement metering
Closest analytics substitute is BMRS/Insights generation/outturn datasets (e.g., ‚ÄúActual Generation Output per Generation Unit‚Äù style datasets), but these are not the same settlement flow and can differ in timing/definitions.
‚ö†Ô∏è
Bottom line
	‚Ä¢	If you need settlement-run truth (SAA/CDCA flows): P114 is the source of record.
	‚Ä¢	If you need operational / market analytics: Insights APIs are the right tool ‚Äî but they won‚Äôt replicate settlement packaging.

‚∏ª

‚ÄúSo what should I do in practice?‚Äù

If your goal is settlement-grade BM Unit metered volumes

Use P114 c0421 (CDCA-I042). It‚Äôs explicitly the BM Unit settlement metered volume feed.  Ôøº

If your goal is ‚Äúall BM Unit operational data‚Äù (not settlement)

Then you usually don‚Äôt want P114 at all; you want Insights/BMRS datasets (PN/FPN/BOAs/MEL/MID etc.). Those are different questions.

‚∏ª

Next step: I can make this mapping ‚Äúactionable‚Äù for your pipeline

Tell me which output you‚Äôre trying to build:
	1.	BM Unit metered energy by SP (settlement) ‚Üí P114 c0421 (+ I can help you parse/normalise)
	2.	BM Unit actions (acceptances/dispatch) ‚Üí Insights/BMRS operational datasets
	3.	GSP/GSP-group demand ‚Üí P114 c0291/c0301 vs Insights demand datasets (analytics)

If you pick one, I‚Äôll produce:
	‚Ä¢	the exact ‚Äútarget tables‚Äù you should build (BMU/SP schema)
	‚Ä¢	how to join to reference data (BMU attributes)
	‚Ä¢	and the recommended ingestion route (Portal list/download vs API).Absolutely ‚Äî here are all three, with:
	‚Ä¢	target tables (clean BMU/SP schemas you can build in BigQuery/Sheets)
	‚Ä¢	join strategy to reference data (BM Unit attributes)
	‚Ä¢	recommended ingestion routes (Portal P114 list/download vs Insights APIs)

I‚Äôm going to recommend a 2-layer design everywhere:
	1.	Raw landing tables (store exactly what you downloaded / received)
	2.	Normalised tables (analytics-ready, stable schema)

‚∏ª

1) BM Unit metered energy by SP (settlement) ‚Üí P114 c0421 (CDCA-I042)

What it is

c0421 is the CDCA BM Unit Aggregation Report: metered energy volumes for each BM Unit by settlement period / run. (This is the settlement-grade ‚ÄúBM Unit metered volume‚Äù concept, QMij.)  Ôøº

Recommended ingestion route

Portal HTTPS (best):
	‚Ä¢	List files: .../p114/list?key=...&date=YYYY-MM-DD&filter=c0421
	‚Ä¢	Download file by filename: .../p114/download?key=...&filename=...

This is the most robust route because it doesn‚Äôt depend on FTP and is explicitly supported for scripting (as you saw in your account page).

Target tables to build

A) p114_c0421_files (file inventory) olumn
type
notes
file_date
DATE
date you requested in the list endpoint
filename
STRING
from the list response
filter
STRING
always c0421
downloaded_at
TIMESTAMP
ingestion time
source_url
STRING
download URL (without key)
checksum
STRING
optional (dedupe)
raw_bytes
BYTES / STRING
optional (store raw)
B) p114_c0421_raw_rows (raw parsed rows)

Store every row as parsed fields + a JSON blob of the original row. column
type
notes
filename
STRING
join to file inventory
settlement_date
DATE
from file content
settlement_period
INT
1‚Äì48 (or 46/50 on clock-change days)
bm_unit_id
STRING
BM Unit identifier
metered_mwh
NUMERIC
settlement metered energy for BMU+SP (QMij equivalent)
run_type
STRING
if present (SF/RF/DF etc.)
run_id
STRING
if present (aggregation run identifier)
raw_row_json
JSON
full original row for traceability
column
type
notes
filename
STRING
join to file inventory
settlement_date
DATE
from file content
settlement_period
INT
1‚Äì48 (or 46/50 on clock-change days)
bm_unit_id
STRING
BM Unit identifier
metered_mwh
NUMERIC
settlement metered energy for BMU+SP (QMij equivalent)
run_type
STRING
if present (SF/RF/DF etc.)
run_id
STRING
if present (aggregation run identifier)
raw_row_json
JSON
full original row for traceability
column
type
notes
settlement_date
DATE
settlement_period
INT
bm_unit_id
STRING
metered_mwh
NUMERIC
settlement metered MWh
run_id
STRING
nullable
run_type
STRING
nullable
source_filename
STRING
lineage
loaded_at
TIMESTAMP
lineage
114 filter
Underlying flow
What it is (plain English)
Typical use
Closest ‚ÄúInsights equivalent‚Äù
Replacement quality
s0142
SAA-I014 subflow 2
Settlement Report to NETSO: ‚Äúcontains data on all BSC Parties in relation to a Settlement Run‚Äù
Settlement-run reconciliation / audit; settlement debits/credits context; ‚Äúwhat the settlement system did‚Äù
There is no single Insights endpoint that reproduces a settlement-run report. Insights is operational/transparency; SAA settlement-run packaging is a different product.
‚ùå
c0291
CDCA-I029
Metered energy aggregated at GSP Group level (Elexon describes it as ‚Äúenergy consumption by each regional GSP Group‚Äù)
Regional demand allocation, historic settlement metered demand at GSP-group granularity
Insights has demand datasets, but they‚Äôre generally system/outturn/forecast style‚Äînot CDCA settlement metered ‚ÄúGSP Group Take Volumes‚Äù.
‚ö†Ô∏è / often ‚ùå for strict settlement equivalence
c0301
CDCA-I030
Metered energy at each Distribution System Connection Point (effectively per GSP / connection-point metered volumes)
GSP-level net demand analysis, regional breakdowns, losses studies
Same issue: Insights demand endpoints may be usable for analytics, but not a like-for-like settlement metered flow replacement.
‚ö†Ô∏è / often ‚ùå
c0421
CDCA-I042
Metered volumes for each BM Unit per settlement period for each aggregation run/day
BM Unit metered energy (settlement), historic/unit-level settlement metering
Closest analytics substitute is BMRS/Insights generation/outturn datasets (e.g., ‚ÄúActual Generation Output per Generation Unit‚Äù style datasets), but these are not the same settlement flow and can differ in timing/definitions.
‚ö†Ô∏è
Bottom line
	‚Ä¢	If you need settlement-run truth (SAA/CDCA flows): P114 is the source of record.
	‚Ä¢	If you need operational / market analytics: Insights APIs are the right tool ‚Äî but they won‚Äôt replicate settlement packaging.

‚∏ª

‚ÄúSo what should I do in practice?‚Äù

If your goal is settlement-grade BM Unit metered volumes

Use P114 c0421 (CDCA-I042). It‚Äôs explicitly the BM Unit settlement metered volume feed.  Ôøº

If your goal is ‚Äúall BM Unit operational data‚Äù (not settlement)

Then you usually don‚Äôt want P114 at all; you want Insights/BMRS datasets (PN/FPN/BOAs/MEL/MID etc.). Those are different questions.

‚∏ª

Next step: I can make this mapping ‚Äúactionable‚Äù for your pipeline

Tell me which output you‚Äôre trying to build:
	1.	BM Unit metered energy by SP (settlement) ‚Üí P114 c0421 (+ I can help you parse/normalise)
	2.	BM Unit actions (acceptances/dispatch) ‚Üí Insights/BMRS operational datasets
	3.	GSP/GSP-group demand ‚Üí P114 c0291/c0301 vs Insights demand datasets (analytics)

If you pick one, I‚Äôll produce:
	‚Ä¢	the exact ‚Äútarget tables‚Äù you should build (BMU/SP schema)
	‚Ä¢	how to join to reference data (BMU attributes)
	‚Ä¢	and the recommended ingestion route (Portal list/download vs API). Absolutely ‚Äî here are all three, with:
	‚Ä¢	target tables (clean BMU/SP schemas you can build in BigQuery/Sheets)
	‚Ä¢	join strategy to reference data (BM Unit attributes)
	‚Ä¢	recommended ingestion routes (Portal P114 list/download vs Insights APIs)

I‚Äôm going to recommend a 2-layer design everywhere:
	1.	Raw landing tables (store exactly what you downloaded / received)
	2.	Normalised tables (analytics-ready, stable schema)

‚∏ª

1) BM Unit metered energy by SP (settlement) ‚Üí P114 c0421 (CDCA-I042)

What it is

c0421 is the CDCA BM Unit Aggregation Report: metered energy volumes for each BM Unit by settlement period / run. (This is the settlement-grade ‚ÄúBM Unit metered volume‚Äù concept, QMij.)  Ôøº

Recommended ingestion route

Portal HTTPS (best):
	‚Ä¢	List files: .../p114/list?key=...&date=YYYY-MM-DD&filter=c0421
	‚Ä¢	Download file by filename: .../p114/download?key=...&filename=...

This is the most robust route because it doesn‚Äôt depend on FTP and is explicitly supported for scripting (as you saw in your account page).

Target tables to build

A) p114_c0421_files (file inventory) olumn
type
notes
file_date
DATE
date you requested in the list endpoint
filename
STRING
from the list response
filter
STRING
always c0421
downloaded_at
TIMESTAMP
ingestion time
source_url
STRING
download URL (without key)
checksum
STRING
optional (dedupe)
raw_bytes
BYTES / STRING
optional (store raw)
) p114_c0421_raw_rows (raw parsed rows)

Store every row as parsed fields + a JSON blob of the original row. column
type
notes
filename
STRING
join to file inventory
settlement_date
DATE
from file content
settlement_period
INT
1‚Äì48 (or 46/50 on clock-change days)
bm_unit_id
STRING
BM Unit identifier
metered_mwh
NUMERIC
settlement metered energy for BMU+SP (QMij equivalent)
run_type
STRING
if present (SF/RF/DF etc.)
run_id
STRING
if present (aggregation run identifier)
raw_row_json
JSON
full original row for traceability
C) bm_unit_metered_mwh_sp (normalised ‚Äútruth‚Äù table)

This is the one you‚Äôll actually query.

Primary key (recommended):
(settlement_date, settlement_period, bm_unit_id, run_id)
If run_id isn‚Äôt available in the file, use (settlement_date, settlement_period, bm_unit_id, filename). column
type
notes
settlement_date
DATE
settlement_period
INT
bm_unit_id
STRING
metered_mwh
NUMERIC
settlement metered MWh
run_id
STRING
nullable
run_type
STRING
nullable
source_filename
STRING
lineage
loaded_at
TIMESTAMP
lineage
Joining to reference data (BMU attributes)

Join bm_unit_id to Insights reference BM Units:
	‚Ä¢	Insights endpoint: GET /reference/bmunits/all (current list of BM Units)  Ôøº

Create:
	‚Ä¢	ref_bm_units snapshot table (daily)
	‚Ä¢	Join keys: bm_unit_id (or equivalent field name returned)

‚∏ª

2) BM Unit actions (acceptances/dispatch) ‚Üí Insights/BMRS operational datasets

This is the ‚Äúwhat the BM did / what units were instructed‚Äù world ‚Äî it‚Äôs not settlement-run packaged, and Insights is the right source.

Recommended ingestion route

Use Insights APIs (no FTP; public APIs are documented on the BMRS API documentation site and the Developer Portal).  Ôøº

A) Dispatch / intended positions: Physical Notifications (PN)
	‚Ä¢	Dataset endpoint: GET /datasets/PN (and /datasets/PN/stream)  Ôøº
PN is defined as the participant‚Äôs best estimate of generation/demand for a BM Unit per settlement period.  Ôøº

Target tables

insights_pn_raw
type
settlement_date
DATE
settlement_period
INT
bm_unit_id
STRING
pn_mw
NUMERIC
received_at
TIMESTAMP
raw_json
JSON
bm_unit_dispatch_sp
Normalised view you query: umn
type
notes
settlement_date
DATE
settlement_period
INT
bm_unit_id
STRING
pn_mw
NUMERIC
best estimate for that SP
source
STRING
‚ÄúPN‚Äù
loaded_at
TIMESTAMP
B) Acceptances (BOAs): Bid-Offer Acceptances

You can pull acceptances via:
	‚Ä¢	GET /balancing/acceptances/{acceptanceNumber}  Ôøº
‚Ä¶and/or use the dataset-style acceptances feed where applicable (many users use datasets like BOALF/BOD for bulk). The Insights catalogue lists BOALF and BOD under datasets.  Ôøº

Target tables

insights_acceptances_raw column
type
notes
acceptance_id
INT
from {acceptanceNumber}
bm_unit_id
STRING
settlement_date
DATE
settlement_period
INT
direction
STRING
bid/offer
accepted_mw
NUMERIC
price
NUMERIC
if present
raw_json
JSON
bm_unit_acceptances_sp olumn
type
settlement_date
DATE
settlement_period
INT
bm_unit_id
STRING
acceptance_id
INT
accepted_mw
NUMERIC
price
NUMERIC
loaded_at
TIMESTAMP
Joining to reference data

Same as above:
	‚Ä¢	GET /reference/bmunits/all  Ôøº

You‚Äôll typically want to add:
	‚Ä¢	fuel type / technology
	‚Ä¢	party / operator
	‚Ä¢	active dates (for backfills)

‚∏ª

3) GSP / GSP-group demand ‚Üí P114 c0291/c0301 vs Insights demand datasets

There are two different ‚Äútruths‚Äù depending on your use case:

A) Settlement-grade regional metered demand (CDCA) ‚Üí P114
	‚Ä¢	Use P114 c0291 / c0301 when you need settlement-relevant GSP / GSP-group volumes (metered, corrected, used in settlement processes).
These are CDCA-derived and sit within the CDCA service scope (collect/validate/aggregate metered data for settlement timescales).  Ôøº
GSP Group Take / corrections are explicitly BSC constructs used in settlement calculations.  Ôøº

Target tables

p114_c0291_gsp_group_take_sp olumn
type
notes
settlement_date
DATE
settlement_period
INT
gsp_group_id
STRING
e.g., _A, _B‚Ä¶
energy_mwh
NUMERIC
metered/aggregated (settlement basis)
run_id
STRING
nullable
source_filename
STRING
p114_c0301_gsp_take_sp (if it‚Äôs at GSP/connection-point granularity)
| column | type |
|‚Äî|‚Äî|‚Äî|
| settlement_date | DATE |
| settlement_period | INT |
| gsp_id | STRING |
| energy_mwh | NUMERIC |
| run_id | STRING |
| source_filename | STRING |

Join strategy:
	‚Ä¢	Use BSC reference mappings for GSP ‚Üí GSP Group and any correction factors/scaling weights (depending on what you‚Äôre doing). The concept and usage are described in Elexon‚Äôs guidance on GSP Group Take and corrections.  Ôøº

B) Analytics-grade demand (operational / transparency) ‚Üí Insights datasets

If you want fast, clean demand analytics (not settlement-run accounting), use Insights demand datasets like:
	‚Ä¢	ATL (B0610) Actual Total Load per Bidding Zone: GET /datasets/ATL  Ôøº
	‚Ä¢	Forecast equivalents (DATL, MATL, etc.) are also listed in the Insights dataset catalogue  Ôøº

Target table: insights_demand_bz_sp Note: This is not the same as CDCA settlement metered GSP take; it‚Äôs for operational/transparency analytics.

‚∏ª

One unified model: how it all fits together

If you want a single ‚Äúmaster‚Äù table per SP for modelling, create a wide fact table keyed by:
(settlement_date, settlement_period) plus dimensions:
	‚Ä¢	bm_unit_metered_mwh_sp from P114 c0421 (settlement metered)
	‚Ä¢	bm_unit_dispatch_sp from PN (operational intent)  Ôøº
	‚Ä¢	bm_unit_acceptances_sp from acceptances (operational actions)  Ôøº
	‚Ä¢	gsp_group_take_sp from P114 c0291/c0301 (settlement regional)
	‚Ä¢	insights_demand_bz_sp from ATL (analytics)  Ôøº
	‚Ä¢	ref_bm_units from /reference/bmunits/all  Ôøº

‚∏ª

Next: I‚Äôll make this ‚Äúimplementable‚Äù with your Portal key + your date range

If you paste (with your key redacted):
	‚Ä¢	an example p114/list?...&filter=c0421 response (just filenames), and
	‚Ä¢	one sample downloaded c0421 file header (first ~20 lines, redact anything sensitive)

‚Ä¶I‚Äôll give you:
	‚Ä¢	exact parsing rules (delimiter, column mapping)
	‚Ä¢	a normaliser that outputs clean CSV per table (BMU/SP, GSP/SP)
	‚Ä¢	and a recommended ingestion schedule (daily vs settlement-run cadence)

(And if you prefer, I can format the above schemas as BigQuery DDL or a Google Sheets tab layout.) This page is simply Elexon‚Äôs permission model for your Portal account. It controls which datasets, tools, and areas you are allowed to see or automate. Nothing here is wrong or unusual ‚Äî and it actually explains why your P114 access works the way it does.

Below is a plain-English explanation, focused on what matters for you.

‚∏ª

The key takeaway (important)

You are not a BSC Party, but you are licensed for P114.

That is exactly what this line means:

Non BSC Party P114 Access ‚Äî Granted

This is why:
	‚Ä¢	You can list and download P114 files
	‚Ä¢	You do not see participant-only operational or settlement systems
	‚Ä¢	Your P114 data may look ‚Äúhistoric‚Äù or ‚Äúsettlement-run based‚Äù

This is expected and correct.

‚∏ª

What each relevant permission actually means

‚úÖ Registered User ‚Äî Granted

Baseline access.
	‚Ä¢	Lets you log in
	‚Ä¢	View public/standard Portal content
	‚Ä¢	Use scripted downloads for permitted files

‚∏ª

‚úÖ BMRA Data Archive ‚Äî Granted

You can access:
	‚Ä¢	Historic BMRA report files
	‚Ä¢	Legacy balancing mechanism archives

This is file-based historic data, not live operations.

‚∏ª

‚úÖ Non BSC Party P114 Access ‚Äî Granted  ‚Üê this is the big one

This is the licence-based permission that allows non-participants (consultants, analysts, aggregators, etc.) to access P114 settlement files.

What it gives you:
	‚Ä¢	p114/list
	‚Ä¢	p114/download
	‚Ä¢	Optional FTP (if enabled)

What it does not give you:
	‚Ä¢	Direct access to settlement systems
	‚Ä¢	Participant-only operational feeds
	‚Ä¢	CVA/SVA internal processing views

So:

You have legitimate, licensed P114 access, but only in the form Elexon offers to non-BSC parties.

‚∏ª

‚ùå CVA Operational Data ‚Äî Not granted

This would allow:
	‚Ä¢	Viewing CVA system processing
	‚Ä¢	File loading status
	‚Ä¢	Sequence numbers
	‚Ä¢	Internal operational views

Only relevant if:
	‚Ä¢	You are a Supplier / Generator / Interconnector
	‚Ä¢	You operate CVA assets

Not needed for analytics.

‚∏ª

‚ùå Party Imbalance History ‚Äî Not granted

This would allow:
	‚Ä¢	Viewing imbalance positions for your own Participant IDs

You don‚Äôt have Participant IDs linked, so:
	‚Ä¢	This permission would be meaningless
	‚Ä¢	Elexon won‚Äôt grant it unless you become a BSC Party

‚∏ª

‚ùå Billing / Credit / Authorised Signatories

These are Participant-only:
	‚Ä¢	Supplier / Generator settlement
	‚Ä¢	Credit cover
	‚Ä¢	Billing & invoices

You should not have these.

‚∏ª

‚ùå AMSID Data (P375)

This is a separate licensed dataset (Advanced Metering System ID data).
Not part of P114.
Only needed for specific metering research.

‚∏ª

‚ùå Transmission Loss Tool / TLFA

These are CDCA/TLFA specialist tools.
Useful for:
	‚Ä¢	Loss calculations
	‚Ä¢	Transmission loss factors
But not required for:
	‚Ä¢	P114
	‚Ä¢	BM Unit analytics
	‚Ä¢	Insights/BMRS work

‚∏ª

Why you‚Äôre seeing ‚Äúold‚Äù data (reconfirmed)

Because:
	‚Ä¢	P114 = settlement reporting
	‚Ä¢	Settlement runs happen after the fact
	‚Ä¢	Some flows (especially CDCA) are re-issued months later

This has nothing to do with your permissions being wrong.

Your permissions are correct for:
	‚Ä¢	A non-BSC analytical user
	‚Ä¢	With licensed settlement data access

‚∏ª

What this page tells us about next steps

You do NOT need to request anything else for:
	‚Ä¢	P114 c0421 (BM Unit metered energy)
	‚Ä¢	P114 c0291 / c0301 (GSP/GSP group settlement demand)
	‚Ä¢	BMRA archive access

You already have what you need.

If you want live or near-real-time BM Unit activity

That is not a Portal permission problem.

That comes from:
	‚Ä¢	Insights / BMRS APIs (separate product, separate auth)
	‚Ä¢	Not from P114
	‚Ä¢	Not from Portal permissions

‚∏ª

One-sentence summary

This page confirms you are a licensed non-BSC P114 user with correct access; P114 looks ‚Äúold‚Äù because it is settlement data, not because anything is missing or misconfigured.

‚∏ª

If you want, next I can:
	‚Ä¢	Tell you exactly which permissions you should never request (to avoid delays)
	‚Ä¢	Show how to combine P114 settlement data with Insights operational data cleanly
	‚Ä¢	Help you draft a one-paragraph internal explanation of why P114 ‚â† live BM data

Just say which. You‚Äôre reading that page correctly: you don‚Äôt have ‚Äúeverything granted‚Äù. You have:
	‚Ä¢	Granted = you can use it now
	‚Ä¢	Pending = you‚Äôve requested it, but you cannot use it yet
	‚Ä¢	Requestable = you could request it (but haven‚Äôt / or it‚Äôs not in review)

Right now, the only things you definitely have for data work are:

‚úÖ Registered User
‚úÖ BMRA Data Archive
‚úÖ Non-BSC Party P114 Access

Everything else you listed as Pending is not available until approved.

‚∏ª

What you can do right now (immediately useful)

1) Use P114 properly (you already have it)

You can already use:
	‚Ä¢	p114/list and p114/download via HTTPS scripting key
	‚Ä¢	and (if configured) FTP

So you can build:
	‚Ä¢	BM Unit settlement metered volumes (c0421)
	‚Ä¢	GSP / GSP-group settlement takes (c0291/c0301)

Action: start automating via the HTTPS list/download endpoints (most reliable) using your scripting key.

‚∏ª

2) Use BMRA Data Archive (historic)

You can automate BMRA archive pulls (filename-based). Useful for backfills / legacy reporting.

‚∏ª

What you cannot do yet (and why)

Anything tied to ‚Äúyour Participant IDs‚Äù

These permissions are designed for actual BSC Parties (Suppliers/Generators/Interconnectors):
	‚Ä¢	View Authorised Signatories (Pending)
	‚Ä¢	Billing (Requestable)
	‚Ä¢	Credit (Requestable)
	‚Ä¢	Party Imbalance History (Requestable)
	‚Ä¢	CVA Operational Data (Pending)

Even if Elexon granted them, they won‚Äôt show anything unless your portal account has Participant IDs linked (which you likely don‚Äôt have as a non-BSC party).

So don‚Äôt wait on these to solve your ‚ÄúI want BM Unit data‚Äù problem ‚Äî they aren‚Äôt the route.

‚∏ª

What you should do next (the smart path)

A) Decide what you‚Äôre trying to achieve

If your goal is settlement-grade data

You already have the key permission: Non-BSC Party P114 Access (Granted) ‚úÖ
So you can proceed now.

If your goal is operational BM Unit data (dispatch/acceptances/live-ish)

That is not controlled by these Portal permissions.
You need the Insights / BMRS API access (separate system), not ‚ÄúCVA Operational Data‚Äù.

‚∏ª

B) Get the right approvals (only the ones worth having)

1) Ask Elexon to approve the P114-adjacent things you genuinely need

Worth having if you‚Äôre building a full settlement analytics pipeline:
	‚Ä¢	‚úÖ Transmission Loss Tool (Pending) ‚Äî can help with CDCA loss-related analysis
	‚Ä¢	‚úÖ TLFA (Pending) ‚Äî shared area, sometimes useful depending on your work
	‚Ä¢	‚úÖ Non-BSC Party P315 Access (Pending) ‚Äî if you also want P315 files

Not essential for basic P114 volume extraction.

2) Don‚Äôt spend time chasing participant-only permissions unless you are a BSC Party

Billing/Credit/Authorised signatories/Party Imbalance History won‚Äôt help unless you have Participant IDs linked.

‚∏ª

C) Practical ‚ÄúWhat can I do now?‚Äù checklist

‚úÖ Do now
	1.	Build an automated downloader using:
	‚Ä¢	p114/list?date=YYYY-MM-DD&filter=c0421
	‚Ä¢	p114/download?...filename=...
	2.	Land to ‚Äúraw‚Äù storage (Drive / local / BigQuery staging)
	3.	Parse into tables:
	‚Ä¢	bm_unit_metered_mwh_sp (from c0421)
	‚Ä¢	gsp_group_take_sp (from c0291/c0301)
	4.	Join to BM Unit standing data using:
	‚Ä¢	Registered BM Units file (you already can download), and/or
	‚Ä¢	Insights reference BM Units endpoint (if you have developer access)

‚úÖ Do next
	5.	Get Insights API key from Elexon Developer portal (separate from Portal permissions)
	6.	Pull PN / acceptances / MEL etc. via APIs for ‚Äúactions/dispatch‚Äù

‚∏ª

If you want, I‚Äôll make this concrete

Tell me which environment you want to run in:
	‚Ä¢	Python on your Mac
	‚Ä¢	Google Apps Script
	‚Ä¢	BigQuery ingestion

‚Ä¶and I‚Äôll give you a working starter that:
	‚Ä¢	downloads yesterday + today P114 files
	‚Ä¢	parses c0421 into a clean CSV/table
	‚Ä¢	dedupes by filename + row keys
	‚Ä¢	writes to your target (Sheets/BigQuery)

Also: if you paste (redacting your key) the output of one p114/list call, I‚Äôll show you exactly what‚Äôs available for your account and how to pull it. Good question ‚Äî these two labels are very BSC-specific, and the Portal does a poor job of explaining them. Here‚Äôs the plain-English explanation, what they actually contain, and whether you should care.

‚∏ª

1Ô∏è‚É£ TLFA (Pending) ‚Äî Transmission Loss Factor Agent shared area

What TLFA actually is

TLFA = Transmission Loss Factor Agent.
It‚Äôs the BSC service responsible for calculating Transmission Loss Multipliers (TLMs) and related loss factors used in settlement.

The TLFA shared area is not an API and not a dataset in the modern sense. It‚Äôs a document and file-sharing area used by:
	‚Ä¢	Elexon
	‚Ä¢	The TLFA
	‚Ä¢	BSC Parties (mainly Suppliers / Generators)

What you get if granted

Typically:
	‚Ä¢	Methodology documents
	‚Ä¢	Occasionally CSV/XLS outputs related to loss calculations
	‚Ä¢	Change notices
	‚Ä¢	Technical papers explaining how TLMs are derived

What it is not
	‚Ä¢	‚ùå Not live operational data
	‚Ä¢	‚ùå Not a replacement for TLM.csv (you already have that)
	‚Ä¢	‚ùå Not required to calculate imbalance prices or BM Unit dispatch

When TLFA access is useful

‚úî If you are doing deep settlement reconciliation
‚úî If you want to audit or explain loss methodologies
‚úî If you are supporting disputes / formal settlement analysis

When it‚Äôs not useful

‚úñ If you just want:
	‚Ä¢	BM Unit data
	‚Ä¢	Demand / generation
	‚Ä¢	Prices
	‚Ä¢	Dispatch / acceptances

Recommendation for you

Optional.
Nice to have for completeness, not required for your current P114 + Insights work.

‚∏ª

2Ô∏è‚É£ Non-BSC Party P315 Access (Pending) ‚Äî Settlement run output files

What P315 is

P315 is another BSC settlement dataflow, like P114, but it serves a different purpose.

In simple terms:
	‚Ä¢	P114 ‚Üí ‚ÄúHere are the settlement results / aggregation reports‚Äù
	‚Ä¢	P315 ‚Üí ‚ÄúHere are settlement run outputs & notifications used during settlement processing‚Äù

P315 files are primarily:
	‚Ä¢	Settlement run notices
	‚Ä¢	Status / run confirmation outputs
	‚Ä¢	Supporting files used during settlement processing

Who normally uses P315
	‚Ä¢	BSC Parties
	‚Ä¢	Settlement teams
	‚Ä¢	Auditors
	‚Ä¢	People tracking which settlement run happened when

What data is inside P315

Usually:
	‚Ä¢	Settlement run identifiers
	‚Ä¢	Run dates
	‚Ä¢	Run types (SF, RF, DF)
	‚Ä¢	Status flags
	‚Ä¢	Sometimes cross-references to P114 outputs

What it is not
	‚Ä¢	‚ùå Not BM Unit operational data
	‚Ä¢	‚ùå Not demand/generation time series
	‚Ä¢	‚ùå Not prices or dispatch

When P315 is useful

‚úî If you want to:
	‚Ä¢	Track settlement lifecycle timing
	‚Ä¢	Align P114 files to specific settlement runs
	‚Ä¢	Build an audit trail of settlement processing

When it‚Äôs not useful

‚úñ If your goal is:
	‚Ä¢	‚ÄúAll BM Unit data‚Äù
	‚Ä¢	Market analysis
	‚Ä¢	Dashboards
	‚Ä¢	Forecasting

Recommendation for you

Useful only if you are building a settlement-run-aware analytics pipeline
Otherwise: low priority

‚∏ª

üîé How these fit with what you already have

Item	What it adds	Do you need it now?
P114 (Granted)	Settlement metered volumes, aggregation reports	‚úÖ Yes
P315 (Pending)	Settlement run metadata & lifecycle	‚ö†Ô∏è Optional
TLFA (Pending)	Loss methodology docs & supporting files	‚ö†Ô∏è Optional
Insights / BMRS API	Operational & transparency data	‚úÖ Yes (separate system)


‚∏ª

üß† One-sentence summary

TLFA is about how losses are calculated; P315 is about when settlement runs happen ‚Äî neither gives you new BM Unit time-series data beyond what P114 + Insights already provide.

‚∏ª

What I‚Äôd do in your position
	1.	Proceed now with P114 c0421 / c0291 / c0301
	2.	Use Insights APIs for dispatch & acceptances
	3.	Let TLFA + P315 approvals tick along in the background
	4.	Only integrate them if you later need audit-grade settlement lineage

If you want, next I can:
	‚Ä¢	Draw a one-page diagram of how P114, P315, TLFA, Insights fit together
	‚Ä¢	Or help you decide which pending permissions are safe to withdraw to keep the Portal clean
‚∏ª

üîé How these fit with what you already have
What it adds
Do you need it now?
P114 (Granted)
Settlement metered volumes, aggregation reports
‚úÖ Yes
P315 (Pending)
Settlement run metadata & lifecycle
‚ö†Ô∏è Optional
TLFA (Pending)
Loss methodology docs & supporting files
‚ö†Ô∏è Optional
Insights / BMRS API
Operational & transparency data
‚úÖ Yes (separate system)
One-sentence summary

TLFA is about how losses are calculated; P315 is about when settlement runs happen ‚Äî neither gives you new BM Unit time-series data beyond what P114 + Insights already provide.

‚∏ª

What I‚Äôd do in your position
	1.	Proceed now with P114 c0421 / c0291 / c0301
	2.	Use Insights APIs for dispatch & acceptances
	3.	Let TLFA + P315 approvals tick along in the background
	4.	Only integrate them if you later need audit-grade settlement lineage

If you want, next I can:
	‚Ä¢	Draw a one-page diagram of how P114, P315, TLFA, Insights fit together
	‚Ä¢	Or help you decide which pending permissions are safe to withdraw to keep the Portal clean You‚Äôve got three moving parts that sound like different products, but they‚Äôre really just different delivery methods for overlapping datasets:
	‚Ä¢	Insights = the overall modern data service (APIs + documentation + datasets)
	‚Ä¢	IRIS = the push/streaming delivery of the same datasets
	‚Ä¢	Portal (P114 etc.) = separate BSC settlement file flows (not the same as Insights/IRIS)

Elexon explicitly says IRIS and the Insights Solution APIs provide the same data in the same output format, so you can mix-and-match.  Ôøº

Here‚Äôs the diagram. flowchart TB
  subgraph A["BSC settlement systems (file flows)"]
    SAA["SAA/CDCA/SVAA etc.\nSettlement processes"]
    P114["P114 files (S0142, C0291, C0301, C0421)\nSettlement-run outputs"]
    PortalDL["Elexon Portal scripted downloads\n(HTTPS list/download, FTP)"]
    SAA --> P114 --> PortalDL
  end

  subgraph B["Elexon Insights Solution (modern market & operational data)"]
    Datasets["Datasets (raw feeds)\n(e.g., PN, BOALF, BOD, MELS, ATL‚Ä¶)"]
    Opinionated["Opinionated APIs\n(curated/derived views)"]
    Reference["Reference data\n(standing data like BM Units)"]
    APIDocs["API documentation + OpenAPI schemas\n(JSON/YAML/WADL)"]
    APIGW["Insights REST APIs\n(bmrs.elexon.co.uk)"]
    APIDocs --> APIGW
    Datasets --> APIGW
    Opinionated --> APIGW
    Reference --> APIGW
  end

  subgraph C["IRIS (push/near real-time delivery)"]
    IRIS["IRIS queues / messages\nDataset name as subject\nSame output format as APIs"]
  end

  NGESO["Data providers (e.g., NGESO)\n& other sources feeding datasets"]
  NGESO --> Datasets

  APIGW <--> IRIS

  User["You / your data platform\n(BigQuery/Sheets/Python)"]
  PortalDL --> User
  APIGW --> User
  IRIS --> User  When you say ‚ÄúRevenue for all virtual lead parties and others‚Äù, there are two completely different things people mean in the GB market. Your sentence is too short to know which one you want, so I‚Äôll give you both, and you can pick the one you mean.

‚∏ª

1) If you mean ‚Äúrevenue earned by Virtual Lead Parties from the Balancing Mechanism / ESO‚Äù

A Virtual Lead Party (VLP) is an independent aggregator that participates in the Balancing Mechanism (BM).  Ôøº

Can you get ‚ÄúVLP revenue‚Äù directly from Elexon?

Not as a single public ‚Äúrevenue by VLP‚Äù dataset. What you can do is calculate it by combining:
	‚Ä¢	Acceptances / dispatch actions by BM Unit (who was accepted, volume, price)
	‚Ä¢	Mapping BM Units to Lead Party (the VLP)

Elexon‚Äôs Insights Solution even provides a ‚ÄúBalancing Mechanism data by BMU‚Äù view where you can search by ‚ÄúBMU ID, lead party or NGC BMU name‚Äù, which confirms the Lead Party concept exists in their dataset/view layer.  Ôøº

What you would calculate

You‚Äôd compute (roughly):
	‚Ä¢	Accepted volume (MWh) √ó Acceptance price (¬£/MWh)
split by:
	‚Ä¢	Lead Party (VLP / generator / supplier)
	‚Ä¢	BMU
	‚Ä¢	date / SP

Caveat: ‚ÄúRevenue‚Äù can mean:
	‚Ä¢	gross acceptance value (what the action is worth)
	‚Ä¢	net of charges (BSUoS, TLM, etc.)
	‚Ä¢	contract revenue (which is outside Elexon/ESO public data)

Elexon/Insights data supports the gross BM acceptance value style calculation; true ‚Äúrevenue‚Äù depends on commercial arrangements.

‚∏ª

2) If you mean ‚Äúwhat does a VLP pay / get charged‚Äù under BSC

That‚Äôs different: it‚Äôs about BSC charges and settlement liabilities, not BM acceptance value.

VLPs are a distinct BSC Party type under the BSC.  Ôøº
But ‚Äúrevenue‚Äù in this framing would come from settlement positions, which are generally party-private unless you‚Äôre that party (or licensed).

‚∏ª

What I need from you to do this properly (pick one line)

A) Do you want BM revenue from acceptances (gross, calculated)?

If yes, tell me:
	1.	Time window (e.g., 2024‚Äì2025, last month, all time)
	2.	Output level: by Lead Party only, or Lead Party + BMU
	3.	Include only VLPs, or VLPs + all other Lead Parties

B) Or do you want a list of all VLP organisations (not revenue)?

I can pull the list of VLPs/AMVLPs from Elexon market-entry/role pages, but that still won‚Äôt give ‚Äúrevenue‚Äù without the calculation route.

‚∏ª

If you choose A (most likely), here‚Äôs the exact approach I‚Äôll deliver

I will produce:
	‚Ä¢	a schema for a table like bm_acceptance_value_by_lead_party_day
	‚Ä¢	the exact Insights endpoints to extract:
	‚Ä¢	acceptances (volume/price)
	‚Ä¢	BMU ‚Üí lead party mapping
	‚Ä¢	and the formula to compute gross value and aggregate it.

The reason this is feasible is that Elexon‚Äôs Insights BMU view supports searching ‚Äúby lead party,‚Äù implying lead-party mapping is available in the data layer.  Ôøº

‚∏ª

Which meaning did you intend ‚Äî A (gross BM acceptance value by VLP/lead party) or B (BSC settlement/charges)? Perfect ‚Äî Option A it is.

Below is a clean, end-to-end, implementation-ready explanation of how to calculate gross Balancing Mechanism (BM) revenue for all Virtual Lead Parties (VLPs) and all other Lead Parties, using Elexon Insights / BMRS data.

This is not settlement revenue and not private contract revenue ‚Äî this is the gross value of BM acceptances, which is the only thing that is publicly reconstructable.

‚∏ª

What ‚ÄúBM revenue by Virtual Lead Party‚Äù actually means (precisely)

Gross BM Acceptance Value (¬£)
= accepted volume (MWh) √ó acceptance price (¬£/MWh)

Calculated per:
	‚Ä¢	Settlement Period
	‚Ä¢	BM Unit
	‚Ä¢	Lead Party (VLP, generator, supplier, etc.)

Then aggregated.

‚ö†Ô∏è This is:
	‚Ä¢	‚úÖ public / reconstructable
	‚Ä¢	‚ùå not net of BSUoS, TLM, credit, or bilateral contracts
	‚Ä¢	‚ùå not the party‚Äôs actual profit

‚∏ª

The authoritative data sources (Insights / BMRS)

You need three data inputs, all from Insights (IRIS can be substituted later).

‚∏ª

1Ô∏è‚É£ Bid-Offer Acceptances (BOAs) ‚Äî the money

Dataset

Balancing Mechanism Acceptances
(BOA / BOALF family in BMRS terminology)

What it gives you (key fields)
Meaning
acceptanceNumber
Unique action ID
bmUnitId
BM Unit
settlementDate
Date
settlementPeriod
SP
acceptedVolume
MW
price
¬£/MWh
bidOfferIndicator
Bid or Offer
onversion to energy

BM volumes are in MW over a half-hour: accepted_MWh = accepted_MW √ó 0.5 Gross value per acceptance gross_value_¬£ = accepted_MWh √ó price
‚∏ª

2Ô∏è‚É£ BM Unit ‚Üí Lead Party mapping ‚Äî who earns it

Dataset

Reference BM Units
(Insights reference data)

What it gives you
Meaning
bmUnitId
BM Unit
leadPartyName
Commercial counterparty
partyType
VLP / Generator / Supplier
fuelType
Optional
effectiveFrom / To
Validity
This is where Virtual Lead Parties are identified.

üîë There is no separate ‚ÄúVLP dataset‚Äù
VLPs are identified because:
	‚Ä¢	they appear as leadPartyName
	‚Ä¢	their partyType = VLP (or equivalent classification)

‚∏ª

3Ô∏è‚É£ Optional: Physical Notifications (PN/FPN) ‚Äî context only

Not required for revenue, but useful for:
	‚Ä¢	validation
	‚Ä¢	dispatch context
	‚Ä¢	explaining why acceptances occurred

‚∏ª

Target tables you should build (recommended)

A) bm_acceptances_raw

(1 row = 1 acceptance) olumn
type
acceptance_id
INT
settlement_date
DATE
settlement_period
INT
bm_unit_id
STRING
accepted_mw
NUMERIC
accepted_mwh
NUMERIC
price_gbp_per_mwh
NUMERIC
gross_value_gbp
NUMERIC
bid_offer
STRING
source
STRING = INSIGHTS
loaded_at
TIMESTAMP
B) ref_bm_units

(slow-changing dimension)
type
bm_unit_id
STRING
lead_party
STRING
party_type
STRING (VLP / Generator / Supplier)
fuel_type
STRING
effective_from
DATE
effective_to
DATE

‚∏ª

C) bm_acceptance_value_by_lead_party_sp

(analytics-ready)

Primary key (settlement_date, settlement_period, lead_party) column
type
settlement_date
DATE
settlement_period
INT
lead_party
STRING
party_type
STRING
total_accepted_mwh
NUMERIC
gross_value_gbp
NUMERIC

‚∏ª

How to calculate it (logic)

Step 1 ‚Äî Pull acceptances

From Insights:
	‚Ä¢	loop by date (or SP range)
	‚Ä¢	land all acceptances

Step 2 ‚Äî Convert MW ‚Üí MWh accepted_mwh = accepted_mw √ó 0.5 Step 3 ‚Äî Calculate value bm_acceptances_raw
JOIN ref_bm_units
ON bm_unit_id
AND settlement_date BETWEEN effective_from AND effective_to Step 5 ‚Äî Aggregate

Group by:
	‚Ä¢	settlement_date
	‚Ä¢	settlement_period
	‚Ä¢	lead_party
	‚Ä¢	party_type
Final outputs you can produce

1Ô∏è‚É£ Revenue by Virtual Lead Party (headline table) ead Party
Total BM Value (¬£)
VLP A
¬£xx,xxx,xxx
VLP B
¬£yy,yyy,yyy
2Ô∏è‚É£ Revenue comparison: VLPs vs others rty Type
Gross BM Value (¬£)
VLP
¬£Xbn
Generator
¬£Ybn
Supplier
¬£Zbn

‚∏ª

3Ô∏è‚É£ Time series
	‚Ä¢	¬£/day
	‚Ä¢	¬£/month
	‚Ä¢	¬£/SP

‚∏ª

What this does not include (important)

This excludes:
	‚Ä¢	BSUoS charges
	‚Ä¢	TLM adjustments
	‚Ä¢	Credit cover
	‚Ä¢	Bilateral contracts
	‚Ä¢	Capacity Market payments
	‚Ä¢	Ancillary services outside BM

So do not label this ‚Äúprofit‚Äù or ‚Äúnet revenue‚Äù.

Correct labels:
	‚Ä¢	‚ÄúGross BM Acceptance Value‚Äù
	‚Ä¢	‚ÄúBM dispatch value‚Äù

‚∏ª

What I can do next (pick one)

1Ô∏è‚É£ Produce BigQuery SQL that does all of the above
2Ô∏è‚É£ Produce Python code that pulls Insights data + builds tables
3Ô∏è‚É£ Produce a CSV output schema ready for publication
4Ô∏è‚É£ Add charts: VLP share of BM value over time

Just tell me which. Got it ‚Äî here are all 4 deliverables (SQL, Python, CSV schema, graph ideas) for gross BM acceptance value (‚Äúrevenue‚Äù) by Lead Party, with a clean join to BM Units reference data.

I‚Äôm basing this on the public Elexon Insights Solution endpoints:
	‚Ä¢	BM Units reference list: GET /reference/bmunits/all  Ôøº
	‚Ä¢	Acceptances bulk feed: BOALF dataset (/datasets/BOALF) which ‚Äúprovides bid offer acceptance data‚Äù with time filtering  Ôøº
	‚Ä¢	Base API is available via data.elexon.co.uk swagger and documented as public on the developer portal  Ôøº

‚∏ª

1) BigQuery SQL (transforms + revenue rollups)

Assumption: you land raw JSON rows into:
	‚Ä¢	raw.boalf_json with a JSON column payload (one acceptance record per row)
	‚Ä¢	ref.bmunits_json with JSON column payload (one BM unit per row)

A) Normalise reference BM Units CREATE OR REPLACE TABLE mart.ref_bmunits AS
SELECT
  JSON_VALUE(payload, '$.elexonBmUnit')            AS bm_unit_id,
  JSON_VALUE(payload, '$.bmUnitName')             AS bm_unit_name,
  JSON_VALUE(payload, '$.leadPartyId')            AS lead_party_id,
  JSON_VALUE(payload, '$.leadPartyName')          AS lead_party_name,
  JSON_VALUE(payload, '$.fuelType')               AS fuel_type,
  JSON_VALUE(payload, '$.bmUnitType')             AS bm_unit_type,
  CAST(JSON_VALUE(payload, '$.fpnFlag') AS BOOL)  AS fpn_flag,
  SAFE_CAST(JSON_VALUE(payload, '$.generationCapacity') AS FLOAT64) AS generation_capacity_mw,
  SAFE_CAST(JSON_VALUE(payload, '$.demandCapacity')     AS FLOAT64) AS demand_capacity_mw,
  CURRENT_TIMESTAMP() AS loaded_at
FROM ref.bmunits_json; Note: field names above match the commonly-published structure of BM unit reference outputs (leadPartyName/Id, elexonBmUnit, fuelType etc.).  Ôøº

B) Normalise BOALF acceptances (gross acceptance value)

BOALF is a bulk acceptances dataset with time filtering.  ÔøºCREATE OR REPLACE TABLE mart.boalf_acceptances AS
SELECT
  SAFE_CAST(JSON_VALUE(payload, '$.acceptanceNumber') AS INT64) AS acceptance_id,
  -- BOALF often has timestamps; keep both time + derived settlement keys if present
  TIMESTAMP(JSON_VALUE(payload, '$.timeFrom')) AS time_from_utc,
  TIMESTAMP(JSON_VALUE(payload, '$.timeTo'))   AS time_to_utc,

  -- If BOALF includes settlement keys use them; otherwise compute later from time_from_utc
  DATE(JSON_VALUE(payload, '$.settlementDate')) AS settlement_date,
  SAFE_CAST(JSON_VALUE(payload, '$.settlementPeriod') AS INT64) AS settlement_period,

  JSON_VALUE(payload, '$.bmUnitId') AS bm_unit_id,

  -- Volumes/prices naming can vary; these cover common variants
  SAFE_CAST(COALESCE(
    JSON_VALUE(payload, '$.acceptedVolume'),
    JSON_VALUE(payload, '$.volume')
  ) AS FLOAT64) AS accepted_mw,

  SAFE_CAST(COALESCE(
    JSON_VALUE(payload, '$.price'),
    JSON_VALUE(payload, '$.offerPrice'),
    JSON_VALUE(payload, '$.bidPrice')
  ) AS FLOAT64) AS price_gbp_per_mwh,

  JSON_VALUE(payload, '$.bidOfferIndicator') AS bid_offer,

  -- Energy and value
  SAFE_MULTIPLY(
    SAFE_CAST(COALESCE(JSON_VALUE(payload, '$.acceptedVolume'), JSON_VALUE(payload, '$.volume')) AS FLOAT64),
    0.5
  ) AS accepted_mwh,

  SAFE_MULTIPLY(
    SAFE_MULTIPLY(
      SAFE_CAST(COALESCE(JSON_VALUE(payload, '$.acceptedVolume'), JSON_VALUE(payload, '$.volume')) AS FLOAT64),
      0.5
    ),
    SAFE_CAST(COALESCE(JSON_VALUE(payload, '$.price'), JSON_VALUE(payload, '$.offerPrice'), JSON_VALUE(payload, '$.bidPrice')) AS FLOAT64)
  ) AS gross_value_gbp,

  payload AS raw_payload,
  CURRENT_TIMESTAMP() AS loaded_at
FROM raw.boalf_json; CREATE OR REPLACE TABLE mart.bm_value_by_lead_party_sp AS
SELECT
  a.settlement_date,
  a.settlement_period,
  r.lead_party_id,
  r.lead_party_name,
  SUM(a.accepted_mwh)      AS total_accepted_mwh,
  SUM(a.gross_value_gbp)   AS gross_value_gbp,
  COUNT(DISTINCT a.acceptance_id) AS acceptance_count
FROM mart.boalf_acceptances a
LEFT JOIN mart.ref_bmunits r
  USING (bm_unit_id)
GROUP BY 1,2,3,4; ‚ÄúVLP vs others‚Äù classification

Important: the BM Units reference feed doesn‚Äôt reliably label ‚ÄúVLP‚Äù as a field. In practice you maintain a small dimension table of VLP Lead Party IDs/names (from BSC Party role lists), then join. -- You maintain this table manually/periodically:
-- mart.dim_vlp(lead_party_id STRING, lead_party_name STRING, is_vlp BOOL)

CREATE OR REPLACE TABLE mart.bm_value_by_party_type_day AS
SELECT
  settlement_date,
  IF(v.is_vlp, 'VLP', 'Other') AS party_type,
  SUM(gross_value_gbp) AS gross_value_gbp,
  SUM(total_accepted_mwh) AS total_accepted_mwh
FROM mart.bm_value_by_lead_party_sp s
LEFT JOIN mart.dim_vlp v
  ON s.lead_party_id = v.lead_party_id
GROUP BY 1,2; ) Python code (pull Insights APIs + load to BigQuery + build tables)

Key facts:
	‚Ä¢	Elexon publishes public APIs and interactive docs via Insights Solution; the Swagger is on data.elexon.co.uk  Ôøº
	‚Ä¢	Bulk acceptances via BOALF dataset endpoint  Ôøº
	‚Ä¢	BM Units reference endpoint exists as GET /reference/bmunits/all  Ôøº
	‚Ä¢	Common base used in the ecosystem: https://data.elexon.co.uk/bmrs/api/v1  Ôøº

Here‚Äôs a runnable starter (Mac-friendly) that:
	‚Ä¢	downloads BM units reference
	‚Ä¢	downloads BOALF in day chunks
	‚Ä¢	loads JSON rows into BigQuery tables import os
import time
import json
import datetime as dt
from typing import Dict, Any, Iterable, List, Optional

import requests
from google.cloud import bigquery

BASE = "https://data.elexon.co.uk/bmrs/api/v1"  # commonly used base for the new API
SESSION = requests.Session()
SESSION.headers.update({"accept": "application/json"})

BQ_PROJECT = os.environ["BQ_PROJECT"]
BQ_DATASET_RAW = os.environ.get("BQ_DATASET_RAW", "raw")
BQ_DATASET_REF = os.environ.get("BQ_DATASET_REF", "ref")

client = bigquery.Client(project=BQ_PROJECT)

def get_json(url: str, params: Optional[dict] = None, retries: int = 5) -> Any:
    backoff = 1.0
    for i in range(retries):
        r = SESSION.get(url, params=params, timeout=60)
        if r.status_code == 200:
            return r.json()
        if r.status_code in (429, 500, 502, 503, 504):
            time.sleep(backoff)
            backoff *= 2
            continue
        raise RuntimeError(f"GET {r.url} failed: {r.status_code} {r.text[:500]}")
    raise RuntimeError(f"GET {url} failed after retries")

def iter_records(payload: Any) -> Iterable[Dict[str, Any]]:
    """
    Elexon endpoints often return either:
    - a list of records, or
    - an object containing a list under 'data'/'results'
    This makes it resilient.
    """
    if isinstance(payload, list):
        for row in payload:
            if isinstance(row, dict):
                yield row
        return
    if isinstance(payload, dict):
        for key in ("data", "results", "items"):
            v = payload.get(key)
            if isinstance(v, list):
                for row in v:
                    if isinstance(row, dict):
                        yield row
                return
        # fallback: single record dict
        yield payload

def load_json_rows(table_fq: str, rows: List[Dict[str, Any]]):
    if not rows:
        return
    # We store each row as a JSON string for maximum compatibility
    bq_rows = [{"payload": json.dumps(r)} for r in rows]

    job_config = bigquery.LoadJobConfig(
        schema=[bigquery.SchemaField("payload", "JSON")],
        write_disposition=bigquery.WriteDisposition.WRITE_APPEND,
    )

    load_job = client.load_table_from_json(bq_rows, table_fq, job_config=job_config)
    load_job.result()

def fq(dataset: str, table: str) -> str:
    return f"{BQ_PROJECT}.{dataset}.{table}"

def fetch_and_store_bmunits():
    url = f"{BASE}/reference/bmunits/all"
    payload = get_json(url)
    rows = list(iter_records(payload))
    load_json_rows(fq(BQ_DATASET_REF, "bmunits_json"), rows)
    print(f"Loaded {len(rows)} BM units into {fq(BQ_DATASET_REF,'bmunits_json')}")

def fetch_and_store_boalf(date_from: dt.date, date_to: dt.date):
    """
    Pull BOALF in daily chunks. BOALF supports from/to filtering on TimeFrom.  [oai_citation:9‚Ä°BMRS](https://bmrs.elexon.co.uk/api-documentation/endpoint/datasets/BOALF?utm_source=chatgpt.com)
    """
    url = f"{BASE}/datasets/BOALF"
    d = date_from
    total = 0
    while d <= date_to:
        # Use ISO date-times (UTC). If you prefer SP windows, use settlementPeriodFrom/To when available.
        start = dt.datetime.combine(d, dt.time(0, 0)).isoformat() + "Z"
        end = dt.datetime.combine(d, dt.time(23, 59, 59)).isoformat() + "Z"
        payload = get_json(url, params={"from": start, "to": end, "format": "json"})
        rows = list(iter_records(payload))
        load_json_rows(fq(BQ_DATASET_RAW, "boalf_json"), rows)
        total += len(rows)
        print(f"{d}: {len(rows)} rows")
        d += dt.timedelta(days=1)
    print(f"Loaded {total} BOALF rows into {fq(BQ_DATASET_RAW,'boalf_json')}")

if __name__ == "__main__":
    # Example: last 7 days
    today = dt.date.today()
    start = today - dt.timedelta(days=7)

    fetch_and_store_bmunits()
    fetch_and_store_boalf(start, today) After loading, run the BigQuery SQL in section (1) to build the marts.

‚∏ª

3) CSV output schema (publication-ready)

A) Core published table: bm_acceptance_value_by_lead_party_sp.csv

One row per Lead Party √ó Settlement Period.

Columns:
	‚Ä¢	settlement_date (YYYY-MM-DD)
	‚Ä¢	settlement_period (1‚Äì48)
	‚Ä¢	lead_party_id
	‚Ä¢	lead_party_name
	‚Ä¢	total_accepted_mwh
	‚Ä¢	gross_value_gbp
	‚Ä¢	acceptance_count
	‚Ä¢	is_vlp (true/false; from your maintained VLP dimension)
	‚Ä¢	source (e.g., Elexon Insights BOALF)
	‚Ä¢	loaded_at_utc (timestamp)

B) Daily rollup: bm_acceptance_value_by_lead_party_day.csv
	‚Ä¢	date
	‚Ä¢	lead_party_id
	‚Ä¢	lead_party_name
	‚Ä¢	is_vlp
	‚Ä¢	total_accepted_mwh
	‚Ä¢	gross_value_gbp
	‚Ä¢	acceptance_count

C) Party-type summary: bm_acceptance_value_by_party_type_day.csv
	‚Ä¢	date
	‚Ä¢	party_type (VLP/Other)
	‚Ä¢	total_accepted_mwh
	‚Ä¢	gross_value_gbp

Publication notes (so nobody misuses it):
	‚Ä¢	Definition: gross acceptance value = accepted MW √ó 0.5 √ó ¬£/MWh
	‚Ä¢	Not net of charges, not profit.

‚∏ª

4) Graph suggestions (what will look good and tell the story)
	1.	Stacked area: daily gross value (¬£) by party type (VLP vs Other)
	‚Ä¢	Shows VLP share over time.
	2.	Top-10 VLP bar chart: gross value (¬£) over selected period
	‚Ä¢	Use a date range selector (7d, 30d, 6m, 1y).
	3.	Heatmap: settlement period (1‚Äì48) vs day-of-week for VLP gross value (¬£)
	‚Ä¢	Reveals whether VLP activity clusters in peak periods.
	4.	Scatter: accepted MWh vs price (¬£/MWh), coloured by bid/offer
	‚Ä¢	Shows whether value is dominated by volume or price spikes.
	5.	Lorenz / concentration curve: cumulative share of value by lead party
	‚Ä¢	Quantifies market concentration.

‚∏ª

Quick question (no need to answer now if you want me to assume)

Do you want the BigQuery marts to be built in your existing dataset (e.g., uk_energy_insights), or should I assume raw/ref/mart datasets as above and you‚Äôll adapt names? reat ‚Äî I‚Äôll assume you want the same raw/ref/mart pattern, but located inside your existing BigQuery project/dataset structure.

Here‚Äôs a concrete setup you can apply immediately:

BigQuery layout (recommended)

Datasets
	‚Ä¢	raw ‚Äî landing JSON (append-only)
	‚Ä¢	ref ‚Äî reference snapshots (overwrite or slowly changing)
	‚Ä¢	mart ‚Äî analytics tables (overwrite / partitioned)

If you prefer everything under one dataset (e.g., uk_energy_insights), just prefix table names, but the logic stays the same.

‚∏ª

BigQuery SQL (production-friendly version)

1) Reference BM Units table CREATE SCHEMA IF NOT EXISTS `ref`;
CREATE OR REPLACE TABLE `ref.bmunits` AS
SELECT
  JSON_VALUE(payload, '$.elexonBmUnit') AS bm_unit_id,
  JSON_VALUE(payload, '$.bmUnitName') AS bm_unit_name,
  JSON_VALUE(payload, '$.leadPartyId') AS lead_party_id,
  JSON_VALUE(payload, '$.leadPartyName') AS lead_party_name,
  JSON_VALUE(payload, '$.fuelType') AS fuel_type,
  JSON_VALUE(payload, '$.bmUnitType') AS bm_unit_type,
  CURRENT_TIMESTAMP() AS loaded_at
FROM `ref.bmunits_json`; CREATE SCHEMA IF NOT EXISTS `mart`;
CREATE OR REPLACE TABLE `mart.boalf_acceptances` AS
SELECT
  SAFE_CAST(JSON_VALUE(payload, '$.acceptanceNumber') AS INT64) AS acceptance_id,
  JSON_VALUE(payload, '$.bmUnitId') AS bm_unit_id,

  -- Prefer settlement keys if present
  DATE(JSON_VALUE(payload, '$.settlementDate')) AS settlement_date,
  SAFE_CAST(JSON_VALUE(payload, '$.settlementPeriod') AS INT64) AS settlement_period,

  -- Volumes/prices
  SAFE_CAST(COALESCE(JSON_VALUE(payload, '$.acceptedVolume'), JSON_VALUE(payload, '$.volume')) AS FLOAT64) AS accepted_mw,
  SAFE_CAST(COALESCE(JSON_VALUE(payload, '$.price'), JSON_VALUE(payload, '$.offerPrice'), JSON_VALUE(payload, '$.bidPrice')) AS FLOAT64) AS price_gbp_per_mwh,
  JSON_VALUE(payload, '$.bidOfferIndicator') AS bid_offer,

  -- Derived metrics
  SAFE_MULTIPLY(
    SAFE_CAST(COALESCE(JSON_VALUE(payload, '$.acceptedVolume'), JSON_VALUE(payload, '$.volume')) AS FLOAT64),
    0.5
  ) AS accepted_mwh,

  SAFE_MULTIPLY(
    SAFE_MULTIPLY(
      SAFE_CAST(COALESCE(JSON_VALUE(payload, '$.acceptedVolume'), JSON_VALUE(payload, '$.volume')) AS FLOAT64),
      0.5
    ),
    SAFE_CAST(COALESCE(JSON_VALUE(payload, '$.price'), JSON_VALUE(payload, '$.offerPrice'), JSON_VALUE(payload, '$.bidPrice')) AS FLOAT64)
  ) AS gross_value_gbp,

  payload AS raw_payload,
  CURRENT_TIMESTAMP() AS loaded_at
FROM `raw.boalf_json`; CREATE OR REPLACE TABLE `mart.bm_value_by_lead_party_sp` AS
SELECT
  a.settlement_date,
  a.settlement_period,
  r.lead_party_id,
  r.lead_party_name,
  SUM(a.accepted_mwh) AS total_accepted_mwh,
  SUM(a.gross_value_gbp) AS gross_value_gbp,
  COUNT(DISTINCT a.acceptance_id) AS acceptance_count
FROM `mart.boalf_acceptances` a
LEFT JOIN `ref.bmunits` r
  USING (bm_unit_id)
GROUP BY 1,2,3,4; CREATE OR REPLACE TABLE `mart.dim_vlp` (
  lead_party_id STRING,
  lead_party_name STRING,
  is_vlp BOOL
); CREATE OR REPLACE TABLE `mart.bm_value_by_lead_party_sp_vlp` AS
SELECT
  s.*,
  IFNULL(v.is_vlp, FALSE) AS is_vlp
FROM `mart.bm_value_by_lead_party_sp` s
LEFT JOIN `mart.dim_vlp` v
  ON s.lead_party_id = v.lead_party_id; CREATE OR REPLACE TABLE `mart.bm_value_by_party_type_day` AS
SELECT
  settlement_date AS date,
  IF(is_vlp, 'VLP', 'Other') AS party_type,
  SUM(total_accepted_mwh) AS total_accepted_mwh,
  SUM(gross_value_gbp) AS gross_value_gbp,
  SUM(acceptance_count) AS acceptance_count
FROM `mart.bm_value_by_lead_party_sp_vlp`
GROUP BY 1,2; Graphs to build (from these marts)
	1.	Stacked area: date vs gross_value_gbp split by party_type
	2.	Bar: top 10 VLPs by gross_value_gbp over selected window
	3.	Heatmap: settlement_period √ó weekday for VLP gross value
	4.	Scatter: acceptance accepted_mwh vs price (if you keep raw acceptance-level table)

‚∏ª

Next thing you need (so this works end-to-end)

You need to populate mart.dim_vlp (lead parties that are VLPs).

If you want, I can generate a starter list by:
	‚Ä¢	pulling the public BSC role lists (web) and extracting VLP lead parties, then
	‚Ä¢	outputting it as a CSV to load into BigQuery.

Say ‚Äúgenerate VLP list‚Äù and I‚Äôll do that. You can generate a definitive Virtual Lead Party (VLP) list automatically from Elexon data ‚Äî and you already have the key ingredient: the Registered Participants file (or the public ‚ÄúBSC Signatories‚Äù table).

What ‚ÄúVLP‚Äù looks like in Elexon data

On Elexon‚Äôs ‚ÄúBSC Signatories and Qualified Persons‚Äù page, the Party Role key shows:
	‚Ä¢	VP = Virtual Lead Party
	‚Ä¢	AV = Asset Metering Virtual Lead Party (AMVLP)  Ôøº

So a ‚ÄúVLP list‚Äù is simply: all Parties whose roles include VP (and optionally AV).

‚∏ª

Option A (best): Use your Portal ‚ÄúRegistered Participants‚Äù file and filter VP/AV

Because you‚Äôre already able to scripted-download:
	‚Ä¢	REGISTERED_PARTICIPANTS_FILE (from your Elexon Portal scripted URLs list)

1) Download (curl)

Use the REGISTERED_PARTICIPANTS_FILE scripted URL from your profile page (the one that includes .../REGISTERED_PARTICIPANTS_FILE?key=...).

(Don‚Äôt paste your scripting key into chat logs or repos ‚Äî treat it like a password.)

2) Parse + produce a VLP CSV list (Python)

Below is a robust pattern (handles ‚Äúroles‚Äù being a comma-separated string like BP, TN, VP): import pandas as pd

# Point this at the file you downloaded from REGISTERED_PARTICIPANTS_FILE
df = pd.read_csv("REGISTERED_PARTICIPANTS_FILE.csv")

# Try to auto-detect likely column names (you may need to adjust once you see the header)
# Common patterns are like: PartyName, PartyId, PartyRoles (or similar)
cols = {c.lower(): c for c in df.columns}
party_name_col = cols.get("party name") or cols.get("partyname") or "Party Name"
party_id_col   = cols.get("party id")   or cols.get("partyid")   or "Party ID"
roles_col      = cols.get("party roles") or cols.get("partyroles") or "Party Roles"

# Normalise roles into a set per row
def has_role(role_str, role_code):
    if pd.isna(role_str):
        return False
    roles = [r.strip() for r in str(role_str).split(",")]
    return role_code in roles

vlp = df[df[roles_col].apply(lambda x: has_role(x, "VP") or has_role(x, "AV"))].copy()

# Optional: separate pure VLP vs AMVLP
vlp["is_vlp"] = vlp[roles_col].apply(lambda x: has_role(x, "VP"))
vlp["is_amvlp"] = vlp[roles_col].apply(lambda x: has_role(x, "AV"))

out = vlp[[party_id_col, party_name_col, roles_col, "is_vlp", "is_amvlp"]].drop_duplicates()
out = out.sort_values([ "is_vlp", "is_amvlp", party_id_col ], ascending=[False, False, True])

out.to_csv("vlp_list.csv", index=False)
print(f"Wrote {len(out)} rows to vlp_list.csv") Output schema (what you‚Äôll end up with)
	‚Ä¢	party_id
	‚Ä¢	party_name
	‚Ä¢	party_roles
	‚Ä¢	is_vlp (VP present)
	‚Ä¢	is_amvlp (AV present)

That‚Äôs the clean ‚ÄúVLP list‚Äù you can then load to BigQuery as a reference/dimension table.

‚∏ª

Option B: Use the public Elexon ‚ÄúBSC Signatories‚Äù table

This table is public and includes roles + a role-key confirming VP/AV meanings.  Ôøº
In a browser, the Export button usually downloads the table, then you filter VP/AV the same way.

(From my side, the export endpoint is returning a fetch error in this environment, but it works interactively on the site for many users.)

‚∏ª

Quick sanity check (examples)

From the public signatories table, you can already see examples like:
	‚Ä¢	Adela Energy Ltd (ADELA) includes VP  Ôøº
	‚Ä¢	Axle Energy (AXLEENER) includes VP and AV  Ôøº

‚∏ª

If you want, I can generate it exactly for you

If you paste just the header row + ~5 sample rows from your REGISTERED_PARTICIPANTS_FILE (no secrets), I‚Äôll:
	‚Ä¢	pin down the exact column names/types,
	‚Ä¢	give you the final production-ready Python and BigQuery load SQL,
	‚Ä¢	and define the BigQuery table schema (types, constraints, clustering). Here‚Äôs a practical ‚ÄúElexon definitions + API‚Äù crib sheet for GB power-market work.

What Elexon is (context)

Elexon administers the Balancing and Settlement Code (BSC) and runs/procures the services that compare traded vs metered volumes, calculate the imbalance price, and transfer funds.  Ôøº

Core definitions you‚Äôll see everywhere
	‚Ä¢	Settlement Period (SP): a 30-minute trading/settlement interval; SPs are in local time (GMT/BST), so clock changes create 46 ‚Äúshort day‚Äù SPs or 50 ‚Äúlong day‚Äù SPs.  Ôøº
	‚Ä¢	Balancing Mechanism Unit (BM Unit / BMU): the ‚Äúunit of trade‚Äù in the Balancing Mechanism; essentially the smallest controllable/metered grouping of plant and/or demand.  Ôøº
	‚Ä¢	Imbalance / Cash-out: after each SP, the BSC compares a party‚Äôs contracted (traded) volume with metered volume; differences are settled at the imbalance price.  Ôøº
	‚Ä¢	SBP / SSP (System Buy Price / System Sell Price): the (cash-out) imbalance prices (in ¬£/MWh). Elexon notes there is now a single price calculation, so SBP = SSP in each SP.  Ôøº
	‚Ä¢	NIV (Net Imbalance Volume): system net imbalance (MWh) for an SP; sign indicates ‚Äúshort‚Äù vs ‚Äúlong‚Äù system.  Ôøº

If you want ‚Äúofficial wording‚Äù for almost any acronym, the BSC glossary and the technical glossary are the canonical sources.  Ôøº

The modern Elexon data API (Insights Solution / BMRS)

Elexon‚Äôs Insights Solution APIs are public and no API key is required.  Ôøº

The two most-used ways to consume data
	1.	REST APIs (pull)
	‚Ä¢	‚ÄúDataset endpoints‚Äù expose named datasets (many ‚Äúas received‚Äù from the data provider) and include ‚Äústream‚Äù variants.  Ôøº
	2.	IRIS (push)
	‚Ä¢	A near real-time push service + a publicly accessible archive (useful for backfills).  Ôøº

Base URLs & specs (quick orientation)
	‚Ä¢	The platform exposes OpenAPI/Swagger definitions (e.g. /swagger/v1/swagger.json).  Ôøº
	‚Ä¢	A common dataset-style pattern you‚Äôll see in the wild is:
https://data.elexon.co.uk/bmrs/api/v1/datasets/<DATASET>/stream?...  Ôøº

Example ‚Äúwhat should I pull?‚Äù dataset shortlist
	‚Ä¢	FREQ: system frequency (real-time-ish)  Ôøº
	‚Ä¢	FUELHH: half-hourly generation outturn by fuel type (30-min granularity)  Ôøº
	‚Ä¢	MID: Market Index Data (used in imbalance price construction context)  Ôøº
	‚Ä¢	BOAL / BOD / PN / NETBSAD: core balancing/physical notification & adjustment datasets  Ôøº
	‚Ä¢	Reference data (standing data), e.g. BM Units list via /reference/bmunits/all  Ôøº

Copy/paste examples

Curl (dataset ‚Äústream‚Äù pattern) curl -s "https://data.elexon.co.uk/bmrs/api/v1/datasets/DCI/stream?publishDateTimeFrom=2019-01-01T13:08:00Z&publishDateTimeto=2024-02-24T15:08:00Z" That exact pattern returns JSON records for the dataset stream.  Ôøº

Python (generic) import requests

url = "https://data.elexon.co.uk/bmrs/api/v1/datasets/FUELHH/stream"
params = {"publishDateTimeFrom": "2025-12-01T00:00:00Z", "publishDateTimeTo": "2025-12-02T00:00:00Z"}
r = requests.get(url, params=params, timeout=60)
r.raise_for_status()
data = r.json()
print(len(data), data[0].keys())
‚∏ª

If you tell me which data you‚Äôre trying to pull (e.g., SSP/SBP, BOAs, fuel mix, demand, forecasts), I‚Äôll map it to the exact dataset code(s) + the most convenient endpoint style (dataset vs ‚Äúopinionated‚Äù endpoint vs IRIS). Below are the main places Elexon and National Grid / NESO publish official definitions / glossaries, plus the Elexon API docs.

Elexon definitions (BSC / settlement / pricing)

Elexon‚Äôs BSC site glossary and related BSC guidance:  Ôøºhttps://www.elexon.co.uk/bsc/glossary/
https://www.elexon.co.uk/bsc/settlement/
https://bscdocs.elexon.co.uk/guidance-notes/imbalance-pricing-guidance
https://www.elexon.co.uk/bsc/bsc-and-codes/
https://www.elexon.co.uk/bsc/glossary/balancing-and-settlement-code/ lexon API documentation (BMRS / Insights Solution)

Elexon‚Äôs developer portal + Swagger/OpenAPI for endpoints:  https://developer.data.elexon.co.uk/
https://bmrs.elexon.co.uk/api-documentation
https://data.elexon.co.uk/swagger/index.html
https://data.elexon.co.uk/swagger/v1/swagger.json
https://github.com/elexon-data/insights-docs/Ôøº National Grid / NESO definitions

NESO‚Äôs glossary + balancing mechanism explainer; plus the Grid Code ‚ÄúGlossary & Definitions‚Äù:  Ôøºhttps://www.neso.energy/industry-information/connections/help-and-support/glossary-terms
https://www.neso.energy/what-we-do/systems-operations/what-balancing-mechanism
https://dcm.nationalenergyso.com/
https://www.nationalgrid.com/sites/default/files/documents/8589935286-04_GLOSSARY__DEFINITIONS_I5R20.pdf A few important points, then a practical way to use it safely.

1) First: you‚Äôve pasted your live scripting key

That key=03omen6i9lhv5fa token is effectively a password-equivalent for scripted downloads. I‚Äôd treat it as compromised now:
	‚Ä¢	Regenerate / rotate it (or ask Elexon to do so if you can‚Äôt self-serve), and
	‚Ä¢	Don‚Äôt email / paste it into docs or tickets.

Elexon‚Äôs own scripting page makes clear the key is the auth mechanism for scripted access.  Ôøº

2) What this ‚Äúscripting‚Äù interface is for (and what not to do)

What you copied is exactly the Elexon Portal‚Äôs supported approach:
	‚Ä¢	Use only the downloads.elexonportal.co.uk URLs + your scripting key.
	‚Ä¢	Do not automate the normal portal login / scrape pages (they explicitly say accounts can be suspended for that).  Ôøº
	‚Ä¢	Most files update ~daily, so fetch once per day; heavy polling risks temporary key disablement (per their guidance).  Ôøº

Official page (your permalink): 3) Recommended approach on a Mac (curl + ‚Äúonly download if changed‚Äù)

Use curl in a daily scheduled job, but avoid re-downloading unchanged files.

A) Put the key in an environment variable (not in your script)

In your shell profile (zsh):export ELEXON_SCRIPTING_KEY="REDACTED_REPLACE_WITH_NEW_KEY"  Use curl with conditional download

curl -z <localfile> tells curl: only download if the remote file is newer (uses Last-Modified). This is a simple way to stay ‚Äúnon-excessive‚Äù.

Example for SSPSBPNIV_FILE (your ‚Äússpsbpniv.csv‚Äù): mkdir -p ~/elexon_downloads
cd ~/elexon_downloads

curl -L -z sspsbpniv.csv -o sspsbpniv.csv \
  "https://downloads.elexonportal.co.uk/file/download/SSPSBPNIV_FILE?key=${ELEXON_SCRIPTING_KEY}" ou can do the same for:
	‚Ä¢	REGISTERED_BMUNITS_FILE
	‚Ä¢	REGISTERED_PARTICIPANTS_FILE
	‚Ä¢	BESTVIEWPRICES_FILE
	‚Ä¢	RCRC_FILE
	‚Ä¢	TLM_FILE
	‚Ä¢	etc.

This stays aligned with Elexon‚Äôs ‚Äúdaily at most‚Äù guidance.  Ôøº

4) For P114: use the /list endpoint first, then download only new files

Elexon provides a listing interface for P114 files (optionally by date/filter). That‚Äôs the clean way to automate it.  Ôøº

List (today by default): https://downloads.elexonportal.co.uk/p114/list?key=YOUR_KEY List (specific date & filter): https://downloads.elexonportal.co.uk/p114/list?key=YOUR_KEY&date=2018-01-17&filter=s0142 Then download a specific file by replacing filename=: https://downloads.elexonportal.co.uk/p114/download?key=YOUR_KEY&filename=filename 5) Scheduling (daily)
	‚Ä¢	cron (simple) or launchd (more ‚ÄúMac-native‚Äù).
	‚Ä¢	If you‚Äôre already running other energy jobs at 18:00, put this at e.g. 18:05 Europe/London to avoid overlap.

6) One more strategic tip: prefer the public BMRS API when you can

Some of what you‚Äôre downloading (e.g., MID / FUELHH / frequency-type data) is also available via the public Insights Solution / BMRS API (no portal key), which can be easier to integrate and less sensitive to key leakage.  Ôøº
Docs: https://developer.data.elexon.co.uk/
https://bmrs.elexon.co.uk/api-documentation/introduction
‚∏ª

If you tell me which of these you want to automate first:
	‚Ä¢	(A) ‚Äúdaily static files‚Äù (SSPSBPNIV, RCRC, TLM, BM Units, etc.), or
	‚Ä¢	(B) ‚Äúrolling archives‚Äù (BMRA Data Archive / P114 with filenames),

‚Ä¶I‚Äôll give you a ready-to-run script that:
	‚Ä¢	stores downloads by date,
	‚Ä¢	avoids duplicates (hash/manifest),
	‚Ä¢	and logs what changed each day‚Äîwithout ever hardcoding the key. What this ‚ÄúPortal scripting‚Äù is vs IRIS vs the public API

1) Elexon Portal ‚Äúscripting key‚Äù downloads (what you pasted)
	‚Ä¢	Purpose: download a fixed set of pre-generated files (often CSV/XLS) that would otherwise require being logged into the Portal.
	‚Ä¢	Auth: your personal ‚Äúscripting key‚Äù appended to the downloads.elexonportal.co.uk/... URL.
	‚Ä¢	Cadence: Elexon explicitly notes most of these files are updated no more than once per day (except BMRA Data Archive and P114) and warns about excessive polling.  Ôøº
	‚Ä¢	Typical use: ‚Äúgive me the latest published file each day‚Äù (e.g., BM Units, SSP/SBP/NIV CSV, TLM, MID, calendars).

2) IRIS (Insights Real-Time Information Service)
	‚Ä¢	Purpose: near real-time push of Insights Solution data ‚Äúas files come in‚Äù.
	‚Ä¢	Transport: AMQP message queue (you run a client/subscriber).
	‚Ä¢	Bonus: there‚Äôs a public IRIS Archive (Azure blob) containing everything IRIS delivered, which can be browsed/downloaded for backfill.  Ôøº
	‚Ä¢	Typical use: ‚ÄúI want data seconds after Elexon receives/publishes it, continuously, without polling.‚Äù

3) Insights Solution / BMRS REST API
	‚Ä¢	Purpose: pull data on demand via HTTP endpoints (developer docs + OpenAPI definitions).
	‚Ä¢	Typical use: ‚ÄúI want parameterised queries (date ranges, filters), JSON responses, and selective retrieval rather than whole daily files.‚Äù  Ôøº

Helpful official entry points: Portal scripting page:
https://www.elexonportal.co.uk/SCRIPTING

IRIS:
https://bmrs.elexon.co.uk/iris
IRIS Archive (Azure blob URL mentioned by Elexon):
https://archive.data.elexon.co.uk/iris-archive

API docs:
https://bmrs.elexon.co.uk/api-documentation/introduction  What ‚ÄúDo not automate the normal portal login / scrape pages‚Äù means (practically)

Elexon is saying:
	‚Ä¢	‚úÖ Allowed: use the explicit scripted download URLs they provide (the downloads.elexonportal.co.uk/... endpoints) with your scripting key.  Ôøº
	‚Ä¢	‚ùå Not allowed: write a bot that:
	‚Ä¢	logs in via the normal login form (username/password),
	‚Ä¢	maintains cookies/sessions,
	‚Ä¢	crawls portal HTML pages,
	‚Ä¢	scrapes content or calls non-scripting endpoints at scale.

Reason: the main portal systems are sized for interactive human use, not automated scraping; they state accounts can be suspended for automating login/scraping outside the permitted URLs.  Ôøº

‚∏ª

What counts as ‚Äúheavy use‚Äù / ‚Äúexcessive scripting‚Äù here?

Elexon gives a concrete example: downloading a file every five minutes when it only updates daily could be deemed excessive and may lead to temporary disabling of your scripting key.  Ôøº

So, ‚Äúheavy use‚Äù typically includes:
	‚Ä¢	High-frequency polling of daily-updated files (e.g., every few minutes/hour).
	‚Ä¢	Repeated re-downloads of the same file when nothing has changed.
	‚Ä¢	Parallel bursts across many files/users that look like a scraper.
	‚Ä¢	Any automation that bypasses the provided scripted URLs (login + scrape).

‚∏ª

A safe rule-of-thumb that keeps you compliant
	‚Ä¢	For the ‚Äúregularly updated file‚Äù list (SSPSBPNIV, RCRC, TLM, MID, BM Units, etc.):
download once per day (or use conditional download: ‚Äúonly if changed‚Äù).  Ôøº
	‚Ä¢	For P114: use their /list endpoint to fetch what‚Äôs available for a date/filter, then download only the new filenames.  Ôøº
	‚Ä¢	For ‚ÄúI need near-real-time continuously‚Äù: use IRIS instead of hammering daily files.  Ôøº
	‚Ä¢	For ‚ÄúI need custom date-range queries‚Äù: use the API instead of downloading whole daily files.  Ôøº

If you tell me which exact outputs you want (e.g., SSP/SBP/NIV, TLM, FUELHH, MID, P114 s0142/c0291), I‚Äôll suggest the best channel (Portal vs API vs IRIS) and the lowest-load way to automate it. How this ‚ÄúPortal scripting‚Äù differs from the public API + IRIS

Portal scripting (downloads.elexonportal.co.uk + key=)
	‚Ä¢	Not public/anonymous: it‚Äôs authenticated with your personal scripting key (tied to a portal user account).  Ôøº
	‚Ä¢	File-based: you download pre-built files (CSV/XLS/etc.) using fixed URLs like /file/download/... or /p114/download?....  Ôøº
	‚Ä¢	Limited whitelist: only the URLs shown on your scripting page are ‚Äútolerated‚Äù for automation; everything else on the portal is not meant for bots.  Ôøº
	‚Ä¢	Often daily cadence: Elexon says most of these files update no more frequently than once per day (except BMRA Data Archive + P114).  Ôøº

Public BMRS/Insights Solution REST API (data.elexon.co.uk / bmrs.elexon.co.uk)
	‚Ä¢	Public access: designed as a self-service developer API; you discover endpoints via their API documentation / OpenAPI.  Ôøº
	‚Ä¢	Query-based: you request records by parameters (time windows, filters) and get JSON/CSV/XML back.  Ôøº
	‚Ä¢	Interchangeable with IRIS output: Elexon state the JSON output matches IRIS and can be used interchangeably.  Ôøº

IRIS (push)
	‚Ä¢	Public, near real-time ‚Äúpush‚Äù service (AMQP) that delivers new data with minimal latency; plus a public IRIS Archive blob store for backfill.  Ôøº

‚∏ª

Is this ‚Äúreally 2025 data‚Äù (e.g., MID_2025.csv, fuelhh_2024.csv)?

Most likely yes ‚Äî but don‚Äôt trust the filename alone. Those ‚ÄúLATEST_*‚Äù links are basically aliases to the most current file Elexon is publishing via the portal scripting downloads; they often name the file by year (commonly year-to-date). Your link LATEST_MID_FILE being labelled MID_2025.csv is consistent with it being the current-year MID file as of 2025. (But the definitive check is inside the file.)

Quick ways to verify (no guesswork)
	1.	Check the HTTP headers (Last-Modified + filename) curl -I "https://downloads.elexonportal.co.uk/file/download/LATEST_MID_FILE?key=YOURKEY" Look for Last-Modified: and Content-Disposition: (often contains the real filename).
	2.	Check the first few rows for dates/timestamps curl -L "https://downloads.elexonportal.co.uk/file/download/LATEST_MID_FILE?key=YOURKEY" | head -n 5 or download then head MID_2025.csv). The first date column will tell you whether it contains 2025 timestamps, and whether it‚Äôs YTD or a fixed window.
	3.	Spot-check the newest record tail -n 5 MID_2025.csv How this ‚ÄúPortal scripting‚Äù differs from the public API + IRIS

Portal scripting (downloads.elexonportal.co.uk + key=)
	‚Ä¢	Not public/anonymous: it‚Äôs authenticated with your personal scripting key (tied to a portal user account).  Ôøº
	‚Ä¢	File-based: you download pre-built files (CSV/XLS/etc.) using fixed URLs like /file/download/... or /p114/download?....  Ôøº
	‚Ä¢	Limited whitelist: only the URLs shown on your scripting page are ‚Äútolerated‚Äù for automation; everything else on the portal is not meant for bots.  Ôøº
	‚Ä¢	Often daily cadence: Elexon says most of these files update no more frequently than once per day (except BMRA Data Archive + P114).  Ôøº

Public BMRS/Insights Solution REST API (data.elexon.co.uk / bmrs.elexon.co.uk)
	‚Ä¢	Public access: designed as a self-service developer API; you discover endpoints via their API documentation / OpenAPI.  Ôøº
	‚Ä¢	Query-based: you request records by parameters (time windows, filters) and get JSON/CSV/XML back.  Ôøº
	‚Ä¢	Interchangeable with IRIS output: Elexon state the JSON output matches IRIS and can be used interchangeably.  Ôøº

IRIS (push)
	‚Ä¢	Public, near real-time ‚Äúpush‚Äù service (AMQP) that delivers new data with minimal latency; plus a public IRIS Archive blob store for backfill.  Ôøº

‚∏ª

Is this ‚Äúreally 2025 data‚Äù (e.g., MID_2025.csv, fuelhh_2024.csv)?

Most likely yes ‚Äî but don‚Äôt trust the filename alone. Those ‚ÄúLATEST_*‚Äù links are basically aliases to the most current file Elexon is publishing via the portal scripting downloads; they often name the file by year (commonly year-to-date). Your link LATEST_MID_FILE being labelled MID_2025.csv is consistent with it being the current-year MID file as of 2025. (But the definitive check is inside the file.)

Quick ways to verify (no guesswork)
	1.	Check the HTTP headers (Last-Modified + filename)

curl -I "https://downloads.elexonportal.co.uk/file/download/LATEST_MID_FILE?key=YOURKEY"

Look for Last-Modified: and Content-Disposition: (often contains the real filename).
	2.	Check the first few rows for dates/timestamps

curl -L "https://downloads.elexonportal.co.uk/file/download/LATEST_MID_FILE?key=YOURKEY" | head -n 5

(or download then head MID_2025.csv). The first date column will tell you whether it contains 2025 timestamps, and whether it‚Äôs YTD or a fixed window.
	3.	Spot-check the newest record

tail -n 5 MID_2025.csv

If the last rows show dates in 2025 (or even right up to recent days), it‚Äôs current.

‚∏ª

‚ÄúHeavy use‚Äù ‚Äî what they mean in practice

Elexon explicitly warns that (except BMRA Data Archive + P114) files are updated no more than daily, and gives an example of ‚Äúexcessive‚Äù as downloading something every five minutes when it only updates daily.  Ôøº

A safe operating pattern:
	‚Ä¢	For daily files (SSPSBPNIV, TLM, RCRC, BM Units, MID, etc.): 1√ó per day max, or use conditional download (curl -z) so you only fetch when it changes.
	‚Ä¢	For P114: use /p114/list once per day (or a small number of times) and download only new filenames.  Ôøº
	‚Ä¢	Do not script normal portal login + scrape HTML pages (they say accounts can be suspended for that).  Ôøº

‚∏ª

If you tell me which exact feeds you care about (MID, SSP/SBP/NIV, fuelhh, BMRA archive, P114), I‚Äôll recommend the cleanest route (Portal file vs API vs IRIS) and a low-load download pattern for each. You‚Äôre not wrong that a lot of ‚ÄúBMRS-era‚Äù access has migrated ‚Äî but the thing you pasted (Elexon Portal ‚Äúscripting key‚Äù downloads) hasn‚Äôt been shut down in general. It‚Äôs a separate channel from the newer Insights Solution API and IRIS.

1) Are the portal scripting downloads ‚Äúclosed / migrated‚Äù?

The Portal scripting page is still live and still describes the scripting-key approach and the ‚Äúdownload once per day‚Äù expectation.  Ôøº

What has migrated is the old BMRS endpoints, which Elexon states are replaced by the Insights Solution.  Ôøº

So:
	‚Ä¢	Portal scripting = authenticated file downloads for a whitelisted set of portal files.  Ôøº
	‚Ä¢	Insights Solution = modern ‚Äúreplacement‚Äù for BMRS endpoints; open/self-serve access.  Ôøº

2) Portal scripting vs Public API vs IRIS (what‚Äôs actually different)

Portal scripting (downloads.elexonportal.co.uk + scripting key)
	‚Ä¢	Not public: tied to a portal user and a scripting key.  Ôøº
	‚Ä¢	File-based: you fetch whole files (CSV/XLS/etc.) by URL.  Ôøº
	‚Ä¢	Rate expectation: most items update no more than once per day (except BMRA archive + P114).  Ôøº

Insights Solution REST API (public pull)
	‚Ä¢	Open/self-serve: Elexon says the data is open and available for anyone to access/modify/distribute.  Ôøº
	‚Ä¢	Query-based: you request datasets/endpoints with parameters rather than downloading a daily file.  Ôøº

IRIS (public push + archive)
	‚Ä¢	Near real-time push service; plus a public archive you can browse/download (Azure blob).  Ôøº

3) ‚ÄúSafe operating pattern‚Äù ‚Äî still valid, but here‚Äôs the modern interpretation

Your bullets are still the right idea for the portal scripting channel because Elexon explicitly says those files are usually daily and warns about excessive polling.  Ôøº

A good ‚Äú2025-proof‚Äù pattern is:
	‚Ä¢	Daily portal files (SSPSBPNIV, TLM, RCRC, BM Units, MID, etc.):
1√ó per day or conditional download (only if changed).  Ôøº
	‚Ä¢	P114:
call /p114/list for the date/filter you care about, then download only filenames you haven‚Äôt seen.  Ôøº
	‚Ä¢	Do not automate login/scrape portal pages:
Elexon explicitly says only the listed scripted URLs are tolerated and automating normal login/scraping can get accounts suspended.  Ôøº

And the ‚Äúmigration‚Äù angle:
	‚Ä¢	If you need lots of history or parameterised pulls, prefer the Insights API.  Ôøº
	‚Ä¢	If you need near-real-time updates without polling, prefer IRIS.  Ôøº

4) What counts as ‚Äúheavy use‚Äù?

Elexon‚Äôs own example of ‚Äúexcessive‚Äù is hitting a daily-updated file every five minutes.  Ôøº

So ‚Äúheavy‚Äù in practice usually means:
	‚Ä¢	high-frequency polling of daily files,
	‚Ä¢	repeated re-downloading unchanged files,
	‚Ä¢	parallel scraping-style behaviour,
	‚Ä¢	or automating the portal login flow instead of using the provided scripted URLs.  Ôøº

5) If you‚Äôre seeing failures that look like ‚Äúit‚Äôs closed‚Äù

Common causes (with portal scripting) are:
	‚Ä¢	permissions (many portal pages/files require being logged in / having access)  Ôøº
	‚Ä¢	key disabled due to over-polling (they say they may temporarily disable the key)  Ôøº
	‚Ä¢	redirects / SSO changes (curl needs -L), or corporate proxy/TLS interception

If you paste (redacting the key) the HTTP status + headers you get from one of the download URLs (e.g. curl -IL ... output), I can tell you which bucket it‚Äôs in and whether you should switch that feed to Insights API or IRIS. Yes ‚Äî you can do a proper BigQuery audit by treating Elexon REST API and/or IRIS (or IRIS Archive) as the ground-truth feed and comparing what you‚Äôve ingested into BigQuery.

Key enabler: Elexon‚Äôs docs state the API JSON output matches IRIS and can be used interchangeably, so you can validate BigQuery against either source.  Ôøº
IRIS also has a public archive with a defined folder structure, useful for backfills/audits.  Ôøº

‚∏ª

Audit design (what to compare)

Choose your audit ‚Äútruth‚Äù per use case
	‚Ä¢	Historical completeness / backfills ‚Üí use REST API (pull by time window) or IRIS Archive (download the exact files IRIS delivered).  Ôøº
	‚Ä¢	Near-real-time latency / freshness ‚Üí use IRIS push and compare ‚Äúarrival time‚Äù vs what landed in BigQuery.  Ôøº

What you usually audit
	1.	Completeness: did BigQuery miss any intervals/messages?
	2.	Duplicates: did BigQuery ingest the same key more than once?
	3.	Timeliness: how long between ‚Äúpublished‚Äù and ‚Äúlanded in BQ‚Äù?
	4.	Integrity: do key fields and values match source (within expected tolerances)?
	5.	Schema drift: did the source schema change vs your BQ schema?

‚∏ª

Concrete approach that works well

Step 1 ‚Äî Create an audit schema in BigQuery

Create 2 tables:
	‚Ä¢	audit_runs (dataset, window_start, window_end, run_time, source_used)
	‚Ä¢	audit_results (dataset, metric_name, metric_value, window_start, window_end, details_json)

Step 2 ‚Äî For each dataset, define a ‚Äúprimary key‚Äù for dedupe + comparison

Example patterns:
	‚Ä¢	FREQ: measurement_time (or timestamp + resolution)
	‚Ä¢	FUELHH: settlement_date + settlement_period + fuel_type
	‚Ä¢	MID: depends on record fields (often date/time + product/key fields)
(You‚Äôll map this once per dataset.)

Step 3 ‚Äî Pull the same window from REST API (or IRIS Archive), load to a temp table, then compare

REST API has dataset and stream endpoints (e.g. /datasets/<CODE> and /datasets/<CODE>/stream).  Ôøº
IRIS Archive lets you fetch the delivered files by dataset folder.  Ôøº

‚∏ª

Example BigQuery checks (templates)

A) Completeness (missing keys) -- Replace with your dataset table + key
WITH src AS (
  SELECT DISTINCT key_field
  FROM `project.raw_elexon.freq`
  WHERE measurement_time BETWEEN @start AND @end
),
bq AS (
  SELECT DISTINCT key_field
  FROM `project.curated_elexon.freq`
  WHERE measurement_time BETWEEN @start AND @end
)
SELECT
  COUNT(*) AS missing_in_bq
FROM src
LEFT JOIN bq USING (key_field)
WHERE bq.key_field IS NULL; SELECT
  key_field,
  COUNT(*) AS n
FROM `project.raw_elexon.freq`
WHERE measurement_time BETWEEN @start AND @end
GROUP BY key_field
HAVING COUNT(*) > 1
ORDER BY n DESC; SELECT
  APPROX_QUANTILES(TIMESTAMP_DIFF(ingested_at, published_at, SECOND), 101) AS latency_seconds_percentiles
FROM `project.raw_elexon.some_dataset`
WHERE published_at BETWEEN @start AND @end; D) Settlement-period expected-count check (48/46/50 caveat)

For SP-based datasets, build an ‚Äúexpected SP calendar‚Äù table in BQ (Europe/London) and join to actual keys. (This is where BST days produce 46/50 SPs, so your expected set must be timezone-aware.)
How to wire REST API / IRIS into the audit

REST API pull (audit/backfill)
	‚Ä¢	Use API docs to pick dataset codes and call /datasets/<CODE>/stream for the audit window.  Ôøº
	‚Ä¢	Load the response to a temporary BQ table (e.g. audit_source_<dataset>_<runid>) and run the SQL comparisons above.

IRIS push (latency + ‚Äúdid we miss messages?‚Äù)
	‚Ä¢	Subscribe to IRIS (AMQP) using a client; Elexon provides example clients and notes it‚Äôs AMQP-based.  Ôøº
	‚Ä¢	Ingest messages into a ‚Äúraw‚Äù table with received_at and compute:
	‚Ä¢	gaps vs BigQuery curated table
	‚Ä¢	latency percentiles

IRIS Archive (for exact replay / reconciliation)
	‚Ä¢	When you need ‚Äúwhat IRIS actually delivered‚Äù, use IRIS Archive structure by dataset folders.  Ôøº
	‚Ä¢	This is great for reconciling a suspected missed-ingest day without relying on the current API view.

‚∏ª

Practical recommendation for your BigQuery estate
	‚Ä¢	Keep two layers:
	‚Ä¢	raw_elexon_* (append-only, store published_at, received_at, full payload)
	‚Ä¢	curated_elexon_* (deduped, typed, partitioned)
	‚Ä¢	Run an audit job daily per dataset:
	‚Ä¢	completeness + duplicates + latency
	‚Ä¢	write results to audit_results
	‚Ä¢	For any failed day, reconcile via IRIS Archive (most deterministic).  Ôøº

If you tell me your BigQuery table names for 2‚Äì3 datasets (e.g., frequency, fuelhh, mid) and whether you already store published_at / ingested_at, I‚Äôll draft the exact audit SQL + a minimal Python puller that hits the correct Elexon /datasets/<CODE>/stream endpoints. Yep ‚Äî you can absolutely audit your BigQuery ingests using (1) REST API pull and/or (2) IRIS push + IRIS Archive, and you can run the whole thing as a Railway Cron job (plus an optional always-on IRIS consumer).

Below is a ‚Äúdo-this‚Äù blueprint you can copy into your repo + Railway.

‚∏ª

Why this works (source-of-truth choices)

Elexon explicitly says the dataset API JSON output matches IRIS and can be used interchangeably.  Ôøº
IRIS is a public AMQP push service.  Ôøº
IRIS Archive stores everything IRIS sends and requires no auth, which is perfect for reconciliation/backfills.  Ôøº
The developer portal also states the REST APIs are public and no API key is required, and dataset endpoints support retrieval by published times (handy for backfill/superseded records).  Ôøº

So your audit can use either:
	‚Ä¢	REST API as truth (simple daily pull and compare), or
	‚Ä¢	IRIS/IRIS Archive as truth (more deterministic for ‚Äúwhat actually got sent‚Äù).

‚∏ª

What ‚Äúaudit‚Äù should check (minimum viable)

For each dataset (e.g., FREQ, FUELHH, MID):
	1.	Completeness: missing keys/time-buckets in BigQuery vs source
	2.	Duplicates: same natural key ingested >1 time
	3.	Freshness/latency: ingested_at - published_at percentiles (if you store both)
	4.	Schema drift: fields added/removed/changed type

‚∏ª

Railway setup (recommended architecture)

Service A ‚Äî ‚Äúdaily audit‚Äù (Railway Cron)

Runs once/day, pulls from REST API (or IRIS Archive), compares to BigQuery, writes results into an audit_results table, then exits.

Railway Cron requirements:
	‚Ä¢	Cron runs your service start command, and the process must exit when done.  Ôøº
	‚Ä¢	Schedules are UTC, and Railway won‚Äôt run the next one if the previous is still running.  Ôøº

Optional Service B ‚Äî ‚ÄúIRIS consumer‚Äù (always-on)

Subscribes via AMQP and writes raw messages into BigQuery (or Pub/Sub ‚Üí BQ). This gives you true ‚Äúpush arrival time‚Äù and lets you audit ingestion latency cleanly. IRIS is AMQP and Elexon provides example clients.  Ôøº

‚∏ª

Railway configuration details

1) Cron schedule

In Railway ‚Üí Service Settings ‚Üí Cron Schedule.
Example: run at 18:05 UTC daily:

5 18 * * *

(If you want ‚Äú18:05 London time‚Äù year-round, you‚Äôll need to adjust for DST because Railway cron is UTC.  Ôøº)

2) Start command

Set a start command like:

python -m audit.run

Railway supports overriding start commands in service settings.  Ôøº

3) Secrets / env vars

Put config and secrets in Railway Variables (they become environment variables at build/runtime).  Ôøº

Typical variables:
	‚Ä¢	GOOGLE_APPLICATION_CREDENTIALS_JSON_B64 (base64 of service account JSON)
	‚Ä¢	BQ_PROJECT, BQ_DATASET
	‚Ä¢	AUDIT_DATASETS=FREQ,FUELHH,MID
	‚Ä¢	ELEXON_API_BASE=https://data.elexon.co.uk/bmrs/api/v1 (or from your chosen base)

4) (Optional) Persist ‚Äúwatermarks‚Äù with a Volume

If you want a durable ‚Äúlast audited published time‚Äù, attach a Railway Volume and store a small state file there. Railway volumes persist data.  Ôøº

‚∏ª

Repo skeleton (copy/paste layout)

.
‚îú‚îÄ audit/
‚îÇ  ‚îú‚îÄ run.py
‚îÇ  ‚îú‚îÄ elexon.py
‚îÇ  ‚îú‚îÄ bigquery.py
‚îÇ  ‚îú‚îÄ checks.py
‚îÇ  ‚îî‚îÄ config.py
‚îú‚îÄ requirements.txt
‚îî‚îÄ README.md

requirements.txt (starter)

google-cloud-bigquery
google-auth
requests
pandas
pyarrow
python-dateutil


‚∏ª

How the audit job works (logic)

Step A ‚Äî fetch source rows (REST API pull)

Use ‚Äúdataset endpoints‚Äù / ‚Äústream variants‚Äù (Elexon calls these out as core patterns).  Ôøº
Because the exact query params vary by endpoint, the robust approach is:
	‚Ä¢	Keep dataset codes in config (FREQ, FUELHH, MID)
	‚Ä¢	For each dataset, hit:
GET {ELEXON_API_BASE}/datasets/{CODE}/stream  Ôøº
	‚Ä¢	Use the OpenAPI/Swagger UI to confirm parameter names for time windows. (The docs are published and meant for this.)  Ôøº

Step B ‚Äî compare with BigQuery

You‚Äôll do comparisons on natural keys and/or time buckets.

Example duplicate check template:

SELECT
  CONCAT(CAST(settlement_date AS STRING), '-', CAST(settlement_period AS STRING), '-', fuel_type) AS natural_key,
  COUNT(*) AS n
FROM `project.dataset.elexon_fuelhh_raw`
WHERE settlement_date BETWEEN @start_date AND @end_date
GROUP BY natural_key
HAVING n > 1
ORDER BY n DESC;

Example completeness check (keys missing in BQ vs source temp table):

SELECT COUNT(*) AS missing_in_bq
FROM `project.audit.source_keys_temp` s
LEFT JOIN `project.dataset.elexon_fuelhh_raw` b
ON s.natural_key = b.natural_key
WHERE b.natural_key IS NULL;

Step C ‚Äî write results back into BigQuery

Write metrics into something like:
	‚Ä¢	audit.audit_runs
	‚Ä¢	audit.audit_results

so you can chart them (Looker Studio / Sheets / whatever).

‚∏ª

If you want IRIS-based auditing (strongest method)
	‚Ä¢	IRIS is AMQP and Elexon provides client examples.  Ôøº
	‚Ä¢	IRIS Archive stores the exact files sent and is unauthenticated; great for replay.  Ôøº
	‚Ä¢	Because API output matches IRIS, you can mix: ‚ÄúREST for backfill, IRIS for realtime‚Äù.  Ôøº

Railway pattern:
	‚Ä¢	Service B (always-on): consume IRIS ‚Üí BigQuery raw_iris_* tables (store received_at).
	‚Ä¢	Service A (cron): compare raw_iris_* vs your curated tables + compute latency.

‚∏ª

What I need from you to make it ‚Äúexact‚Äù (no guessing)

No need for credentials ‚Äî just names.

Send:
	1.	Your BigQuery table names for 2‚Äì3 datasets (e.g. uk_energy_insights.elexon_frequency, ...elexon_fuelhh, ...elexon_market_index)
	2.	Whether you store published_at and ingested_at (or similar)
	3.	Which truth source you prefer for the audit: REST, IRIS Archive, or both

‚Ä¶and I‚Äôll give you:
	‚Ä¢	the exact dataset-to-table mapping config,
	‚Ä¢	the exact SQL for completeness/dupes/latency per dataset,
	‚Ä¢	and a ready-to-deploy Railway audit.run module that exits cleanly (Cron-friendly). .
‚îú‚îÄ audit/
‚îÇ  ‚îú‚îÄ run.py
‚îÇ  ‚îú‚îÄ elexon.py
‚îÇ  ‚îú‚îÄ bigquery.py
‚îÇ  ‚îú‚îÄ checks.py
‚îÇ  ‚îî‚îÄ config.py
‚îú‚îÄ requirements.txt
‚îî‚îÄ README.md google-cloud-bigquery
google-auth
requests
pandas
pyarrow
python-dateutil  can‚Äôt literally ‚Äúrun it now‚Äù from here because I don‚Äôt have network access to hit Elexon endpoints, and I don‚Äôt have access to your BigQuery project. What I can do is give you a drop-in audit job you can run locally or deploy on Railway as a scheduled (cron) service, and it will produce the audit output against your BigQuery tables immediately.

Below is: (1) what‚Äôs different vs public API/IRIS, (2) how to confirm ‚Äúis this really 2025 data‚Äù, and (3) a Railway-ready BigQuery audit job that compares Elexon ‚Äúlatest publish time‚Äù vs your BigQuery max publish time + simple freshness/coverage checks.

‚∏ª

1) Portal ‚Äúscripting key‚Äù vs public API vs IRIS

Elexon Portal ‚ÄúScripting Key‚Äù (downloads.elexonportal.co.uk)
	‚Ä¢	Purpose: scripted download of a curated list of files that normally require portal login (so they give you a token instead of username/password).
	‚Ä¢	Auth: your scripting key (treat it like a password; don‚Äôt paste it into chats/logs).
	‚Ä¢	Rate expectations: explicitly ‚Äúdon‚Äôt hammer it‚Äù; many files update ~daily.
	‚Ä¢	Risk: they explicitly warn not to automate normal portal login + scrape HTML (account suspension).

Insights Solution Public REST API (data.elexon.co.uk / developer portal)
	‚Ä¢	Purpose: production-grade REST APIs providing datasets ‚Äúas received‚Äù from data providers, plus derived ‚Äúopinionated‚Äù endpoints.  Ôøº
	‚Ä¢	Auth: no API key required (public).  Ôøº
	‚Ä¢	Streaming endpoints: ‚ÄúStream endpoints‚Äù exist and return efficient JSON; the portal text says these have no restrictions/limits on data return.  Ôøº
	‚Ä¢	Dataset list includes things like FREQ, FUELHH, MID, etc.  Ôøº
	‚Ä¢	Example of the base path in use: https://data.elexon.co.uk/bmrs/api/v1/datasets/...  Ôøº

IRIS (push)
	‚Ä¢	Purpose: near real-time push over AMQP (good for low-latency ingestion), plus a public archive for backfills (handy when your pipeline missed messages).
	‚Ä¢	Auth: requires an account + IRIS client credentials (queue name, client ID/secret).  Ôøº

Practical difference:
	‚Ä¢	Portal scripting key = ‚Äúdownload these portal-managed files safely (daily-ish)‚Äù.
	‚Ä¢	Public Insights REST = ‚Äúquery datasets directly (including superseded/historic via publish times)‚Äù.  Ôøº
	‚Ä¢	IRIS = ‚Äústream updates as they happen‚Äù.

‚∏ª

2) ‚ÄúIs this really 2025 data?‚Äù

Two separate issues get mixed up:
	1.	Your portal download list showing 2025/2026 filenames
That indicates those specific calendar/report files are current on the portal, but doesn‚Äôt prove each dataset row is ‚Äú2025 data‚Äù.
	2.	Operational datasets (FREQ/MID/FUELHH etc.) being current today
The robust way is to check the latest publish time from the public API and compare it to what you‚Äôve ingested into BigQuery. The developer portal explicitly mentions ‚ÄúLatest publish times for each dataset available in the API‚Äù and shows a metadata/latest path.  Ôøº

So: you don‚Äôt guess‚Äîyou measure the lag.

‚∏ª

3) BigQuery audit job (REST metadata vs your BigQuery)

This job:
	‚Ä¢	calls the public endpoint ‚Ä¶/datasets/METADATA/latest (as shown on the developer portal)  Ôøº
	‚Ä¢	extracts latest publish times per dataset
	‚Ä¢	queries BigQuery for each dataset table to compute:
	‚Ä¢	bq_max_publish_time
	‚Ä¢	lag_minutes vs Elexon latest
	‚Ä¢	rows_last_window (e.g., last 24h based on your publish time column)

A) Python audit script (single file) #!/usr/bin/env python3
"""
Elexon ‚Üî BigQuery freshness audit.

What it does:
- Fetches latest publish timestamps from the public Insights dataset METADATA endpoint.
- For each dataset you specify, queries BigQuery to get MAX(publish_time) and row counts in a window.
- Prints a clear audit report and optionally writes results to an audit table.

Requirements:
- pip install google-cloud-bigquery requests python-dateutil
- BigQuery credentials via GOOGLE_APPLICATION_CREDENTIALS
  OR SERVICE_ACCOUNT_JSON_BASE64 (Railway-friendly)
"""

from __future__ import annotations

import base64
import json
import os
import sys
from dataclasses import dataclass
from datetime import datetime, timezone
from typing import Any, Dict, List, Optional, Tuple

import requests
from dateutil import parser as dtparser
from google.cloud import bigquery


DEFAULT_BASE = "https://data.elexon.co.uk/bmrs/api/v1"
DEFAULT_METADATA_PATH = "/datasets/METADATA/latest"


@dataclass
class DatasetConfig:
    dataset_code: str
    bq_table: str                 # fully-qualified: project.dataset.table
    publish_ts_col: str = "publishTime"  # your column name in BigQuery


def _ensure_sa_file_from_base64() -> Optional[str]:
    """
    Railway pattern:
      - store service account JSON as SERVICE_ACCOUNT_JSON_BASE64
      - write it to /tmp/sa.json
      - set GOOGLE_APPLICATION_CREDENTIALS to that path
    """
    b64 = os.getenv("SERVICE_ACCOUNT_JSON_BASE64", "").strip()
    if not b64:
        return None

    sa_path = "/tmp/gcp_sa.json"
    if not os.path.exists(sa_path):
        raw = base64.b64decode(b64.encode("utf-8"))
        with open(sa_path, "wb") as f:
            f.write(raw)
    os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = sa_path
    return sa_path


def fetch_metadata_latest(base_url: str) -> Any:
    url = base_url.rstrip("/") + DEFAULT_METADATA_PATH
    r = requests.get(url, timeout=30)
    r.raise_for_status()
    return r.json()


def _parse_latest_publish_map(payload: Any) -> Dict[str, datetime]:
    """
    The metadata payload format can evolve, so parse defensively.
    We try to produce: { "FREQ": datetime(...UTC...), ... }
    """
    def as_dt(v: Any) -> Optional[datetime]:
        if v is None:
            return None
        if isinstance(v, (int, float)):
            # unlikely, but handle epoch seconds
            return datetime.fromtimestamp(float(v), tz=timezone.utc)
        if isinstance(v, str) and v.strip():
            dt = dtparser.isoparse(v)
            return dt.astimezone(timezone.utc) if dt.tzinfo else dt.replace(tzinfo=timezone.utc)
        return None

    latest: Dict[str, datetime] = {}

    # Common patterns: {"data":[{...},{...}]} or a list directly
    items = None
    if isinstance(payload, dict):
        if "data" in payload and isinstance(payload["data"], list):
            items = payload["data"]
        elif "items" in payload and isinstance(payload["items"], list):
            items = payload["items"]
    if items is None and isinstance(payload, list):
        items = payload

    if not isinstance(items, list):
        return latest

    for row in items:
        if not isinstance(row, dict):
            continue

        # dataset code field candidates
        code = (
            row.get("dataset")
            or row.get("datasetCode")
            or row.get("name")
            or row.get("datasetName")
        )
        if not isinstance(code, str) or not code.strip():
            continue
        code = code.strip()

        # timestamp field candidates
        ts = (
            row.get("latestPublishTime")
            or row.get("publishTime")
            or row.get("publishedTime")
            or row.get("lastUpdated")
            or row.get("latest")
        )
        dt = as_dt(ts)
        if dt:
            latest[code] = dt

    return latest


def bq_audit_dataset(
    bq: bigquery.Client,
    cfg: DatasetConfig,
    window_hours: int,
) -> Tuple[Optional[datetime], int]:
    """
    Returns:
      (max_publish_time, rows_in_window)
    """
    query = f"""
    SELECT
      MAX({cfg.publish_ts_col}) AS max_publish_time,
      COUNTIF({cfg.publish_ts_col} >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL @window_hours HOUR)) AS rows_in_window
    FROM `{cfg.bq_table}`
    """
    job = bq.query(
        query,
        job_config=bigquery.QueryJobConfig(
            query_parameters=[bigquery.ScalarQueryParameter("window_hours", "INT64", window_hours)]
        ),
    )
    rows = list(job.result())
    if not rows:
        return None, 0

    max_ts = rows[0]["max_publish_time"]
    if isinstance(max_ts, datetime):
        max_ts = max_ts.astimezone(timezone.utc) if max_ts.tzinfo else max_ts.replace(tzinfo=timezone.utc)
    else:
        max_ts = None

    return max_ts, int(rows[0]["rows_in_window"] or 0)


def main() -> int:
    _ensure_sa_file_from_base64()

    base_url = os.getenv("ELEXON_BASE_URL", DEFAULT_BASE).strip() or DEFAULT_BASE
    window_hours = int(os.getenv("WINDOW_HOURS", "24"))

    # DATASETS_CONFIG is a JSON array like:
    # [
    #   {"dataset_code":"FREQ","bq_table":"myproj.uk_energy_insights.freq","publish_ts_col":"publishTime"},
    #   ...
    # ]
    cfg_raw = os.getenv("DATASETS_CONFIG", "").strip()
    if not cfg_raw:
        print("ERROR: Set DATASETS_CONFIG env var (JSON array).", file=sys.stderr)
        return 2

    cfg_list = json.loads(cfg_raw)
    configs: List[DatasetConfig] = [DatasetConfig(**c) for c in cfg_list]

    print(f"Using Elexon base: {base_url}")
    print(f"Audit window: last {window_hours} hours")
    print("Fetching Elexon METADATA/latest ...")
    meta_payload = fetch_metadata_latest(base_url)
    latest_map = _parse_latest_publish_map(meta_payload)

    bq = bigquery.Client()

    now_utc = datetime.now(timezone.utc)
    print("\n=== AUDIT REPORT (UTC) ===")
    print(f"Run time: {now_utc.isoformat()}")

    any_fail = False

    for cfg in configs:
        elexon_latest = latest_map.get(cfg.dataset_code)
        bq_max, rows_in_window = bq_audit_dataset(bq, cfg, window_hours)

        lag_min = None
        if elexon_latest and bq_max:
            lag_min = int((elexon_latest - bq_max).total_seconds() / 60)

        print("\n---")
        print(f"Dataset: {cfg.dataset_code}")
        print(f"BQ table: {cfg.bq_table}")
        print(f"Publish TS col: {cfg.publish_ts_col}")
        print(f"Elexon latest publish: {elexon_latest.isoformat() if elexon_latest else 'NOT FOUND in METADATA/latest'}")
        print(f"BQ max publish:        {bq_max.isoformat() if bq_max else 'NO ROWS / NULL'}")
        print(f"Lag (minutes):         {lag_min if lag_min is not None else 'n/a'}")
        print(f"Rows in last window:   {rows_in_window}")

        # simple health heuristics (tune to taste)
        if elexon_latest and not bq_max:
            any_fail = True
        if lag_min is not None and lag_min > 60:
            any_fail = True

    print("\n=== RESULT ===")
    print("FAIL" if any_fail else "PASS")
    return 1 if any_fail else 0


if __name__ == "__main__":
    raise SystemExit(main()) Because they‚Äôre different services with different access models, even though they may ultimately be publishing overlapping operational data.

1) Why you have to log in to bmrs.elexon.co.uk/iris

IRIS is a push service (AMQP). To receive the live stream you need:
	‚Ä¢	an Insights Solution account, and
	‚Ä¢	client credentials / queue details issued to you so your AMQP client can subscribe.  Ôøº

What‚Äôs public without logging in:
	‚Ä¢	The IRIS Archive is publicly accessible (Azure blob container).  Ôøº

So: login isn‚Äôt to ‚Äúview updates‚Äù, it‚Äôs to provision credentials for the push channel.

2) Why you have to log in to elexonportal.co.uk/scripting?...

That page is part of the ELEXON Portal (separate site) and is about scripted downloads of portal-managed files using a personal scripting key:
	‚Ä¢	The page generates URLs pre-filled with your scripting key and only shows the content you‚Äôre permitted to script against.  Ôøº
	‚Ä¢	Those files ‚Äúwould normally require you to be logged in‚Äù, so the scripting key substitutes for username/password in automation.  Ôøº

Also: the cachebust=... query string is just a browser trick to force-refresh the page; it doesn‚Äôt change what you‚Äôre allowed to access.

3) Why you don‚Äôt need to log in for the public REST API

The newer Insights Solution REST APIs are public and no API key is required.  Ôøº

So you get:
	‚Ä¢	REST (pull): public HTTP endpoints, query by parameters, fetch when you want.  Ôøº
	‚Ä¢	IRIS (push): public service, but needs login only to issue subscriber credentials.  Ôøº
	‚Ä¢	Portal scripting: private/controlled, tied to your portal account and scripting key; mostly daily file drops.  Ôøº

4) ‚ÄúIf all updated, why three ways?‚Äù

Because ‚Äúupdated‚Äù means different things:
	‚Ä¢	Portal scripting files: many are explicitly daily (except BMRA archive + P114) and Elexon warns against frequent re-downloading.  Ôøº
	‚Ä¢	IRIS push: designed for seconds-level latency as soon as Elexon receives/publishes data.  Ôøº
	‚Ä¢	REST API: designed for on-demand queries + backfills without needing a subscriber client.  Ôøº

5) Which one you should use (rule of thumb)
	‚Ä¢	If you want simple daily ‚Äúlatest file‚Äù downloads (SSPSBPNIV, TLM, RCRC, etc.) ‚Üí Portal scripting.
	‚Ä¢	If you want near-real-time ingestion with minimal polling ‚Üí IRIS.
	‚Ä¢	If you want data pulls, backfills, and flexible date-range queries ‚Üí Public REST API.

If you tell me which datasets you care about (e.g., SSP/SBP/NIV, MID, FUELHH, Frequency), I‚Äôll map each one to the best channel and the exact endpoints/URLs to use. The daily ‚Äúlatest files‚Äù you mentioned (e.g. SSPSBPNIV, TLM, RCRC, MID, Best View Prices) are operational market/settlement outputs ‚Äî they don‚Äôt contain a roster of Parties, so they won‚Äôt ‚Äúlist all VTPs‚Äù.  Ôøº

The only Portal-scripting files in your list that could contain VTPs

From the scripting catalogue you pasted, the only ones that are about parties are:
	1.	REGISTERED_PARTICIPANTS_FILE

	‚Ä¢	This is the candidate file to use if you want ‚Äúall Parties / participants‚Äù in one place (and then filter for VTP if the file includes a role/capacity field).

	2.	REGISTERED_BMUNITS_FILE

	‚Ä¢	This is about BM Units. It can help you link a Party to BM Units, but it‚Äôs not a clean ‚Äúlist of VTP companies‚Äù on its own.

Everything else in that ‚Äúdaily downloads‚Äù set is not a party register.

Can REGISTERED_PARTICIPANTS_FILE guarantee ‚Äúall VTPs‚Äù?

Only if the file includes a column that explicitly shows the Party‚Äôs registered role/capacity (e.g., VTP / VLP / Supplier / NPT). Elexon treats VTP as a Party/role in its market role documentation.  Ôøº

But I can‚Äôt confirm from public sources that the Registered Participants file definitely includes a ‚ÄúVTP‚Äù flag/field, because Elexon doesn‚Äôt publish that file schema openly.

The practical way to answer this (2-minute check)
	1.	Download REGISTERED_PARTICIPANTS_FILE via your scripting URL.
	2.	Open it and check the headers for anything like:

	‚Ä¢	Role, Party Type, Participation Capacity, Market Role, BSC Role, etc.

	3.	If such a column exists, filter for VTP.

If no role/capacity column exists, then the file may still list all Parties, but you cannot reliably identify which of them are VTPs from that file alone.

If the file doesn‚Äôt show ‚ÄúVTP‚Äù

Your next best options are:
	‚Ä¢	Use Elexon Kinnect / Market Entry sources (often login-gated) because that‚Äôs where role registrations are managed/visible.  Ôøº
	‚Ä¢	Or ask Elexon Support for ‚Äúthe authoritative register/list of current VTPs‚Äù (because VTP is a specific BSC role).  Ôøº

If you paste the column headers (just the first line) from your downloaded REGISTERED_PARTICIPANTS_FILE (remove your key), I‚Äôll tell you immediately whether it contains enough fields to extract all VTPs, and what exact filter to apply. Sure ‚Äî think of those three as three delivery channels for similar underlying operational data, but packaged for different jobs.

Portal ‚Äúscripting key‚Äù downloads (Elexon Portal)

What it includes (typical examples from your list):
	‚Ä¢	Operational CSVs / ‚Äúlatest‚Äù files: SSPSBPNIV_FILE (cash-out), TLM_FILE, RCRC_FILE, LATEST_MID_FILE, LATESTFUELHHFILE, ‚ÄúBest View Prices‚Äù, etc.
	‚Ä¢	Reference / standing data: REGISTERED_BMUNITS_FILE, REGISTERED_PARTICIPANTS_FILE, GSP group correction factor file, etc.
	‚Ä¢	Industry calendars & timetables: SAA/SVAA calendars, D0095 reporting calendar, FAA payment calendar, PF Settlement run calendar, PARMS timetables.
	‚Ä¢	Network settlement artefacts: SVA/CVA Line Loss Factors (downloaded by LDSO + format), etc.
	‚Ä¢	High-volume archives (special cases):
	‚Ä¢	BMRA Data Archive (filename-based; lots of files)
	‚Ä¢	P114 (file list endpoint + downloads; can also be FTP)

What it‚Äôs for:
	‚Ä¢	A safe, approved way to automate downloads of a whitelisted set of portal-managed files that normally require login, without scripting username/password.
	‚Ä¢	Best when you just need the daily ‚Äúlatest file‚Äù version of a handful of artefacts.

Operational limits / behaviour:
	‚Ä¢	Elexon warns most of these are updated no more than once per day, and gives the example that polling every 5 minutes can be ‚Äúexcessive‚Äù and may lead to your key being temporarily disabled.  Ôøº
	‚Ä¢	It‚Äôs explicitly not a general scraping permission: they only tolerate scripted access to the listed URLs.  Ôøº

‚∏ª

IRIS push (Insights Solution)

What it includes:
	‚Ä¢	Near real-time ‚Äúfiles/messages‚Äù for Insights Solution data (same ecosystem as the modern BMRS/Insights datasets).
	‚Ä¢	A public IRIS Archive containing everything sent to IRIS subscribers, accessible via Azure Storage Explorer using the published blob URL.  Ôøº

What it‚Äôs for:
	‚Ä¢	Seconds-level latency ingestion without polling.
	‚Ä¢	Best for pipelines where you want ‚Äúas soon as published, stream it into BigQuery‚Äù.

Why login exists:
	‚Ä¢	IRIS is AMQP-based and you need client credentials to subscribe; Elexon provides example IRIS clients and notes it‚Äôs AMQP.  Ôøº

‚∏ª

Public REST API (Insights Solution / ‚ÄúBMRS API‚Äù)

What it includes:
	‚Ä¢	Dataset endpoints that expose named datasets ‚Äúas received‚Äù from the data provider, with support for retrieving by published times (useful for superseded forecasts and backfills).  Ôøº
	‚Ä¢	Multiple output formats (CSV/XML/JSON) and ‚Äúopinionated endpoints‚Äù derived from datasets (handy for user-friendly questions).  Ôøº

What it‚Äôs for:
	‚Ä¢	On-demand queries (‚Äúgive me this dataset for this window/filter‚Äù).
	‚Ä¢	Backfills and reconciliation (because you can request by published time and re-pull history/superseded versions).  Ôøº

‚∏ª

Which channel to use for what (quick chooser)
	‚Ä¢	Daily file drops / reference lists / calendars ‚Üí Portal scripting
	‚Ä¢	Realtime streaming ingestion ‚Üí IRIS (plus IRIS Archive for replay)  Ôøº
	‚Ä¢	Backfill + parameterised pulls ‚Üí REST API dataset endpoints  Ôøº

‚∏ª

One important practical note

You‚Äôve already seen both ‚Äúportal files‚Äù and ‚ÄúInsights datasets‚Äù that look similar (e.g., MID/FUELHH). The big difference isn‚Äôt ‚Äúis it updated?‚Äù‚Äîit‚Äôs:
	‚Ä¢	how it‚Äôs delivered (file drop vs query vs push),
	‚Ä¢	whether you can pull superseded versions, and
	‚Ä¢	how easy it is to build a reliable pipeline.

If you tell me the exact BigQuery tables you‚Äôre auditing (e.g., elexon_frequency, elexon_fuelhh, elexon_market_index, sspsbpniv), I‚Äôll map each one to the best truth source (Portal vs REST vs IRIS Archive) and the key fields to use for completeness/duplicate checks. It‚Äôs not ‚Äúone dataset‚Äù ‚Äî across Portal scripting / IRIS / REST you‚Äôre getting a mix of:
	‚Ä¢	time-series operational signals (half-hourly or 5-min),
	‚Ä¢	settlement calculation factors (multipliers / correction factors),
	‚Ä¢	reference registers (participants, BM Units),
	‚Ä¢	calendars / timetables, and
	‚Ä¢	(for some endpoints) derived settlement/cashflow outputs.

A useful way to think about it:

What a typical record looks like (IRIS + REST)

For most ‚ÄúInsights‚Äù datasets, each record is keyed by time and often by settlement:
	‚Ä¢	publishTime (when Elexon/NESO published it),
	‚Ä¢	startTime / from / to (the time the value applies to),
	‚Ä¢	settlementDate + settlementPeriod (when it‚Äôs half-hourly),
	‚Ä¢	plus dataset-specific fields (price, MW, volume, etc.).

Elexon explicitly says the JSON output matches IRIS and can be used interchangeably with IRIS output.  Ôøº

‚∏ª

What your ‚ÄúPortal scripting‚Äù files actually contain

Below is what the content represents (and the key fields you should expect). For some portal CSVs, Elexon doesn‚Äôt publish the exact column schema publicly, so I‚Äôm describing the meaning and the usual keys.

1) sspsbpniv.csv (SSP/SBP/NIV)

Contains, for each Settlement Period in a day:
	‚Ä¢	System Sell Price (SSP) and System Buy Price (SBP) (cash-out / imbalance price, ¬£/MWh),
	‚Ä¢	Net Imbalance Volume (NIV) (MWh).
Elexon describes SSP/SBP as the cash-out prices and NIV as part of the ‚Äúlatest pricing‚Äù view per settlement period.  Ôøº
(Under single-price, SBP = SSP for a given SP.  Ôøº)

2) MID_*.csv / LATEST_MID_FILE (Market Index Data)

Contains Market Index Price and Market Index Volume per Settlement Period, per MID Provider (e.g., N2EX/APX), used as an input to deriving the reverse price used in cash-out.  Ôøº

3) fuelhh_*.csv / LATESTFUELHHFILE (Generation by fuel type, half-hourly)

Contains half-hourly generation outturn aggregated by fuel type, as average MW per 30 minutes, with settlement parameters (Settlement Date / Settlement Period).  Ôøº

4) tlm.csv / TLM_FILE (Transmission Loss Multiplier)

Contains the TLM value by Settlement Period (and typically date/period keys).
Conceptually, TLM is part of the settlement mechanism for transmission losses (how losses are allocated via multipliers/factors).  Ôøº

5) RCRC.csv / RCRC_FILE (Residual Cashflow Reallocation Cashflow)

Contains RCRC (normally ¬£/MWh) by Settlement Period (and typically date/period keys).
RCRC is the residual cashflow (sum of imbalance charges) that gets redistributed/collected from Trading Parties.  Ôøº

6) GSPGCF.csv (GSP Group Correction Factors)

Contains GSP Group Correction Factors (by GSP Group and Settlement Period), used to ensure supplier allocations in a GSP Group match energy entering the GSP Group.  Ôøº

7) SVA/CVA Line Loss Factors (LLFs)

These downloads contain line loss factors (by LDSO / LLFC / time band etc., depending on format), used to convert volumes at a distribution boundary to an equivalent at the transmission boundary.  Ôøº

8) REGISTERED_PARTICIPANTS_FILE

A reference register of registered market participants (names/IDs; sometimes roles/attributes depending on file schema).
Good for ‚Äúwho exists‚Äù checks, but it‚Äôs not an operational time series.

9) REGISTERED_BMUNITS_FILE

A reference register of BM Units (identifiers and attributes; used to join BM data to parties/assets).

10) Calendars / timetables (SAA, SVAA, D0095, FAA, PF Settlement run, PARMS)

These are process calendars: settlement run dates, reporting timetable, payment calendar, etc. (no operational MW/¬£ time series).

11) P114 (files + /p114/list)

P114 is a bundle of BSC system data flows made available as files (often flat/pipe-delimited) via Portal and FTP.  Ôøº
It‚Äôs used for settlement/system integration style consumption rather than ‚Äúsingle nice API‚Äù.

‚∏ª

If you want the exact columns for your Portal CSVs

The fastest/most reliable method is: download one file and read the header row. If you paste just the first line (column names) for:
	‚Ä¢	SSPSBPNIV_FILE, TLM_FILE, RCRC_FILE, GSPGROUPCORRECTIONFACTORFILE, LATEST_MID_FILE
‚Ä¶I‚Äôll tell you precisely what each column means and what the primary keys should be for BigQuery (and how to dedupe safely). IRIS and the REST API are basically two delivery methods for the same ‚ÄúInsights Solution‚Äù dataset catalogue:
	‚Ä¢	REST = you pull datasets on demand (often via /datasets/<CODE> and /datasets/<CODE>/stream).  Ôøº
	‚Ä¢	IRIS = you receive the same data as near-real-time pushed JSON files; the IRIS Archive holds everything that was sent.  Ôøº
	‚Ä¢	Elexon‚Äôs Swagger text explicitly says the JSON output matches IRIS and can be used interchangeably.  Ôøº
	‚Ä¢	The APIs are public and no API key is required.  Ôøº

What data is available (high-level groups + the main dataset codes)

Balancing Mechanism: bids/offers, acceptances, and physical notifications
	‚Ä¢	Bid-Offer Data: BOD  Ôøº
	‚Ä¢	Bid-Offer Acceptances: BOALF  Ôøº
	‚Ä¢	Physical Notifications: PN, QPN  Ôøº
	‚Ä¢	Export/Import limits: MELS, MILS  Ôøº
	‚Ä¢	Dynamic/notice/rate style BM data (also exposed as datasets): SEL, SIL, MZT, MNZT, MDV, MDP, NTB, NTO, NDZ, RURE, RDRE, RURI, RDRI  Ôøº

Balancing Services adjustments and related
	‚Ä¢	NETBSAD: NETBSAD  Ôøº
	‚Ä¢	DISBSAD: DISBSAD  Ôøº
	‚Ä¢	Non-BM STOR: NONBM  Ôøº
	‚Ä¢	Balancing Services Volume: QAS  Ôøº

Market prices / indices
	‚Ä¢	Market Index Data: MID  Ôøº
	‚Ä¢	SO-SO prices: SOSO  Ôøº

Generation outturn, availability, and forecasts
	‚Ä¢	Fuel-type outturn: FUELHH (half-hourly), FUELINST (instantaneous)  Ôøº
	‚Ä¢	Wind forecast: WINDFOR  Ôøº
	‚Ä¢	Wind/Solar actual + day-ahead: AGWS, DGWS  Ôøº
	‚Ä¢	Availability horizons (by BMU / fuel / aggregated):
	‚Ä¢	By BMU: UOU2T14D, UOU2T3YW  Ôøº
	‚Ä¢	By fuel type: FOU2T14D, FOU2T3YW  Ôøº
	‚Ä¢	Aggregated: NOU2T14D, NOU2T3YW  Ôøº
	‚Ä¢	Indicated generation: INDGEN  Ôøº

Demand outturn, forecasts, margin/imbalance indicators
	‚Ä¢	Indicated demand: INDDEM  Ôøº
	‚Ä¢	Initial demand outturn: INDO, plus daily INDOD  Ôøº
	‚Ä¢	Initial Transmission System Demand outturn: ITSDO  Ôøº
	‚Ä¢	National / TS demand forecasts: NDF, TSDF  Ôøº
	‚Ä¢	2‚Äì14 day and 2‚Äì52 week demand/surplus forecasts: NDFD, TSDFD, NDFW, TSDFW  Ôøº
	‚Ä¢	Indicated margin / imbalance: MELNGC, IMBALNGC  Ôøº
	‚Ä¢	Demand control instructions: DCI  Ôøº
	‚Ä¢	Restoration Region demand: RZDF, RZDR  Ôøº
	‚Ä¢	Additional margin/surplus forecast series listed: OCNMFD, OCNMFD2, OCNMF3Y, OCNMF3Y2  Ôøº

System conditions / warnings / adequacy
	‚Ä¢	System frequency: FREQ  Ôøº
	‚Ä¢	Temperature: TEMP  Ôøº
	‚Ä¢	LOLP & de-rated margin: LOLPDRM  Ôøº
	‚Ä¢	System warnings: SYSWARN  Ôøº

Market/industry notices & compliance
	‚Ä¢	Credit default notices: CDN  Ôøº
	‚Ä¢	REMIT dataset: REMIT  Ôøº
	‚Ä¢	Trading unit data: TUDM  Ôøº

Reference (‚Äústanding‚Äù) data also available via REST

Examples called out in the Swagger listing:
	‚Ä¢	/reference/bmunits/all (BM Units)  Ôøº
	‚Ä¢	/reference/interconnectors/all (Interconnectors)  Ôøº

How to see the full current catalogue (best links)
	‚Ä¢	Developer portal (explains patterns + lists available datasets):
https://developer.data.elexon.co.uk/  Ôøº
	‚Ä¢	Swagger UI (machine-readable list of endpoints/datasets; includes /datasets/<CODE> and /stream variants):
https://data.elexon.co.uk/swagger/index.html  Ôøº
	‚Ä¢	IRIS (push service + archive link):
https://bmrs.elexon.co.uk/iris and archive at https://archive.data.elexon.co.uk/iris-archive  Ôøº

If you tell me which BigQuery tables you have (e.g., elexon_frequency, elexon_fuelhh, elexon_mid, etc.), I‚Äôll map each one to its exact REST dataset code and the best IRIS folder path to audit against. CDCA-I042 (‚ÄúBM Unit Aggregation Report‚Äù) is available two main ways:

Option 1 (easiest, public): Open Settlement Data download (CSV in ZIP)

Elexon publishes CDCA-I042 as part of Open Settlement Data as a ‚ÄúBM Unit Aggregation Report‚Äù ZIP (month-to-date CSVs inside). On that page, CDCA data includes a BM Unit Aggregation Report ZIP for 2025 (ABV_2025.zip) and also for 2020‚Äì2024.  Ôøº

Use this when you want it for ad-hoc analysis (not operational flows). The page explicitly says it‚Äôs a subset of the source data flows (including CDCA-I042) and is provided to download/use.  Ôøº

Links (public): Open Settlement Data page:
https://www.elexon.co.uk/bsc/data/open-settlement-data/

CDCA ‚ÄúBM Unit Aggregation Report‚Äù (CDCA-I042) zip (example):
ABV_2025.zip is linked from the page Option 2 (operational / full daily flow): P114 delivery (Portal scripting or FTP)

CDCA-I042 is one of the P114 file types delivered daily (alongside SAA-I014 subflow 2, CDCA-I029, CDCA-I030).  Ôøº
This route is for ‚Äúproper operational processes‚Äù (Elexon‚Äôs wording: apply to receive the source data flows).  Ôøº

2A) Portal ‚ÄúP114 scripted download‚Äù (if your account has access)

If you have P114 permission, your portal scripting page gives you:
	‚Ä¢	a list endpoint and
	‚Ä¢	a download endpoint where you pass the filename.

From your earlier snippet, the P114 list filter includes c0421 (commonly used for the CDCA-I042 package on the portal). (You‚Äôll confirm this by listing a day and seeing the returned filenames.)

Pattern: # list today‚Äôs P114 files (or a specific date)
https://downloads.elexonportal.co.uk/p114/list?key=YOURKEY
https://downloads.elexonportal.co.uk/p114/list?key=YOURKEY&date=YYYY-MM-DD
https://downloads.elexonportal.co.uk/p114/list?key=YOURKEY&date=YYYY-MM-DD&filter=c0421

# download a specific file you saw in the list response
https://downloads.elexonportal.co.uk/p114/download?key=YOURKEY&filename=THE_FILENAME_FROM_LIST B) P114 via FTP (if enabled for your portal account)

Elexon‚Äôs portal FAQ says P114 files are available via FTP to portal accounts that either:
	‚Ä¢	have a participant ID linked, or
	‚Ä¢	have licensed P114 data.  Ôøº

FTP details from the FAQ: connect to ftp.elexonportal.co.uk using your portal username/password, then browse p114/YYYY-MM-DD/ which contains files including CDCA-I042.  Ôøº

‚∏ª

What CDCA-I042 contains (at a glance)

Elexon‚Äôs reporting catalogue describes CDCA-I042 as a daily BM Unit Aggregation report detailing aggregated meter flow volumes for BM Units.  Ôøº
Your description matches that: metered volumes per BM Unit, per aggregation run/day, with estimated vs actual indicators.

‚∏ª

Quick ‚Äúwhich should I use?‚Äù
	‚Ä¢	Want it now, public, easiest ‚Üí Open Settlement Data ‚Üí ABV_YYYY.zip  Ôøº
	‚Ä¢	Need the operational P114 feed (daily files, full process) ‚Üí P114 (scripted URLs or FTP)  Ôøº

If you tell me whether you want historic backfill (2020‚Äì2025 ZIPs) or daily operational (P114), I‚Äôll give you the exact download commands for your preferred tool (curl / wget / lftp) and a BigQuery load pattern (partitioning + dedupe keys).
